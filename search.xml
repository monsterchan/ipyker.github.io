<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Ambari 2.7.3与HDP 3.1.0安装过程]]></title>
    <url>%2F2019%2F07%2F30%2Fambari.html</url>
    <content type="text"><![CDATA[概念介绍Amabri介绍 Ambari 跟 Hadoop 等开源软件一样，也是 Apache Software Foundation 中的一个项目，并且是顶级项目。就 Ambari 的作用来说，就是创建、管理、监视 Hadoop 的集群，但是这里的 Hadoop 指的是 Hadoop 整个生态圈（例如 Hive，Hbase，Sqoop，Zookeeper 等）， 而并不仅是特指 Hadoop。用一句话来说，Ambari 就是为了让 Hadoop 以及相关的大数据软件更容易使用的一个工具。 Ambari 自身也是一个分布式架构的软件，主要由两部分组成：Ambari Server 和 Ambari Agent。简单来说，用户通过 Ambari Server 通知 Ambari Agent 安装对应的软件；Agent 会定时地发送各个机器每个软件模块的状态给 Ambari Server，最终这些状态信息会呈现在 Ambari 的 GUI，方便用户了解到集群的各种状态，并进行相应的维护。 HDP介绍 HDP是hortonworks的软件栈，里面包含了hadoop生态系统的所有软件项目，比如HBase,Zookeeper,Hive,Pig等等。 HDP-UTILS介绍 HDP-UTILS是工具类库。 安装前准备主机列表本次实验选择6台Cetnos7.6主机，其中2台作为Ambari Server主机，6台作为Ambari Agent。而2台Ambari Server主机使用静态手动配置VIP方式进行切换高可用。或使用keepalive+haproxy方式进行高可用。 主机名/解析记录 IP 运行组件 bigdata1.jms.com 192.168.12.201 ambari-server、ambari-agent、mysql-master bigdata2.jms.com 192.168.12.202 ambari-server、ambari-agent、mysql-slave bigdata3.jms.com 192.168.12.203 ambari-agent bigdata4.jms.com 192.168.12.204 ambari-agent bigdata5.jms.com 192.168.12.205 ambari-agent、ambari-yumrepo bigdata6.jms.com 192.168.12.206 ambari-agent ambari.jms.com 192.168.12.200 ambari-server VIP地址 注意： 2台Ambari Server主机到Ambari Agent主机配置免密登录 Ambari Server/Agent主机需安装JDK（不低于JDK8） 确保主机的hostname -f 满足FQDN格式（在安装集群的第三步Confirm Host需要） 关闭firewalld和iptables防火墙，（如有规则要清空） 确认主机字符集编码为UTF-8（否则Ambari Server 配置数据库可能报错） 开启NTP服务，（ntp1.aliyun.com，time.windows.com） selinux配置为disbaled 配置描述文件数量65535 配置免密钥登陆123456789101112# bigdata1操作$ ssh-keygen -t rsa$ ssh-copy-id bigdata1.jms.com$ ssh-copy-id bigdata2.jms.com$ ssh-copy-id bigdata3.jms.com$ ssh-copy-id bigdata4.jms.com$ ssh-copy-id bigdata5.jms.com$ ssh-copy-id bigdata6.jms.com# 将bigdata1上的公钥私钥复制到bigdata2上$ scp ~/.ssh/id_rsa.pub bigdata2.jms.com:~/.ssh/$ scp ~/.ssh/id_rsa bigdata2.jms.com:~/.ssh/ 因为在web界面配置集群的时候需要输入ambari-server的私钥，只能配置一个，所以我们手动将bigdata1公钥和私钥复制给bigdate2。以达到同一密钥多主机生效。 手动配置VIP地址先在192.168.12.201服务器上配置ambari-server VIP，当192.168.12.201宕机后，手动配置ifcfg-em1:1到192.168.12.202上。12345678910111213141516$ cat &gt; /etc/sysconfig/network-scripts/ifcfg-em1:1 &lt;&lt; EOFTYPE=EthernetBROWSER_ONLY=noBOOTPROTO=noneDEFROUTE=yesIPV4_FAILURE_FATAL=noNAME=em1:1DEVICE=em1:1ONBOOT=yesIPADDR=192.168.12.200PREFIX=23GATEWAY=192.168.13.1DNS1=192.168.1.211EOF$ systemctl restart network 注意： 切勿在192.168.12.202同时配置，否则由于VIP不稳定，造成ambari集群各种故障。只需在192.168.12.201宕机后配置即可，或者直接使用keepalive+haproxy。 配置本地解析123456789$ cat &gt;&gt; /etc/hosts &lt;&lt; EOF192.168.12.200 ambari.jms.com192.168.12.201 bigdata1.jms.com bigdata1192.168.12.202 bigdata2.jms.com bigdata2192.168.12.203 bigdata3.jms.com bigdata3192.168.12.204 bigdata4.jms.com bigdata4192.168.12.205 bigdata5.jms.com bigdata5192.168.12.206 bigdata6.jms.com bigdata6EOF 配置sysctl文件12345678910111213141516171819202122232425262728293031323334353637383940414243444546$ cat &gt; /etc/sysctl.conf &lt;&lt; EOFfs.file-max=65535vm.swappiness=1net.ipv4.ip_forward = 1net.ipv4.conf.default.rp_filter = 1net.ipv4.conf.default.accept_source_route = 0net.ipv4.tcp_max_tw_buckets = 6000net.ipv4.ip_local_port_range = 1024 65000net.ipv4.tcp_timestamps = 1net.ipv4.tcp_tw_recycle = 0net.ipv4.tcp_tw_reuse = 1net.ipv4.tcp_syncookies = 1kernel.msgmnb = 65536kernel.msgmax = 65536kernel.shmmax = 68719476736kernel.shmall = 4294967296net.ipv4.tcp_max_syn_backlog = 262144net.core.netdev_max_backlog = 262144net.core.somaxconn = 262144net.ipv4.tcp_max_orphans = 262144net.ipv4.tcp_synack_retries = 1net.ipv4.tcp_syn_retries = 1net.ipv4.tcp_fin_timeout = 1net.ipv4.tcp_keepalive_time = 30net.ipv4.tcp_sack = 1net.ipv4.tcp_window_scaling = 1net.ipv4.tcp_rmem = 4096 87380 4194304net.ipv4.tcp_wmem = 4096 16384 4194304net.core.wmem_default = 8388608net.core.rmem_default = 8388608net.core.rmem_max = 16777216net.core.wmem_max = 16777216net.ipv4.tcp_mem = 94500000 915000000 927000000net.nf_conntrack_max = 6553500net.netfilter.nf_conntrack_max = 6553500net.netfilter.nf_conntrack_tcp_timeout_close_wait = 60net.netfilter.nf_conntrack_tcp_timeout_fin_wait = 120net.netfilter.nf_conntrack_tcp_timeout_time_wait = 120net.netfilter.nf_conntrack_tcp_timeout_established = 3600EOF 安装基础软件包1$ yum install -y lrzsz ntpdate supervisor sysstat net-tools vim wget ntp lsof 关闭THP（如果不关闭THP，Hadoop的系统CPU使用率很高）123456789101112$ echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled$ echo never &gt; /sys/kernel/mm/transparent_hugepage/defrag# 开机自启动关闭$ cat &gt;&gt; /etc/rc.local &lt;&lt; EOFif test -f /sys/kernel/mm/transparent_hugepage/enabled; then echo never &gt; /sys/kernel/mm/transparent_hugepage/enabledfiif -f /sys/kernel/mm/transparent_hugepage/defrag; then echo never &gt; /sys/kernel/mm/transparent_hugepage/defragfiEOFchmod +x /etc/rc.local 配置mysql数据库安装mysql主从mysql主从安装这里跳过。但mysql配置文件要保证以下配置正确：123456在[mysqld]下添加collation_server=utf8_general_cicharacter_set_server=utf8default-storage-engine=INNODB在[client]下添加（如果没有[client]，则创建）default_character-set=utf8 配置ambari需要的数据库在192.168.12.201 mysql主上进行新建数据库： 创建ambari数据库及数据库的用户名和密码 123456mysql&gt; create database ambari character set utf8;Query OK, 1 row affected (0.00 sec)mysql&gt; GRANT ALL PRIVILEGES ON ambari.* TO 'ambari'@'%' IDENTIFIED BY 'Ambari123';Query OK, 0 rows affected (0.00 sec)mysql&gt; FLUSH PRIVILEGES;Query OK, 0 rows affected (0.01 sec) 创建hive数据库及hive库的用户名和密码 123456mysql&gt; create database hive character set utf8;Query OK, 1 row affected (0.00 sec)mysql&gt; GRANT ALL PRIVILEGES ON hive.* TO 'hive'@'%' IDENTIFIED BY 'Hive123';Query OK, 0 rows affected (0.00 sec)mysql&gt; FLUSH PRIVILEGES;Query OK, 0 rows affected (0.01 sec) 创建oozie数据库及oozie库的用户名和密码 123456mysql&gt; create database oozie character set utf8;Query OK, 1 row affected (0.00 sec)mysql&gt; GRANT ALL PRIVILEGES ON oozie.* TO 'oozie'@'%' IDENTIFIED BY 'Oozie123';Query OK, 0 rows affected (0.00 sec)mysql&gt; FLUSH PRIVILEGES;Query OK, 0 rows affected (0.01 sec) 创建ranger数据库及ranger库的用户名和密码 123456mysql&gt; create database ranger character set utf8;Query OK, 1 row affected (0.04 sec)mysql&gt; GRANT ALL PRIVILEGES ON rangeradmin.* TO 'rangeradmin'@'%' IDENTIFIED BY 'Ranger123';Query OK, 0 rows affected (0.02 sec)mysql&gt; FLUSH PRIVILEGES;Query OK, 0 rows affected (0.04 sec) 以上数据库只有ambari是必须要新建的，其他3个数据库只有在你需要安装该服务的时候需要进行配置，为了日后需要安装这些服务，我这里就先建立这些数据库。 配置ambari yum私仓在这里ambari yum私仓安装在192.168.12.205服务器上。 下载私仓包12345678$ mkdir -p /data/wwwroot/default/&#123;ambari,hdp/&#123;HDP-UTILS-1.1.0.22&#125;&#125;$ wget http://public-repo-1.hortonworks.com/ambari/centos7/2.x/updates/2.7.3.0/ambari-2.7.3.0-centos7.tar.gz$ wget http://public-repo-1.hortonworks.com/HDP/centos7/3.x/updates/3.1.0.0/HDP-3.1.0.0-centos7-rpm.tar.gz$ wget http://public-repo-1.hortonworks.com/HDP-UTILS-1.1.0.22/repos/centos7/HDP-UTILS-1.1.0.22-centos7.tar.gz$ tar -zxvf ambari-2.7.3.0-centos7.tar.gz -C /data/wwwroot/default/ambari$ tar -zxvf HDP-3.1.0.0-centos7-rpm.tar.gz -C /data/wwwroot/default/hdp/$ tar -zxvf HDP-UTILS-1.1.0.22-centos7.tar.gz -C /data/wwwroot/default/hdp/HDP-UTILS-1.1.0.22/ 配置nginx文件服务安装nginx12# yum安装或者源代码安装nginx，我是源代码安装nginx的$ yum install nginx -y 修改配置文件123456789101112131415161718$ vi /usr/local/nginx/conf/nginx.conf...略 server &#123; listen 80; server_name bigdata5.jms.com; access_log /data/wwwlogs/access_nginx.log combined; index index.html index.htm index.php; autoindex on; autoindex_exact_size on; autoindex_localtime on; location /ambari &#123; root /data/wwwroot/default; &#125; location /hdp &#123; root /data/wwwroot/default; &#125; &#125; 浏览器中访问http://bigdata5.jms.com/ambari 和 http://bigdata5.jms.com/hdp ,访问正常即ambari文件服务配置正常。 配置私仓repo12345678910111213141516171819202122232425262728293031$ cat &gt; /etc/yum.repos.d/ambari.repo &lt;&lt; EOF#VERSION_NUMBER=2.7.3.0-139[ambari-2.7.3.0]#json.url = http://public-repo-1.hortonworks.com/HDP/hdp_urlinfo.jsonname=ambari Version - ambari-2.7.3.0#baseurl=http://public-repo-1.hortonworks.com/ambari/centos7/2.x/updates/2.7.3.0baseurl=http://bigdata5.jms.com/ambari/ambari/centos7/2.7.3.0-139gpgcheck=1gpgkey=http://bigdata5.jms.com/ambari/ambari/centos7/2.7.3.0-139/RPM-GPG-KEY/RPM-GPG-KEY-Jenkinsenabled=1priority=1EOF$ cat &gt; /etc/yum.repos.d/HDP.repo &lt;&lt; EOF#VERSION_NUMBER=3.1.0.0-78[HDP-3.1.0.0]name=HDP Version - HDP-3.1.0.0baseurl=http://bigdata5.jms.com/hdp/HDP/centos7/gpgcheck=1gpgkey=http://bigdata5.jms.com/hdp/HDP/centos7/3.1.0.0-78/RPM-GPG-KEY/RPM-GPG-KEY-Jenkinsenabled=1priority=1[HDP-UTILS-1.1.0.22]name=HDP-UTILS Version - HDP-UTILS-1.1.0.22baseurl=http://bigdata5.jms.com/hdp/HDP-UTILS-1.1.0.22/gpgcheck=1gpgkey=http://bigdata5.jms.com/hdp/HDP-UTILS-1.1.0.22/HDP-UTILS/centos7/1.1.0.22/RPM-GPG-KEY/RPM-GPG-KEY-Jenkinsenabled=1priority=1EOF 将Ambari.repo和HDP.repo分发到bigdata1，2，3，4，6节点/etc/yum.repos.d下。 生成本地源使用createrepo命令，创建yum本地源（软件仓库），即为存放本地特定位置的众多rpm包建立索引，描述各包所需依赖信息，并形成元数据。123$ yum install createrepo -y$ createrepo /data/wwwroot/default/hdp/HDP/centos7/$ createrepo /data/wwwroot/default/hdp/HDP-UTILS-1.1.0.22/ 安装ambari-server在192.168.12.201和192.168.12.202上安装ambari-server。安装前请配置mysql和ambari连接器，如下：12$ yum install -y mysql-connector-java$ ambari-server setup --jdbc-db=mysql --jdbc-driver=/usr/share/java/mysql-connector-java.jar 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455# 安装ambari-server$ yum install -y ambari-server# 配置ambari-server$ ambari-server setupUsing python /usr/bin/pythonSetup ambari-serverChecking SELinux...SELinux status is 'disabled'Customize user account for ambari-server daemon [y/n] (n)? yEnter user account for ambari-server daemon (root):rootAdjusting ambari-server permissions and ownership...Checking firewall status...Checking JDK...Do you want to change Oracle JDK [y/n] (n)? y[1] Oracle JDK 1.8 + Java Cryptography Extension (JCE) Policy Files 8[2] Custom JDK==============================================================================Enter choice (1): 2WARNING: JDK must be installed on all hosts and JAVA_HOME must be valid on all hosts.WARNING: JCE Policy files are required for configuring Kerberos security. If you plan to use Kerberos,please make sure JCE Unlimited Strength Jurisdiction Policy Files are valid on all hosts.Path to JAVA_HOME: /usr/lib/jvm/java-openjdkValidating JDK on Ambari Server...done.Checking GPL software agreement...Completing setup...Configuring database...Enter advanced database configuration [y/n] (n)? YConfiguring database...==============================================================================Choose one of the following options:[1] - PostgreSQL (Embedded)[2] - Oracle[3] - MySQL / MariaDB[4] - PostgreSQL[5] - Microsoft SQL Server (Tech Preview)[6] - SQL Anywhere[7] - BDB==============================================================================Enter choice (3): 3 ####如果主机字符集编码未设置正确，可能会启动报错，具体可以查看日志/var/log/ambari-server/ambari-server.logHostname (localhost): ambari.jms.comPort (3306): # 默认Database name (ambari): Username (ambari): Enter Database Password (bigdata): # 密码不显示Re-enter password: Configuring ambari database...Should ambari use existing default jdbc /usr/share/java/mysql-connector-java.jar [y/n] (y)? yConfiguring remote database connection properties...WARNING: Before starting Ambari Server, you must run the following DDL directly from the database shell to create the schema: /var/lib/ambari-server/resources/Ambari-DDL-MySQL-CREATE.sql # 此处需注意，启动ambari之前需要执行此句Proceed with configuring remote database connection properties [y/n] (y)? yExtracting system views...ambari-admin-2.7.3.0.139.jar....Ambari repo file contains latest json url http://public-repo-1.hortonworks.com/HDP/hdp_urlinfo.json, updating stacks repoinfos with it...Adjusting ambari-server permissions and ownership...Ambari Server 'setup' completed successfully. # 安装成功 登陆主mysql开始导入ambari的数据库12mysql&gt; use ambari;mysql&gt; source /var/lib/ambari-server/resources/Ambari-DDL-MySQL-CREATE.sql; 无报错即导入正常。之后即可启动ambari-server1234567891011121314$ ambari-server startUsing python /usr/bin/pythonStarting ambari-serverAmbari Server running with administrator privileges.Organizing resource files at /var/lib/ambari-server/resources...Ambari database consistency check started...Server PID at: /var/run/ambari-server/ambari-server.pidServer out at: /var/log/ambari-server/ambari-server.outServer log at: /var/log/ambari-server/ambari-server.logWaiting for server start......................................................Server started listening on 8080DB configs consistency check: no errors and warnings were found.Ambari Server 'start' completed successfully. 安装ambari-agent在所有节点上安装ambari-agent，执行以下命令安装：1$ yum install -y ambari-agent 由于ambari-server有2台主机，ambari-server使用的是VIP地址，所以我们这里得手动修改所有节点ambari-agent的配置文件。如下：12345678$ vi /etc/ambari-agent/conf/ambari-agent.ini[server]hostname=ambari.jms.com # 修改此地址即可url_port=8440secured_url_port=8441connect_retry_delay=10max_reconnect_retry_delay=30... 访问ambari-server web页面 默认端口8080，Username：admin；Password：admin；http://ambari.jms.com:8080 创建集群 选择HDP版本以及配置私仓地址 要注意的地方就这些了，剩下的步骤就是按照引导进行下一步操作即可。直到第七步骤，如下图需要选择自己前面安装好的数据库，以及数据库地址和用户名密码进行连接即可。]]></content>
      <categories>
        <category>ambari</category>
      </categories>
      <tags>
        <tag>ambari</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes挂载卷子命令subpath]]></title>
    <url>%2F2019%2F07%2F25%2Fk8s-subpath.html</url>
    <content type="text"><![CDATA[k8s允许我们将不同类型的volume挂载到容器的特定目录下。例如，我们可以将configmap的数据以volume的形式挂到容器下。 定义一个configmap，其中的数据以 key:value 的格式体现。1234567891011apiVersion: v1kind: ConfigMapmetadata: name: special-config namespace: defaultdata: special.level: very special.type: |- property.1=value-1 property.2=value-2 property.3=value-3 创建一个Pod， 它挂载上面定义的comfigmap，并在启动时查看挂载目录 /etc/config/ 下有哪些文件。 1234567891011121314151617181920$ cat dapi-test-pod.yamlapiVersion: v1kind: Podmetadata: name: dapi-test-podspec: containers: - name: test-container image: k8s.gcr.io/busybox command: [ "/bin/sh", "-c", "ls /etc/config/" ] volumeMounts: - name: config-volume mountPath: /etc/config volumes: - name: config-volume configMap: # Provide the name of the ConfigMap containing the files you want # to add to the container name: special-config restartPolicy: Never 123由于pod中的command调用 ls /etc/config/ 命令，那么该容器会显示完文件自动退出！special.levelspecial.type 可以看到pod将configmap定义的data全部挂载进来了。这里需要注意： 挂载目录下的文件名称，即为cm定义里的key值。 挂载目录下的文件的内容，即为cm定义里的value值。value可以多行定义，这在一些稍微复杂的场景下特别有用，比如 my.cnf。 如果挂载目录下原来有文件，挂载后将不可见（AUFS）。 这些规则其实是和kubectl explain pod.spec.volumes.configMap.items 的items参数有关，该参数的意思为：如果items没有指定，则该ConfigMap的data字段中的所有键-值对都将作为一个文件映射到卷中，该文件的名称是configmap的key，内容是configmap的value。如果指定了items，那么指定的key将被映射到指定的路径中，没指定的key将不存在卷中。如果指定了一个在ConfigMap中不存在的key，除非它被标记为optional: true，否则卷设置将出错。路径必须是相对于mountPath的，并且不能包含’..路径或以“..”开头。 有的时候，我们希望将文件挂载到某个目录，但希望只是挂载该文件，不要影响挂载目录下的其他文件。有办法吗？可以用subPath: Path within the volume from which the container’s volume should be mounted。Volume中待挂载的子目录。subPath 的目的是为了在单一Pod中多次使用同一个volume而设计的。 例如，像下面的LAMP，可以将同一个volume下的 mysql 和 html目录，挂载到不同的挂载点上，这样就不需要为 mysql 和 html 单独创建volume了。12345678910111213141516171819202122232425apiVersion: v1kind: Podmetadata: name: my-lamp-sitespec: containers: - name: mysql image: mysql env: - name: MYSQL_ROOT_PASSWORD value: "rootpasswd" volumeMounts: - mountPath: /var/lib/mysql name: site-data subPath: mysql - name: php image: php:7.0-apache volumeMounts: - mountPath: /var/www/html name: site-data subPath: html volumes: - name: site-data persistentVolumeClaim: claimName: my-lamp-site-data 那怎么解决我们的问题呢？很简单，如果subPath不是目录，而是一个文件，那么就可以达到单独挂载的目的了。123456789containers:- volumeMounts: - name: demo-config mountPath: /etc/special.type subPath: special.typevolumes:- name: demo-config configMap: name: special-config 如果volumes使用的是NFS文件存储方式，subPath默认是该卷的子目录，如果想挂载NFS文件存储中的文件，则该文件必须事先存在，且后期不能在NFS文件存储中修改，如果手动修改会报cat: can&#39;t open &#39;xxx&#39;: Stale NFS file handle 的错误。 该错误可能需要重启Pod，让Pod重新挂载才行。但是可以在pod中直接修改该文件。 来看看实现。重点应该是在t.Mode()&amp;os.ModeDir，即根据volume中的subPath指定的是目录还是文件分别动作。123456789101112131415161718192021func doBindSubPath(mounter Interface, subpath Subpath, kubeletPid int) (hostPath string, err error) &#123;... // Create target of the bind mount. A directory for directories, empty file // for everything else. t, err := os.Lstat(subpath.Path) if err != nil &#123; return "", fmt.Errorf("lstat %s failed: %s", subpath.Path, err) &#125; if t.Mode()&amp;os.ModeDir &gt; 0 &#123; if err = os.Mkdir(bindPathTarget, 0750); err != nil &amp;&amp; !os.IsExist(err) &#123; return "", fmt.Errorf("error creating directory %s: %s", bindPathTarget, err) &#125; &#125; else &#123; // "/bin/touch &lt;bindDir&gt;". // A file is enough for all possible targets (symlink, device, pipe, // socket, ...), bind-mounting them into a file correctly changes type // of the target file. if err = ioutil.WriteFile(bindPathTarget, []byte&#123;&#125;, 0640); err != nil &#123; return "", fmt.Errorf("error creating file %s: %s", bindPathTarget, err) &#125; &#125;]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes安装zookeeper 3.5.5]]></title>
    <url>%2F2019%2F07%2F20%2Fk8s-zookeeper.html</url>
    <content type="text"><![CDATA[准备zookeeper statefulset文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384$ cat zk-statefulset.yamlapiVersion: apps/v1beta1kind: StatefulSetmetadata: name: zoospec: serviceName: "zoo" replicas: 3 template: metadata: labels: app: zookeeper spec: terminationGracePeriodSeconds: 10 containers: - name: zookeeper image: registry.cn-shenzhen.aliyuncs.com/pyker/zookeeper:3.5.5 imagePullPolicy: IfNotPresent readinessProbe: httpGet: path: /commands/ruok port: 8080 initialDelaySeconds: 10 timeoutSeconds: 5 periodSeconds: 3 livenessProbe: httpGet: path: /commands/ruok port: 8080 initialDelaySeconds: 30 timeoutSeconds: 5 periodSeconds: 3 env: - name: ZOO_SERVERS value: server.1=zoo-0.zoo:2888:3888;2181 server.2=zoo-1.zoo:2888:3888;2181 server.3=zoo-2.zoo:2888:3888;2181 ports: - containerPort: 2181 name: client - containerPort: 2888 name: peer - containerPort: 3888 name: leader-election volumeMounts: - name: datadir mountPath: /data volumeClaimTemplates: - metadata: name: datadir spec: accessModes: [ "ReadWriteOnce" ] storageClassName: "nfs" resources: requests: storage: 1Gi---apiVersion: v1kind: Servicemetadata: name: zookeeperspec: type: NodePort ports: - port: 2181 name: client targetPort: 2181 selector: app: zookeeper---apiVersion: v1kind: Servicemetadata: name: zoospec: ports: - port: 2888 name: peer - port: 3888 name: leader-election clusterIP: None selector: app: zookeeper 1234567$ kubectl apply -f zk-statefulset.yaml# 查看pod是否正常运行$ kubectl get pod | grep zooNAME READY STATUS RESTARTS AGEzoo-0 1/1 Running 3 6hzoo-1 1/1 Running 0 3hzoo-2 1/1 Running 0 3h 为zookeeper准备图形界面123456789101112131415161718192021222324252627282930313233343536$ cat zkui.yamlapiVersion: v1kind: Servicemetadata: name: zkui labels: app: zkuispec: type: NodePort ports: - port: 9090 protocol: TCP targetPort: 9090 selector: app: zkui---apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: zkuispec: replicas: 1 template: metadata: labels: app: zkui spec: containers: - name: zkui image: registry.cn-shenzhen.aliyuncs.com/pyker/zkui:latest imagePullPolicy: IfNotPresent env: - name: ZK_SERVER# value: "zoo-1.zoo:2181,zoo-2.zoo:2181,zoo-0.zoo:2181"# 此处用选择的是node ip和 nodeport value: "192.168.1.236:2181,192.168.1.236:2182,192.168.1.236:2183" 12345678$ kubectl apply -f zkui.yaml# 查看pod是否正常运行kubectl get pod | grep zkuiNAME READY STATUS RESTARTS AGEzkui-6f65d8b8f8-m27rd 1/1 Running 1 2d23h# 查看service状态$ kubectl get svc | grep zkuizkui NodePort 10.254.2.58 &lt;none&gt; 9090:30458/TCP 6d1h 此时，可以看到zkui Pod的9090映射到宿主机的30458端口了。那么局域网的主机可以通过http://nodeip:30458方式访问zkui界面了。 我这里的默认帐号是admin，密码为manager。密码帐号可以在zkui的pod内的/var/app/config.cfg文件进行修改。 附上脚本和Dockerfile123456789101112131415161718192021222324252627282930313233343536373839404142434445464748$ cat docker-entrypoint.sh#!/bin/bashset -e# Allow the container to be started with `--user`if [[ "$1" = 'zkServer.sh' &amp;&amp; "$(id -u)" = '0' ]]; then chown -R zookeeper "$ZOO_DATA_DIR" "$ZOO_DATA_LOG_DIR" "$ZOO_LOG_DIR" "$ZOO_CONF_DIR" exec gosu zookeeper "$0" "$@"fi# Generate the config only if it doesn't existif [[ ! -f "$ZOO_CONF_DIR/zoo.cfg" ]]; then CONFIG="$ZOO_CONF_DIR/zoo.cfg" echo "dataDir=$ZOO_DATA_DIR" &gt;&gt; "$CONFIG" echo "dataLogDir=$ZOO_DATA_LOG_DIR" &gt;&gt; "$CONFIG" echo "4lw.commands.whitelist=$ZOO_COMMANDS_WHITELIST" &gt;&gt; "$CONFIG" echo "tickTime=$ZOO_TICK_TIME" &gt;&gt; "$CONFIG" echo "initLimit=$ZOO_INIT_LIMIT" &gt;&gt; "$CONFIG" echo "syncLimit=$ZOO_SYNC_LIMIT" &gt;&gt; "$CONFIG" echo "autopurge.snapRetainCount=$ZOO_AUTOPURGE_SNAPRETAINCOUNT" &gt;&gt; "$CONFIG" echo "autopurge.purgeInterval=$ZOO_AUTOPURGE_PURGEINTERVAL" &gt;&gt; "$CONFIG" echo "maxClientCnxns=$ZOO_MAX_CLIENT_CNXNS" &gt;&gt; "$CONFIG" echo "standaloneEnabled=$ZOO_STANDALONE_ENABLED" &gt;&gt; "$CONFIG" echo "admin.enableServer=$ZOO_ADMINSERVER_ENABLED" &gt;&gt; "$CONFIG" if [[ -z $ZOO_SERVERS ]]; then ZOO_SERVERS="server.1=localhost:2888:3888;2181" fi for server in $ZOO_SERVERS; do echo "$server" &gt;&gt; "$CONFIG" donefiZOO_MY_ID=$(($(hostname | sed s/.*-//) + 1))cp $ZOO_CONF_DIR/zoo.cfg $ZOO_CONF_DIR/zoo.cfg.orgsed -i "s/server\.$ZOO_MY_ID\=[a-z0-9.-]*/server.$ZOO_MY_ID=0.0.0.0/" "$ZOO_CONF_DIR/zoo.cfg"# Write myid only if it doesn't existif [ ! -f "$ZOO_DATA_DIR/myid" ]; then echo "$&#123;ZOO_MY_ID:-1&#125;" &gt; "$ZOO_DATA_DIR/myid"fiexec "$@" Dockerfile123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869$ cat DockerfileFROM openjdk:8-jre-slimENV ZOO_CONF_DIR=/conf \ ZOO_DATA_DIR=/data \ ZOO_DATA_LOG_DIR=/datalog \ ZOO_LOG_DIR=/logs \ ZOO_TICK_TIME=2000 \ ZOO_INIT_LIMIT=5 \ ZOO_SYNC_LIMIT=2 \ ZOO_AUTOPURGE_PURGEINTERVAL=0 \ ZOO_AUTOPURGE_SNAPRETAINCOUNT=3 \ ZOO_MAX_CLIENT_CNXNS=60 \ ZOO_STANDALONE_ENABLED=true \ ZOO_ADMINSERVER_ENABLED=true \ ZOO_COMMANDS_WHITELIST=*# Add a user with an explicit UID/GID and create necessary directoriesRUN set -eux; \ groupadd -r zookeeper --gid=1000; \ useradd -r -g zookeeper --uid=1000 zookeeper; \ mkdir -p "$ZOO_DATA_LOG_DIR" "$ZOO_DATA_DIR" "$ZOO_CONF_DIR" "$ZOO_LOG_DIR"; \ chown zookeeper:zookeeper "$ZOO_DATA_LOG_DIR" "$ZOO_DATA_DIR" "$ZOO_CONF_DIR" "$ZOO_LOG_DIR"# Install required packgesRUN set -eux; \ apt-get update; \ DEBIAN_FRONTEND=noninteractive \ apt-get install -y --no-install-recommends \ ca-certificates \ dirmngr \ gosu \ gnupg \ netcat \ wget; \ rm -rf /var/lib/apt/lists/*; \# Verify that gosu binary works gosu nobody trueARG GPG_KEY=3F7A1D16FA4217B1DC75E1C9FFE35B7F15DFA1BAARG SHORT_DISTRO_NAME=zookeeper-3.5.5ARG DISTRO_NAME=apache-zookeeper-3.5.5-bin# Download Apache Zookeeper, verify its PGP signature, untar and clean upCOPY apache-zookeeper-3.5.5-bin.tar.gz ./RUN set -eux; \# wget -q "https://www.apache.org/dist/zookeeper/$SHORT_DISTRO_NAME/$DISTRO_NAME.tar.gz"; \ wget -q "https://www.apache.org/dist/zookeeper/$SHORT_DISTRO_NAME/$DISTRO_NAME.tar.gz.asc"; \ export GNUPGHOME="$(mktemp -d)"; \ gpg --keyserver ha.pool.sks-keyservers.net --recv-key "$GPG_KEY" || \ gpg --keyserver pgp.mit.edu --recv-keys "$GPG_KEY" || \ gpg --keyserver keyserver.pgp.com --recv-keys "$GPG_KEY"; \ gpg --batch --verify "$DISTRO_NAME.tar.gz.asc" "$DISTRO_NAME.tar.gz"; \ tar -zxf "$DISTRO_NAME.tar.gz"; \ mv "$DISTRO_NAME/conf/"* "$ZOO_CONF_DIR"; \ rm -rf "$GNUPGHOME" "$DISTRO_NAME.tar.gz" "$DISTRO_NAME.tar.gz.asc"; \ chown -R zookeeper:zookeeper "/$DISTRO_NAME"WORKDIR $DISTRO_NAMEVOLUME ["$ZOO_DATA_DIR", "$ZOO_DATA_LOG_DIR", "$ZOO_LOG_DIR"]EXPOSE 2181 2888 3888 8080ENV PATH=$PATH:/$DISTRO_NAME/bin \ ZOOCFGDIR=$ZOO_CONF_DIRCOPY docker-entrypoint.sh /ENTRYPOINT ["/docker-entrypoint.sh"]CMD ["zkServer.sh", "start-foreground"]]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes安装redis集群]]></title>
    <url>%2F2019%2F07%2F17%2Fredis-cluster.html</url>
    <content type="text"><![CDATA[部署方式 Statefulset Service &amp; depolyment 对于redis,mysql这种有状态的服务,我们使用statefulset方式为首选.我们这边主要就是介绍statefulset这种方式。 statefulset 的设计原理模型: 拓扑状态：应用的多个实例之间不是完全对等的关系,这个应用实例的启动必须按照某些顺序启动,比如应用的 主节点 A 要先于从节点 B 启动。而如果你把 A 和 B 两个Pod删除掉,他们再次被创建出来是也必须严格按 照这个顺序才行,并且,新创建出来的Pod,必须和原来的Pod的网络标识一样,这样原先的访问者才能使用同样 的方法,访问到这个新的Pod 存储状态：应用的多个实例分别绑定了不同的存储数据.对于这些应用实例来说,Pod A第一次读取到的数据,和 隔了十分钟之后再次读取到的数据,应该是同一份,哪怕在此期间Pod A被重新创建过.一个数据库应用的多个 存储实例。 存储卷了解statefulset状态后，你应该知道要为数据准备一个存储卷了，创建方式有静态方式和动态方式，静态方式就是手动创建PV、PVC，然后POD进行进行调用即可。我们这里使用动态nfs作为挂载卷，可以参考上文：部署NFS动态StorageClass 准备名称空间1$ kubectl create ns sts-app 我这里使用了sts-app名称空间来部署redis，所以必须先创建该名称空间，你也可以不创建使用默认的default名称空间。 准备redis配置文件redis配置文件使用configmap方式进行挂载，如果将配置封装到docker image中的话，俺么每次修改配置就需要重新docker build。个人觉得比较麻烦，所以使用configmap方式挂载配置。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869$ cat redis-configmap.yamlapiVersion: v1kind: ConfigMapmetadata: name: redis-conf-cluster namespace: sts-appdata: fix-ip.sh: | #!/bin/sh CLUSTER_CONFIG="/data/nodes.conf" if [ -f $&#123;CLUSTER_CONFIG&#125; ]; then if [ -z "$&#123;POD_IP&#125;" ]; then echo "Unable to determine Pod IP address!" exit 1 fi echo "Updating my IP to $&#123;POD_IP&#125; in $&#123;CLUSTER_CONFIG&#125;" sed -i.bak -e '/myself/ s/[0-9]\&#123;1,3\&#125;\.[0-9]\&#123;1,3\&#125;\.[0-9]\&#123;1,3\&#125;\.[0-9]\&#123;1,3\&#125;/'$&#123;POD_IP&#125;'/' $&#123;CLUSTER_CONFIG&#125; fi exec "$@" redis.conf: | cluster-enabled yes cluster-config-file /data/nodes.conf cluster-node-timeout 10000 protected-mode no daemonize no pidfile /var/run/redis.pid port 6379 tcp-backlog 511 bind 0.0.0.0 timeout 3600 tcp-keepalive 1 loglevel verbose logfile /data/redis.log databases 16 save 900 1 save 300 10 save 60 10000 stop-writes-on-bgsave-error yes rdbcompression yes rdbchecksum yes dbfilename dump.rdb dir /data #requirepass yl123456 appendonly yes appendfilename "appendonly.aof" appendfsync everysec no-appendfsync-on-rewrite no auto-aof-rewrite-percentage 100 auto-aof-rewrite-min-size 64mb lua-time-limit 20000 slowlog-log-slower-than 10000 slowlog-max-len 128 #rename-command FLUSHALL "" latency-monitor-threshold 0 notify-keyspace-events "" hash-max-ziplist-entries 512 hash-max-ziplist-value 64 list-max-ziplist-entries 512 list-max-ziplist-value 64 set-max-intset-entries 512 zset-max-ziplist-entries 128 zset-max-ziplist-value 64 hll-sparse-max-bytes 3000 activerehashing yes client-output-buffer-limit normal 0 0 0 client-output-buffer-limit slave 256mb 64mb 60 client-output-buffer-limit pubsub 32mb 8mb 60 hz 10 aof-rewrite-incremental-fsync yes fix-ip.sh 脚本的作用用于当redis集群某pod重建后Pod IP发生变化，在/data/nodes.conf中将新的Pod IP替换原Pod IP。不然集群会出问题。 准备redis statsfulset文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118$ cat redis-statefulset.yamlapiVersion: v1kind: Servicemetadata: name: redis-headless namespace: sts-app labels: app: redisspec: ports: - name: redis-port port: 6379 clusterIP: None selector: app: redis---apiVersion: v1kind: Servicemetadata: name: redis-client namespace: sts-app labels: app: redisspec: ports: - name: redis-port port: 6379 targetPort: 6379 type: NodePort selector: app: redis---apiVersion: apps/v1beta1kind: StatefulSetmetadata: name: redis namespace: sts-appspec: serviceName: "redis-headless" replicas: 6 template: metadata: labels: app: redis spec: terminationGracePeriodSeconds: 20# affinity:# podAntiAffinity:# preferredDuringSchedulingIgnoredDuringExecution:# - weight: 100# podAffinityTerm:# labelSelector:# matchExpressions:# - key: app# operator: In# values:# - redis# topologyKey: kubernetes.io/hostname containers: - name: redis image: registry.cn-shenzhen.aliyuncs.com/pyker/redis:4.0.11 command: ["/etc/redis/fix-ip.sh", "redis-server", "/etc/redis/redis.conf"] readinessProbe:# exec:# command: ["/bin/sh", "-c", "redis-cli -h $(hostname) ping"] tcpSocket: port: 6379 timeoutSeconds: 5 periodSeconds: 3 initialDelaySeconds: 2 livenessProbe: tcpSocket: port: 6379 timeoutSeconds: 5 periodSeconds: 3 initialDelaySeconds: 2 env: - name: POD_IP valueFrom: fieldRef: fieldPath: status.podIP resources: requests: memory: "1Gi" ports: - name: redis containerPort: 6379 protocol: TCP - name: cluster containerPort: 16379 protocol: TCP volumeMounts: - name: redis-conf mountPath: /etc/redis/ readOnly: false - name: redis-data mountPath: /data readOnly: false volumes: - name: redis-conf configMap: name: redis-conf-cluster items: - key: redis.conf path: redis.conf - key: fix-ip.sh path: fix-ip.sh defaultMode: 0755 volumeClaimTemplates: - metadata: name: redis-data spec: accessModes: - ReadWriteOnce storageClassName: nfs resources: requests: storage: 2Gi 应用配置12$ kubectl apply -f redis-configmap.yaml$ kubectl apply -f redis-statefulset.yaml 稍等片刻查看redis是否已经运行。12345678kubectl get pod -n sts-appNAME READY STATUS RESTARTS AGEredis-0 1/1 Running 0 1mredis-1 1/1 Running 0 1mredis-2 1/1 Running 0 1mredis-3 1/1 Running 0 1mredis-4 1/1 Running 0 1mredis-5 1/1 Running 0 1m 初始化redis集群如上redis 实例已经全部处于Running状态了。因此我们可以来初始化集群了，如下步骤：12345678910# 随便选一台redis pod实例进去初始化集群。这里选redis-0$ $ kubectl exec -it redis-0 -n sts-app -- bash#初始化集群[root@redis-0]# redis-trib.rb create --replicas 1 \`dig +short redis-0.redis-headless.sts-app.svc.cluster.local`:6379 \`dig +short redis-1.redis-headless.sts-app.svc.cluster.local`:6379 \`dig +short redis-2.redis-headless.sts-app.svc.cluster.local`:6379 \`dig +short redis-3.redis-headless.sts-app.svc.cluster.local`:6379 \`dig +short redis-4.redis-headless.sts-app.svc.cluster.local`:6379 \`dig +short redis-5.redis-headless.sts-app.svc.cluster.local`:6379 redis-trib.rb必须使用ip进行初始化redis集群，使用域名会报如下错误！因此这里使用dig +short获取pod IP。1/var/lib/gems/2.3.0/gems/redis-4.1.2/lib/redis/client.rb:126:in `call’: ERR Invalid node address specified: redis-0.redis-headless.sts-app.svc.cluster.local:6379 (Redis::CommandError) 附上Dockerfile123456789$ cat DockerfileFROM redis:4.0.11RUN apt-get update -yRUN apt-get install -y ruby \rubygemsRUN apt-get clean allRUN gem install redisRUN apt-get install dnsutils -yCOPY redis-trib.rb /usr/local/bin/ 1$ chmod +x redis-trib.rb redis-trib.rb工具可以去redis源码中拷贝一个到当前目录,然后构建镜像。]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用NFS为kubernetes提供动态StorageClass]]></title>
    <url>%2F2019%2F07%2F15%2Fk8s-nfs.html</url>
    <content type="text"><![CDATA[PV和PVC简介PersistentVolume（PV）是指由集群管理员配置提供的某存储系统上的段存储空间，它是对底层共享存储的抽象，将共享存储作为种可由用户申请使的资源，实现了“存储消费”机制。通过存储插件机制，PV支持使用多种网络存储系统或云端存储等多种后端存储系统。 PV是集群级别的资源，不属于任何名称空间，用户对PV资源的使需要通过PersistentVolumeClaim（PVC）提出的使申请（或称为声明）来完成绑定，PVC是PV资源的消费者，它向PV申请特定大小的空间及访问模式（如rw或ro）。 从创建出PVC存储卷请求后再由Pod资源通过PVC持久卷请求来关联PV存储卷使用。 StorageClass通俗的讲它就是将存储资源定义为具有显著特性的类（Class）而不是具体的PV，用户通过PVC直接向意向的存储类发出申请，然后由其按需为用户动态创建PV（也可以由管理员事先创建的PV），这样做甚至免去了需要先创建PV的过程。 PV对存储系统的支持可通过其插件来实现，目前，Kubernetes支持如下类型的插件。官方地址， 而这里我们使用NFS来搭建StorageClass。 环境说明 系统 IP 应用 centos7.5 192.168.3.125 nfs服务器 centos7.5 192.168.3.120 k8s集群master1 centos7.5 192.168.3.121 k8s集群master2 centos7.5 192.168.3.122 k8s集群master3 我们这里k8s集群没有node节点，master节点的不被调度的污点(Taints) 被删掉了。kubectl edit node master1 安装NFS首先我们得有个NFS服务器，不然就不存在使用NFS服务来做存储类这一说法。 安装软件包1$ yum -y install nfs-utils rpcbind nfs-utils软件包请在所有k8s集群上安装，因为StorageClass需要挂载NFS共享目录。 配置共享目录 1234# 这里使用/data/nfs作为共享目录$ cat &gt;&gt; /etc/exports &lt;&lt; EOF/data/nfs *(rw,no_root_squash)EOF 启动NFS服务 12$ systemctl start rpcbind$ systemctl nfs start 测试NFS是否可用 12# 在同网络任意一台装了nfs-utils软件包服务器上测试挂载$ mount -t 192.168.3.125:/data/nfs /mnt 如果一切正常，说明NFS服务正常！此时才能进行后面的操作！ NFS内核性能优化Linux nfs客户端对于同时发起的NFS请求数量进行了控制，若该参数配置较小会导致IO性能较差，请查看该参数：cat /proc/sys/sunrpc/tcp_slot_table_entries默认编译的内核该参数最大值为256，可适当提高该参数的值来取得较好的性能，请以root身份执行以下命令：123$ echo "options sunrpc tcp_slot_table_entries=128" &gt;&gt; /etc/modprobe.d/sunrpc.conf$ echo "options sunrpc tcp_max_slot_table_entries=128" &gt;&gt; /etc/modprobe.d/sunrpc.conf$ sysctl -w sunrpc.tcp_slot_table_entries=128 创建StorageClass RBAC我这里是使用了nfs的名称空间，也可以使用别的或者删掉，使用默认的名称空间。如果要使用名称空间需要先创建。1$ kubectl create ns nfs 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970$ cat rbac.yamlapiVersion: v1kind: ServiceAccountmetadata: name: nfs-provisioner namespace: nfs---kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1metadata: name: nfs-provisioner-runnerrules: - apiGroups: [""] resources: ["persistentvolumes"] verbs: ["get", "list", "watch", "create", "delete"] - apiGroups: [""] resources: ["persistentvolumeclaims"] verbs: ["get", "list", "watch", "update"] - apiGroups: ["storage.k8s.io"] resources: ["storageclasses"] verbs: ["get", "list", "watch"] - apiGroups: [""] resources: ["events"] verbs: ["create", "update", "patch"] - apiGroups: [""] resources: ["services", "endpoints"] verbs: ["get"] - apiGroups: ["extensions"] resources: ["podsecuritypolicies"] resourceNames: ["nfs-provisioner"] verbs: ["use"]---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: run-nfs-provisionersubjects: - kind: ServiceAccount name: nfs-provisioner # replace with namespace where provisioner is deployed namespace: nfsroleRef: kind: ClusterRole name: nfs-provisioner-runner apiGroup: rbac.authorization.k8s.io---kind: RoleapiVersion: rbac.authorization.k8s.io/v1metadata: name: leader-locking-nfs-provisioner namespace: nfsrules: - apiGroups: [""] resources: ["endpoints"] verbs: ["get", "list", "watch", "create", "update", "patch"]---kind: RoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: leader-locking-nfs-provisioner namespace: nfssubjects: - kind: ServiceAccount name: nfs-provisioner # replace with namespace where provisioner is deployed namespace: nfsroleRef: kind: Role name: leader-locking-nfs-provisioner apiGroup: rbac.authorization.k8s.io 123456$ kubectl apply -f rbac.yaml # 应用rbac.yamlserviceaccount/nfs-provisioner createdclusterrole.rbac.authorization.k8s.io/nfs-provisioner-runner createdclusterrolebinding.rbac.authorization.k8s.io/run-nfs-provisioner createdrole.rbac.authorization.k8s.io/leader-locking-nfs-provisioner createdrolebinding.rbac.authorization.k8s.io/leader-locking-nfs-provisioner created 配置deployment文件12345678910111213141516171819202122232425262728293031323334353637$ cat deployment.yaml kind: DeploymentapiVersion: apps/v1metadata: name: nfs-provisioner namespace: nfsspec: replicas: 1 selector: matchLabels: app: nfs-provisioner strategy: type: Recreate template: metadata: labels: app: nfs-provisioner spec: serviceAccount: nfs-provisioner containers: - name: nfs-provisioner image: registry.cn-shenzhen.aliyuncs.com/pyker/nfs-client-provisioner:latest volumeMounts: - name: nfs-client-root mountPath: /persistentvolumes env: - name: PROVISIONER_NAME value: example.com/nfs - name: NFS_SERVER value: 192.168.3.125 # NFS服务器地址 - name: NFS_PATH value: /data/nfs/sc # 需要挂载的路径目录 volumes: - name: nfs-client-root nfs: server: 192.168.3.125 # NFS服务器地址 path: /data/nfs/sc # 需要挂载的路径目录 12$ kubectl apply -f deployment.yaml # 应用deployment.yamldeployment.apps/nfs-provisioner created 创建StorageClass12345678$ cat storageclass.yamlkind: StorageClassapiVersion: storage.k8s.io/v1metadata: name: nfsprovisioner: example.com/nfs#mountOptions:# - vers=4.1 12$ kubectl apply -f storageclass.yaml # 应用storageclass.yamlstorageclass.storage.k8s.io/nfs created 以上创建完成后，可以查看我们刚才新创建的资源信息如下：1234567$ kubectl get deploy -n nfsNAME READY UP-TO-DATE AVAILABLE AGEnfs-provisioner 1/1 1 1 67m # READY为1/1正常$ kubectl get sc -n nfsNAME PROVISIONER AGEnfs example.com/nfs 66m 确认服务正常后，我们可以进行来先进行测试一下NFS的动态存储功能是否正常。 测试PVC动态创建PV手动编写一个PVC来测试一下，注意：storageClassName应确保与上面创建的StorageClass名称一致。123456789101112$ cat test-claim.yamlkind: PersistentVolumeClaimapiVersion: v1metadata: name: test-claim1spec: accessModes: - ReadWriteMany resources: requests: storage: 1Mi storageClassName: nfs 12$ kubectl apply -f test-claim.yaml persistentvolumeclaim/test-claim1 created 1234567$ kubectl get pvcNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEtest-claim1 Bound pvc-d0a0a29b-ab06-11e9-96e4-080027f62143 1Mi RWX nfs 9s$ kubectl get pvNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEpvc-d0a0a29b-ab06-11e9-96e4-080027f62143 1Mi RWX Delete Bound default/test-claim1 nfs 21s 从上面可以看到PVC已经成功的动态的创建了一个PV存储卷并且与之绑定，此时我们到NFS服务器上去看一下，也可以发现已经自动生成了一个存储类的文件夹。12$ ls /data/nfs/sc/default-test-claim1-pvc-d0a0a29b-ab06-11e9-96e4-080027f62143 创建StatefulSet实例演示12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061$ cat sts.yamlapiVersion: v1kind: Servicemetadata: name: nginx labels: app: nginxspec: clusterIP: None selector: app: nginx---apiVersion: v1kind: Servicemetadata: name: nginx-access labels: app: nginxspec: type: NodePort ports: - name: http port: 80 targetPort: web selector: app: nginx---apiVersion: apps/v1kind: StatefulSetmetadata: name: nginx-webspec: selector: matchLabels: app: nginx serviceName: "nginx" replicas: 3 template: metadata: labels: app: nginx spec: terminationGracePeriodSeconds: 10 containers: - name: nginx image: nginx ports: - containerPort: 80 name: web volumeMounts: - name: nginx-data mountPath: /usr/share/nginx/html volumeClaimTemplates: - metadata: name: nginx-data spec: accessModes: [ "ReadWriteOnce" ] storageClassName: "nfs" resources: requests: storage: 1Gi 123$ kubectl apply -f sts.yamlservice/nginx createdstatefulset.apps/nginx-web created 查看3个有状态的nginx是否启动, 也可以发现已经启动完成。12345678$ kubectl get pod,stsNAME READY STATUS RESTARTS AGEnginx-web-0 1/1 Running 0 70mnginx-web-1 1/1 Running 0 70mnginx-web-2 1/1 Running 0 68mNAME READY AGEstatefulset.apps/nginx-web 3/3 71m 然后我们在查看一下PVC和PV的创建状态123456789101112$ kubectl get pvc,pvNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEpersistentvolumeclaim/nginx-data-nginx-web-0 Bound pvc-2c8ec903-ab08-11e9-96e4-080027f62143 1Gi RWO nfs 71mpersistentvolumeclaim/nginx-data-nginx-web-1 Bound pvc-34ce5093-ab08-11e9-96e4-080027f62143 1Gi RWO nfs 71mpersistentvolumeclaim/nginx-data-nginx-web-2 Bound pvc-6e4464ea-ab08-11e9-96e4-080027f62143 1Gi RWO nfs 70mpersistentvolumeclaim/test-claim1 Bound pvc-d0a0a29b-ab06-11e9-96e4-080027f62143 1Mi RWX nfs 81mNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEpersistentvolume/pvc-2c8ec903-ab08-11e9-96e4-080027f62143 1Gi RWO Delete Bound default/nginx-data-nginx-web-0 nfs 71mpersistentvolume/pvc-34ce5093-ab08-11e9-96e4-080027f62143 1Gi RWO Delete Bound default/nginx-data-nginx-web-1 nfs 71mpersistentvolume/pvc-6e4464ea-ab08-11e9-96e4-080027f62143 1Gi RWO Delete Bound default/nginx-data-nginx-web-2 nfs 70mpersistentvolume/pvc-d0a0a29b-ab06-11e9-96e4-080027f62143 1Mi RWX Delete Bound default/test-claim1 nfs 81m 由于statefulset是有状态的，所以他会为每个pod独立分配PVC,PV，且该PV只能该POD使用，所以3个副本pod会生成3个PVC和PV。 验证数据存储上面我们已经搭建好了nfs类型的StorageClass了，也成功启动了。现在我们来测试一下数据是否可以存储到NFS。我们进入刚刚创建的nginx-web-0容器进行操作。12345$ kubectl exec -it nginx-web-0 -- bashroot@nginx-web-0:/# cd /usr/share/nginx/html/root@nginx-web-0:/# cat &gt;&gt; index.html &lt;&lt; EOF&gt; hello this is my new pages!EOF 然后我们到NFS服务器的/data/nfs/sc目录上查看有关nginx-web-0目录的存储目录内容。 也可以发现我们刚才在容器里新建index.html的操作确实存放在NFS上了。12$ cat /data/nfs/sc/default-nginx-data-nginx-web-0-pvc-2c8ec903-ab08-11e9-96e4-080027f62143/index.html hello this is my new pages!]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mongodb4.0分片集群安装]]></title>
    <url>%2F2019%2F07%2F07%2Finstall-mongodb.html</url>
    <content type="text"><![CDATA[mongodb集群概念介绍mongodb支持架构有单机(stand-alone)、主从(master-slave)、副本集(replica set)以及分片(sharding), 而最常用的架构莫过于副本集 + 分片。而分片有三大组件，分别为mongos、configsvr、sharding server，他们的功能如下： mongos: 它是前端路由，应用程序由此接入，且让整个集群看上去像单一数据库，前端应用可以透明使用。它不存储任何数据，只载入configsvr的配置。 configsvr: 它是一个mongod 实例，存储了整个 Cluster Metadata，其中包括 chunk 信息。它是存放着分片和数据的对应关系。 sharding server: 它是一个mongod 实例，用于存储实际的数据块，实际生产环境中一个shard server角色可由几台机器组个一个replica set承担，防止主机单点故障。 安装环境本次试验有三台主机，即三个分片和三个副本集，可以保证高可用，即使一台机器全宕机了，服务仍然能够正常访问, 本次环境的主机分别充当另外一个分片的从，他们的角色功能如下： 角色后面的数字为该服务在该主机上占用的端口 主机 IP 角色 mongo1 192.168.3.125 mongos1:20000configsvr1:21000shard1:27001shard2:27002shard3:27003 mongo2 192.168.3.126 mongos2:20000configsvr2:21000shard1:27001shard2:27002shard3:27003 mongo3 192.168.3.127 configsvr3:21000shard1:27001shard2:27002shard3:27003 试验架构本次试验环境的架构图如下： 环境准备 环境准备的操作在所有主机上执行，除非另有说明。 创建mongod用户1$ useradd mongod 下载mongodb二进制文件123$ wget https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-rhel70-4.0.10.tgz$ tar zxvf mongodb-linux-x86_64-rhel70-4.0.10.tgz$ cp mongodb-linux-x86_64-rhel70-4.0.10/bin/* /usr/bin 准备环境目录123$ mkdir -pv /usr/local/mongodb/conf # 用于存放集群配置文件$ mkdir -pv /data/mongos/log # 用于存放mongos日志文件，它本身不存在数据，只是路由$ mkdir -pv /data/&#123;config,shard1,shard2,shard3&#125;/&#123;data,log&#125; # configsvr和3个分片的数据目录、日志目录 关闭Transparent HugePages123456789101112131415$ cat &gt; /etc/systemd/system/thpset.service &lt;&lt; EOF[Unit]Description="thp set"[Service]User=rootPermissionsStartOnly=trueType=oneshotExecStart=/bin/bash -c 'echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled'ExecStart=/bin/bash -c 'echo never &gt; /sys/kernel/mm/transparent_hugepage/defrag'[Install]WantedBy=multi-user.targetEOF$ systemctl start thpset &amp;&amp; systemctl enable thpset 配置ConfigSvr分别在mongo1、mongo2、mongo3上配置，要修改的地方只有bindIp为本机IP即可：12345678910111213141516171819202122232425262728293031323334$ cat &gt; /usr/local/mongodb/conf/config.conf &lt;&lt; EOF## contentsystemLog: destination: file logAppend: true path: /data/config/log/config.log# Where and how to store data.storage: dbPath: /data/config/data journal: enabled: true# how the process runsprocessManagement: fork: true pidFilePath: /data/config/log/configsrv.pid# network interfacesnet: port: 21000 bindIp: 192.168.3.125#operationProfiling:replication: replSetName: configsharding: clusterRole: configsvr# configure security#security:# authorization: enabled# keyFile: /usr/local/mongodb/keyfileEOF 分别在mongo1、mongo2、mongo3上配置systemctl文件： 使用numactl禁用numa需要安装numactl，yum install -y numactl 123456789101112131415$ cat &gt; /etc/systemd/system/mongo-config.service &lt;&lt; EOF[Unit]Description=High-performance, schema-free document-oriented databaseAfter=network.target[Service]User=mongodType=forkingExecStart=/usr/bin/numactl --interleave=all /usr/bin/mongod --config /usr/local/mongodb/conf/config.confExecReload=/bin/kill -s HUP $MAINPIDExecStop=/usr/bin/mongod --shutdown --config /usr/local/mongodb/conf/config.conf[Install]WantedBy=multi-user.targetEOF 分别启动三台服务器的config server12$ chown -R mongod /data/ &amp;&amp; chown -R mongod /usr/local/mongodb/$ systemctl start mongo-config &amp;&amp; systemctl enable mongo-config 登录任意一台配置服务器，初始化配置副本集1234567891011$ mongo 192.168.3.125:21000&gt; rs.initiate( &#123; _id : "config", members : [ &#123;_id : 0, host : "192.168.3.125:21000" &#125;, &#123;_id : 1, host : "192.168.3.126:21000" &#125;, &#123;_id : 2, host : "192.168.3.127:21000" &#125; ] &#125;)&gt; rs.status(); # 查看当前configsvr副本集状态 其中，&quot;_id&quot; : &quot;configs&quot;应与配置文件中配置的 replicaction.replSetName 一致，”members” 中的 “host” 为三个节点的ip和port。 配置分片、副本集配置mongo1上的3个分片副本集：1234567891011121314151617181920212223242526272829303132333435$ vi /usr/local/mongodb/conf/shard1.conf# where to write logging data.systemLog: destination: file logAppend: true path: /data/shard1/log/shard1.log# Where and how to store data.storage: dbPath: /data/shard1/data journal: enabled: true wiredTiger: engineConfig: cacheSizeGB: 20# how the process runsprocessManagement: fork: true pidFilePath: /data/shard1/log/shard1.pid# network interfacesnet: port: 27001 bindIp: 192.168.3.125#operationProfiling:replication: replSetName: shard1sharding: clusterRole: shardsvr# configure security#security:# authorization: enabled# keyFile: /usr/local/mongodb/keyfile 1234567891011121314151617181920212223242526272829303132333435$ vi /usr/local/mongodb/conf/shard2.conf# where to write logging data.systemLog: destination: file logAppend: true path: /data/shard2/log/shard2.log# Where and how to store data.storage: dbPath: /data/shard2/data journal: enabled: true wiredTiger: engineConfig: cacheSizeGB: 20# how the process runsprocessManagement: fork: true pidFilePath: /data/shard2/log/shard2.pid# network interfacesnet: port: 27002 bindIp: 192.168.3.125#operationProfiling:replication: replSetName: shard2sharding: clusterRole: shardsvr# configure security#security:# authorization: enabled# keyFile: /usr/local/mongodb/keyfile 123456789101112131415161718192021222324252627282930313233343536$ vi /usr/local/mongodb/conf/shard3.conf# where to write logging data.systemLog: destination: file logAppend: true path: /data/shard3/log/shard3.log# Where and how to store data.storage: dbPath: /data/shard3/data journal: enabled: true wiredTiger: engineConfig: cacheSizeGB: 20# how the process runsprocessManagement: fork: true pidFilePath: /data/shard3/log/shard3.pid# network interfacesnet: port: 27003 bindIp: 192.168.3.125#operationProfiling:replication: replSetName: shard3sharding: clusterRole: shardsvr# configure security#security:# authorization: enabled# keyFile: /usr/local/mongodb/keyfile 复制mongo1节点上的shard1.conf、shard2.conf、shard3.conf到mongo2和mongo3主机上。要改动的地方只有bindIp，修改为本机IP即可。 分别在mongo1、mongo2、mongo3上配置分片副本的systemctl文件：123456789101112131415$ cat &gt; /etc/systemd/system/mongo-27001.service &lt;&lt; EOF[Unit]Description=High-performance, schema-free document-oriented databaseAfter=network.target[Service]User=mongodType=forkingExecStart=/usr/bin/numactl --interleave=all /usr/bin/mongod --config /usr/local/mongodb/conf/shard1.confExecReload=/bin/kill -s HUP $MAINPIDExecStop=/usr/bin/mongod --shutdown --config /usr/local/mongodb/conf/shard1.conf[Install]WantedBy=multi-user.targetEOF 123456789101112131415$ cat &gt; /etc/systemd/system/mongo-27002.service &lt;&lt; EOF[Unit]Description=High-performance, schema-free document-oriented databaseAfter=network.target[Service]User=mongodType=forkingExecStart=/usr/bin/numactl --interleave=all /usr/bin/mongod --config /usr/local/mongodb/conf/shard2.confExecReload=/bin/kill -s HUP $MAINPIDExecStop=/usr/bin/mongod --shutdown --config /usr/local/mongodb/conf/shard2.conf[Install]WantedBy=multi-user.targetEOF 123456789101112131415$ cat &gt; /etc/systemd/system/mongo-27003.service &lt;&lt; EOF[Unit]Description=High-performance, schema-free document-oriented databaseAfter=network.target[Service]User=mongodType=forkingExecStart=/usr/bin/numactl --interleave=all /usr/bin/mongod --config /usr/local/mongodb/conf/shard3.confExecReload=/bin/kill -s HUP $MAINPIDExecStop=/usr/bin/mongod --shutdown --config /usr/local/mongodb/conf/shard3.conf[Install]WantedBy=multi-user.targetEOF 分别启动三台服务器的分片副本1234$ chown -R mongod /data/ &amp;&amp; chown -R mongod /usr/local/mongodb/$ systemctl start mongo-27001 &amp;&amp; systemctl enable mongo-27001$ systemctl start mongo-27002 &amp;&amp; systemctl enable mongo-27002$ systemctl start mongo-27003 &amp;&amp; systemctl enable mongo-27003 登录任意一台配置服务器，初始化各个配置副本集1234567891011$ mongo 192.168.3.125:27001&gt; rs.initiate( &#123; _id : "shard1", members : [ &#123;_id : 0, host : "192.168.3.125:27001" &#125;, &#123;_id : 1, host : "192.168.3.126:27001" &#125;, &#123;_id : 2, host : "192.168.3.127:27001" &#125; ] &#125;)&gt; rs.status(); # 查看当前shard1副本集状态 123456789101112$ mongo 192.168.3.125:27002&gt; rs.initiate( &#123; _id : "shard2", members : [ &#123;_id : 0, host : "192.168.3.125:27002" &#125;, &#123;_id : 1, host : "192.168.3.126:27002" &#125;, &#123;_id : 2, host : "192.168.3.127:27002" &#125; ] &#125;)&gt; rs.status(); # 查看当前shard2副本集状态 1234567891011$ mongo 192.168.3.125:27003&gt; rs.initiate( &#123; _id : "shard3", members : [ &#123;_id : 0, host : "192.168.3.125:27003" &#125;, &#123;_id : 1, host : "192.168.3.126:27003" &#125;, &#123;_id : 2, host : "192.168.3.127:27003" &#125; ] &#125;)&gt; rs.status(); # 查看当前shard3副本集状态 如果1分片只需要2副本1仲裁的话，只需要在rs.initiate命令后对应的host加上, arbiterOnly: true即可让该主机称为仲裁节点。 配置mongosmongos这里只配置在mongo1和mongo2上，所以只要在这两台主机上配置即可。 注意：先启动配置服务器和分片服务器,后启动路由实例 分别在mongo1、mongo2上配置，要修改的地方只有bindIp为本机IP即可：1234567891011121314151617181920$ vi /usr/local/mongodb/conf/mongos.confsystemLog: destination: file logAppend: true path: /data/mongos/log/mongos.logprocessManagement: fork: true pidFilePath: /data/mongos/log/mongos.pid# network interfacesnet: port: 20000 bindIp: 192.168.3.125#监听的配置服务器,只能有1个或者3个 configs为配置服务器的副本集名字sharding: configDB: config/192.168.3.125:21000,192.168.3.126:21000,192.168.3.127:21000# configure security#security:# keyFile: /usr/local/mongodb/keyfile 分别在mongo1、mongo2上配置systemctl文件：1234567891011121314$ cat &gt; /etc/systemd/system/mongos.service &lt;&lt; EOF[Unit]Description=High-performance, schema-free document-oriented databaseAfter=network.target[Service]User=mongodType=forkingExecStart=/usr/bin/mongos --config /usr/local/mongodb/conf/mongos.confExecReload=/bin/kill -s HUP $MAINPID[Install]WantedBy=multi-user.targetEOF 分别启动mongo1和mongo2服务器上的mongos123$ chown -R mongod /data/ &amp;&amp; chown -R mongod /usr/local/mongodb/$ systemctl start mongos &amp;&amp; systemctl enable mongos$ systemctl start mongos &amp;&amp; systemctl enable mongos 至此已经搭建了mongodb配置服务器、路由服务器，各个分片服务器，不过应用程序连接到mongos路由服务器并不能使用分片机制，还需要在程序里设置分片配置，让分片生效。登录任意一台mongos，设置分片配置。12345$ mongo 192.168.3.125:20000&gt; sh.addShard("shard1/192.168.3.125:27001,192.168.3.126:27001,192.168.3.127:27001")&gt; sh.addShard("shard2/192.168.3.125:27002,192.168.3.126:27002,192.168.3.127:27002")&gt; sh.addShard("shard3/192.168.3.125:27003,192.168.3.126:27003,192.168.3.127:27003")&gt; sh.status() # 查看集群状态 创建管理员帐号登陆任意一台mongos，建立管理员账号，赋所有权限（admin和config数据库）123456$ mongo 192.168.3.125:20000&gt; use admin&gt; db.createUser(&#123;user: "admin",pwd: "123456",roles: [ &#123; role: "root", db: "admin" &#125; ]&#125;) # root所有权限&gt; use config&gt; db.createUser(&#123;user: "admin",pwd: "123456",roles: [ &#123; role: "root", db: "admin" &#125; ]&#125;) # root所有权限&gt; db.auth("admin","123456") # 登陆认证，返回1为验证成功 123456$ netstat -ptln | grep mongotcp 0 0 192.168.3.125:21000 0.0.0.0:* LISTEN 3277/mongod tcp 0 0 192.168.3.125:27001 0.0.0.0:* LISTEN 3974/mongod tcp 0 0 192.168.3.125:27002 0.0.0.0:* LISTEN 3281/mongod tcp 0 0 192.168.3.125:27003 0.0.0.0:* LISTEN 3280/mongod tcp 0 0 192.168.3.125:20000 0.0.0.0:* LISTEN 3251/mongos 查看集群状态登陆任意一台mongos查看集群状态12345678910111213141516171819202122232425$ mongo 192.168.3.125:20000mongos&gt; use adminmongos&gt; db.auth("admin","123456") # 这里要认证了。不然看不到集群信息mongos&gt; sh.status();--- Sharding Status --- sharding version: &#123; "_id" : 1, "minCompatibleVersion" : 5, "currentVersion" : 6, "clusterId" : ObjectId("5d208c95fdd1bee8ab631a40") &#125; shards: &#123; "_id" : "shard1", "host" : "shard1/192.168.3.125:27001,192.168.3.126:27001,192.168.3.127:27001", "state" : 1 &#125; &#123; "_id" : "shard2", "host" : "shard2/192.168.3.125:27002,192.168.3.126:27002,192.168.3.127:27002", "state" : 1 &#125; &#123; "_id" : "shard3", "host" : "shard3/192.168.3.125:27003,192.168.3.126:27003,192.168.3.127:27003", "state" : 1 &#125; active mongoses: "4.0.10" : 2 autosplit: Currently enabled: yes balancer: Currently enabled: yes Currently running: no Failed balancer rounds in last 5 attempts: 0 Migration Results for the last 24 hours: No recent migrations 这样mongodb如架构图所示的集群搭建就已经完成了。 集群认证文件在分片集群环境中，建议副本集内成员之间需要用keyFile认证或者509.x证书认证，mongos与配置服务器，副本集之间也要keyFile认证，集群所有mongod和mongos实例使用内容相同的keyFile文件，我们这里只在mongo1上生成，然后复制到其他节点上。1234$ openssl rand -base64 753 &gt; /usr/local/mongodb/keyfile$ chmod 400 /usr/local/mongodb/keyfile$ scp /usr/local/mongodb/keyfile mongo2:/usr/local/mongodb/keyfile$ scp /usr/local/mongodb/keyfile mongo3:/usr/local/mongodb/keyfile 然后修改3个configsrv、3个shard、2个mongos实例的配置文件。 configsrv 和 shard增加如下配置： 1234# configure securitysecurity: authorization: enabled keyFile: /usr/local/mongodb/keyfile mongos增加如下配置： 12security: keyFile: /usr/local/mongodb/keyfile mongos比mongod少了authorization：enabled的配置。原因是，副本集加分片的安全认证需要配置两方面的，副本集各个节点之间使用内部身份验证，用于内部各个mongo实例的通信，只有相同keyfile才能相互访问。所以都要开启keyFile: /usr/local/mongodb/keyfile, 然而对于所有的mongod，才是真正的保存数据的分片。mongos只做路由，不保存数据。所以所有的mongod开启访问数据的授权authorization:enabled。这样用户只有账号密码正确才能访问到数据。 重启集群我们已经配置了用户帐号密码连接集群以及集群内部通过keyfile通信，为了使配置生效，需要重启整个集群，启动的顺序为先启动configsrv，在启动分片，最后启动mongos。 为什么我们要先创建管理员帐号在配置集群内部通信认证？ 账号可以在集群认开启认证以后添加。但是那时候添加比较谨慎。只能添加一次，如果忘记了就无法再连接到集群。所以才建议在没开启集群认证的时候先添加好管理员用户名和密码然后再开启认证再重启集群。 插入数据验证分片副本在案例中，创建appuser用户、为数据库实例appdb启动分片。1234567891011$ mongo 192.168.3.125:20000# 创建appuser用户mongos&gt; use appdbmongos&gt; db.createUser(&#123;user:'appuser',pwd:'AppUser@01',roles:[&#123;role:'dbOwner',db:'appdb'&#125;]&#125;)mongos&gt; sh.enableSharding("appdb")# 创建集合book，为其执行分片初始化mongos&gt; use appdbmongos&gt; db.createCollection("book")mongos&gt; db.device.ensureIndex(&#123;createTime:1&#125;)mongos&gt; sh.shardCollection("appdb.book", &#123;bookId:"hashed"&#125;, false, &#123; numInitialChunks: 4&#125; ) 继续往device集合写入1000W条记录，观察chunks的分布情况!123456789101112131415161718192021222324mongos&gt; use appdbmongos&gt; var cnt = 0;mongos&gt; for(var i=0; i&lt;1000; i++)&#123; var dl = []; for(var j=0; j&lt;100; j++)&#123; dl.push(&#123; "bookId" : "BBK-" + i + "-" + j, "type" : "Revision", "version" : "IricSoneVB0001", "title" : "Jackson's Life", "subCount" : 10, "location" : "China CN Shenzhen Futian District", "author" : &#123; "name" : 50, "email" : "RichardFoo@yahoo.com", "gender" : "female" &#125;, "createTime" : new Date() &#125;); &#125; cnt += dl.length; db.book.insertMany(dl); print("insert ", cnt);&#125; 执行db.book.getShardDistribution(),输出如下：123456789101112131415161718192021mongos&gt; db.book.getShardDistribution()Shard shard2 at shard2/192.168.12.22:27002,192.168.12.25:27002 data : 6.62MiB docs : 24641 chunks : 1 estimated data per chunk : 6.62MiB estimated docs per chunk : 24641Shard shard1 at shard1/192.168.12.21:27001,192.168.12.24:27001 data : 13.51MiB docs : 50294 chunks : 2 estimated data per chunk : 6.75MiB estimated docs per chunk : 25147Shard shard3 at shard3/192.168.12.23:27003,192.168.12.26:27003 data : 6.73MiB docs : 25065 chunks : 1 estimated data per chunk : 6.73MiB estimated docs per chunk : 25065Totals data : 26.87MiB docs : 100000 chunks : 4 Shard shard2 contains 24.64% data, 24.64% docs in cluster, avg obj size on shard : 281B Shard shard1 contains 50.29% data, 50.29% docs in cluster, avg obj size on shard : 281B Shard shard3 contains 25.06% data, 25.06% docs in cluster, avg obj size on shard : 281B 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455mongos&gt; mongos&gt; db.book.stats();&#123; "sharded" : true, "capped" : false, "wiredTiger" : &#123; "metadata" : &#123; "formatVersion" : 1 &#125;, "ns" : "appdb.book", "count" : 100000, "size" : 28179000, "storageSize" : 4239360, "totalIndexSize" : 2957312, "indexSizes" : &#123; "_id_" : 999424, "bookId_hashed" : 1957888 &#125;, "avgObjSize" : 281, "maxSize" : NumberLong(0), "nindexes" : 2, "nchunks" : 4, "shards" : &#123; "shard1" : &#123; "ns" : "appdb.book", "size" : 14172376, "count" : 50294, "avgObjSize" : 281, "storageSize" : 2129920, "capped" : false, "wiredTiger" : &#123; "metadata" : &#123; "formatVersion" : 1 &#125;, "shard3" : &#123; "ns" : "appdb.book", "size" : 7063001, "count" : 25065, "avgObjSize" : 281, "storageSize" : 1052672, "capped" : false, "wiredTiger" : &#123; "metadata" : &#123; "formatVersion" : 1 &#125;, "shard2" : &#123; "ns" : "appdb.book", "size" : 6943623, "count" : 24641, "avgObjSize" : 281, "storageSize" : 1056768, "capped" : false, "wiredTiger" : &#123; "metadata" : &#123; "formatVersion" : 1 &#125;, 可以看到数据分到3个分片，各自分片数量为： shard1 “count”: 50294，shard2 “count”: 24641，shard3 “count”: 25065 。加起来为100000，数据分片已经成功了！]]></content>
      <categories>
        <category>SQL</category>
      </categories>
      <tags>
        <tag>mongodb</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes安装dashboard]]></title>
    <url>%2F2019%2F06%2F16%2Fkubernetes-dashboard.html</url>
    <content type="text"><![CDATA[kubernetes dashboard介绍Dashboard是一个基于web的Kubernetes用户界面。您可以使用Dashboard将容器化应用程序部署到Kubernetes集群，对容器化应用程序进行故障诊断，并管理集群资源。您还可以使用Dashboard来获得运行在集群上的应用程序的概述以及创建或修改单个Kubernetes资源(如Deployment、Jobs、DaemonSets等)。例如，您可以扩展部署、启动滚动更新、重启pod或使用deploy向导部署新应用程序。 部署Dashboard默认情况下，kubernetes没有安装它，因此你需要运行以下命来安装: 注意：我们使用的是环境是上一篇文章的环境。 12# 下载dashboard的yaml文件$ wget https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml 正常情况你已经可以直接运行kubectl apply -f kubernetes-dashboard.yaml命令来运行dashboard了。如果你的node节点能科学上网下载dashboard镜像的话。但是为了我们正常使用的需要，我们还需要对该kubernetes-dashboard.yaml文件进行如下配置： 暴露端口 配置证书（否则火狐浏览器可以正常访问，其他浏览器将显示NET::ERR_CERT_INVALID，提示证书未找到） 暴露端口默认配置文件下，dashboard的service端口使用的cluster ip，并没有把端口暴露出去，所以只能在kubernetes集群内部访问，这明显不是我们想要的结果。因此我们修改配置文件，增加type: NodePort12345678910111213141516# ------------------- Dashboard Service ------------------- #kind: ServiceapiVersion: v1metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kube-systemspec: ports: - port: 443 targetPort: 8443 type: NodePort # 我们在此处加入type: NodePort selector: k8s-app: kubernetes-dashboard 当然如果你不使用NodePort暴露端口的话，我们还可以使用ingress-nginx进行暴露。目前我们这里使用NodePort方式。 配置证书当我们直接运行kubectl apply -f kubernetes-dashboard.yaml后，dashboard的pod就算已经是running状态了，我们也只能使用火狐浏览器访问，其他浏览器均访问不了，显示NET::ERR_CERT_INVALID证书未找到的错误，因此我们需要手动来生成有效的dashboard证书。 有些人可能会说kubernetes-dashboard.yml文件中不是有这个--auto-generate-certificates配置吗？ 它不就是自动生成dashboard证书的吗？但是他确实是没给我们生成，如下：123456789kubectl describe secret kubernetes-dashboard-certs -n kube-systemName: kubernetes-dashboard-certsNamespace: kube-systemLabels: k8s-app=kubernetes-dashboardAnnotations: Type: OpaqueData==== # 这里根本没有证书 可以看到这个secret为空，并没有证书，因此我们需要手动生成，并且卷的方式挂载进去。可以使用hostPath方式，nfs方式等等都行，我们这里使用hostPath方式。 为此我们开始生成证书。 创建dashboard证书123456$ mkdir -pv /etc/kubernetes/pki/dashboard &amp;&amp; cd /etc/kubernetes/pki/dashboard$ (umask 077;openssl genrsa -out dashboard.key 2048) # 创建dashboard私钥$ openssl req -new -key dashboard.key -out dashboard.csr -subj "/O=ipyker/CN=dashboard" # 创建证书请求$ openssl x509 -req -in dashboard.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out dashboard.crt -days 3650 # 使用kubernetes的ca证书对dashboard证书请求签署$ ls /etc/kubernetes/pki/dashboarddashboard.crt dashboard.csr dashboard.key 此时我们已经生成了dashboard的证书和密钥了，由于我们采用hostPath方式挂载证书，因此我们需要将该证书目录复制到所有node节点上。来避免当dashboard的pod重建到其他node上找不到证书的情况。123$ scp -r /etc/kubernetes/pki/dashboard/ node1:/etc/kubernetes/pki/$ scp -r /etc/kubernetes/pki/dashboard/ node2:/etc/kubernetes/pki/$ scp -r /etc/kubernetes/pki/dashboard/ node3:/etc/kubernetes/pki/ 我们以hostPath方式挂载卷配置如下：12345volumes:- name: kubernetes-dashboard-certs hostPath: path: /etc/kubernetes/pki/dashboard type: Directory 看了配置文件的你一定发现我们挂载卷的名称和sercet的名称怎么重复了？都是kubernetes-dashboard-certs。是的，我们这里因为使用hostPath挂载，所以系统默认的使用sercet方式挂载的配置都无效，我们需要把它都删掉。修改后完整的配置如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168# Copyright 2017 The Kubernetes Authors.## Licensed under the Apache License, Version 2.0 (the "License");# you may not use this file except in compliance with the License.# You may obtain a copy of the License at## http://www.apache.org/licenses/LICENSE-2.0## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an "AS IS" BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.# ------------------- Dashboard Secret ------------------- ##apiVersion: v1#kind: Secret#metadata:# labels:# k8s-app: kubernetes-dashboard# name: kubernetes-dashboard-certs# namespace: kube-system#type: Opaque#---# Dashboard Secret语句块我们全部注释掉，不使用secret挂载。# ------------------- Dashboard Service Account ------------------- #apiVersion: v1kind: ServiceAccountmetadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kube-system---# ------------------- Dashboard Role &amp; Role Binding ------------------- #kind: RoleapiVersion: rbac.authorization.k8s.io/v1metadata: name: kubernetes-dashboard-minimal namespace: kube-systemrules: # Allow Dashboard to create 'kubernetes-dashboard-key-holder' secret.- apiGroups: [""] resources: ["secrets"] verbs: ["create"] # Allow Dashboard to create 'kubernetes-dashboard-settings' config map.- apiGroups: [""] resources: ["configmaps"] verbs: ["create"] # Allow Dashboard to get, update and delete Dashboard exclusive secrets.- apiGroups: [""] resources: ["secrets"] resourceNames: ["kubernetes-dashboard-key-holder", "kubernetes-dashboard-certs"] verbs: ["get", "update", "delete"] # Allow Dashboard to get and update 'kubernetes-dashboard-settings' config map.- apiGroups: [""] resources: ["configmaps"] resourceNames: ["kubernetes-dashboard-settings"] verbs: ["get", "update"] # Allow Dashboard to get metrics from heapster.- apiGroups: [""] resources: ["services"] resourceNames: ["heapster"] verbs: ["proxy"]- apiGroups: [""] resources: ["services/proxy"] resourceNames: ["heapster", "http:heapster:", "https:heapster:"] verbs: ["get"]---apiVersion: rbac.authorization.k8s.io/v1kind: RoleBindingmetadata: name: kubernetes-dashboard-minimal namespace: kube-systemroleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: kubernetes-dashboard-minimalsubjects:- kind: ServiceAccount name: kubernetes-dashboard namespace: kube-system---# ------------------- Dashboard Deployment ------------------- #kind: DeploymentapiVersion: apps/v1metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kube-systemspec: replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: k8s-app: kubernetes-dashboard template: metadata: labels: k8s-app: kubernetes-dashboard spec: containers: - name: kubernetes-dashboard image: k8s.gcr.io/kubernetes-dashboard-amd64:v1.10.1 # 该镜像地址需要科学上网哦 ports: - containerPort: 8443 protocol: TCP args: - --auto-generate-certificates # Uncomment the following line to manually specify Kubernetes API server Host # If not specified, Dashboard will attempt to auto discover the API server and connect # to it. Uncomment only if the default does not work. # - --apiserver-host=http://my-address:port volumeMounts: - name: kubernetes-dashboard-certs mountPath: /certs # Create on-disk volume to store exec logs - mountPath: /tmp name: tmp-volume livenessProbe: httpGet: scheme: HTTPS path: / port: 8443 initialDelaySeconds: 30 timeoutSeconds: 30 volumes:# - name: kubernetes-dashboard-certs # secret方式挂载全部注释或删除# secret:# secretName: kubernetes-dashboard-certs - name: tmp-volume emptyDir: &#123;&#125; - name: kubernetes-dashboard-certs hostPath: # 此处是我们采用的hostPath方式挂载证书 path: /etc/kubernetes/pki/dashboard type: Directory serviceAccountName: kubernetes-dashboard # Comment the following tolerations if Dashboard must not be deployed on master tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule---# ------------------- Dashboard Service ------------------- #kind: ServiceapiVersion: v1metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kube-systemspec: ports: - port: 443 targetPort: 8443 type: NodePort selector: k8s-app: kubernetes-dashboard 此时我们再来运行以下命令来启动dashboard：1$ kubectl apply -f kubernetes-dashboard.yaml 如果你不能科学上网从k8s.gcr.io上下载dashboard镜像，请在所有node节点上运行以下命令手动下载。123$ docker pull registry.cn-shenzhen.aliyuncs.com/pyker/kubernetes-dashboard:v1.10.1$ docker tag registry.cn-shenzhen.aliyuncs.com/pyker/kubernetes-dashboard:v1.10.1 k8s.gcr.io/kubernetes-dashboard-amd64:v1.10.1$ docker rmi registry.cn-shenzhen.aliyuncs.com/pyker/kubernetes-dashboard:v1.10.1 访问dashboard当dashboard部署成功后，我们首先查看他的nodeport端口是多少。123$ kubectl get svc -n kube-system kubernetes-dashboardNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes-dashboard NodePort 10.101.56.113 &lt;none&gt; 443:30320/TCP 62m 现在你可以在局域网任意一台主机上的任意浏览器以https://nodeip:nodeport的方式访问该dashboard了。虽然会提示网站不安全是因为你的证书是自签的，并不是第三方机构签署的。 dashboard登陆认证dashboard默认有两种登陆验证的方式，分别为： Kubeconfig文件认证 token令牌认证 token令牌认证创建kubernetes-dashboard集群管理员角色1234567891011121314151617181920$ cat &gt; dashboard-admin.yaml &lt;&lt; EOFapiVersion: v1kind: ServiceAccountmetadata: name: dashboard-admin namespace: kube-system---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: dashboard-adminsubjects: - kind: ServiceAccount name: dashboard-admin namespace: kube-systemroleRef: kind: ClusterRole name: cluster-admin apiGroup: rbac.authorization.k8s.ioEOF 1$ kubectl apply -f dashboard-admin.yaml 查看serviceaccount生成的secret文件12$ kubectl get secret -n kube-system | grep dashboard-admindashboard-admin-token-h84v9 kubernetes.io/service-account-token 3 101m 1234567891011121314$ kubectl describe secret dashboard-admin-token-h84v9 -n kube-systemName: dashboard-admin-token-h84v9Namespace: kube-systemLabels: &lt;none&gt;Annotations: kubernetes.io/service-account.name: dashboard-admin kubernetes.io/service-account.uid: c03f5dec-9c05-11e9-aec0-000c29b438b3Type: kubernetes.io/service-account-tokenData====namespace: 11 bytestoken: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkYXNoYm9hcmQtYWRtaW4tdG9rZW4taDg0djkiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGFzaGJvYXJkLWFkbWluIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiYzAzZjVkZWMtOWMwNS0xMWU5LWFlYzAtMDAwYzI5YjQzOGIzIiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOmRhc2hib2FyZC1hZG1pbiJ9.JFxxIEltq8AiYdTTKxy-QfmR2l6lcyPNOuGlAGR8bXTjVfveCQFj8GEg2eyOJsrjwqgZRN_CwvSpoTjxD39Xv2nvTj-jvWONuLhfr8cHLnTpSgB1KC3oYc0k27w5S0jmZ0Xsimn2pIXAaqzvX3ndZCKW4FngXjRwq27N6pWvynCOuY_625OeDW0Nuonx2AMBXZuu3cqdkM3JSxCc1NDjlJWiTj2Na1-A6uKnG1BxjfGYG5cVTdt4BbBr1Xvom-ky8ts9fUODZozmRyDcJ8kiezx2KTGE4TVMDtotC4TrE0pfWwoo2UsCi0SFmvSA5Vvy9gFHTIi9PskmsYq5ejkn6Qca.crt: 1025 bytes 其中token就是我们以token令牌认证登陆的密钥信息，我们可以在dashboard输入该字符串进行登陆。 Kubeconfig文件认证Kubeconfig文件认证其实就是将token写入到类似~/.kube/config的文件中，以该文件进行登陆的。步骤如下： 获取base64加密的token 12$ kubectl get secret dashboard-admin-token-h84v9 -n kube-system -o jsonpath=&#123;.data.token&#125;ZXlKaGJHY2lPaUpTVXpJMU5pSXNJbXRwWkNJNklpSjkuZXlKcGMzTWlPaUpyZFdKbGNtNWxkR1Z6TDNObGNuWnBZMlZoWTJOdmRXNTBJaXdpYTNWaVpYSnVaWFJsY3k1cGJ5OXpaWEoyYVdObFlXTmpiM1Z1ZEM5dVlXMWxjM0JoWTJVaU9pSnJkV0psTFhONWMzUmxiU0lzSW10MVltVnlibVYwWlhNdWFXOHZjMlZ5ZG1salpXRmpZMjkxYm5RdmMyVmpjbVYwTG01aGJXVWlPaUprWVhOb1ltOWhjbVF0WVdSdGFXNHRkRzlyWlc0dGFEZzBkamtpTENKcmRXSmxjbTVsZEdWekxtbHZMM05sY25acFkyVmhZMk52ZFc1MEwzTmxjblpwWTJVdFlXTmpiM1Z1ZEM1dVlXMWxJam9pWkdGemFHSnZZWEprTFdGa2JXbHVJaXdpYTNWaVpYSnVaWFJsY3k1cGJ5OXpaWEoyYVdObFlXTmpiM1Z1ZEM5elpYSjJhV05sTFdGalkyOTFiblF1ZFdsa0lqb2lZekF6WmpWa1pXTXRPV013TlMweE1XVTVMV0ZsWXpBdE1EQXdZekk1WWpRek9HSXpJaXdpYzNWaUlqb2ljM2x6ZEdWdE9uTmxjblpwWTJWaFkyTnZkVzUwT210MVltVXRjM2x6ZEdWdE9tUmhjMmhpYjJGeVpDMWhaRzFwYmlKOS5KRnh4SUVsdHE4QWlZZFRUS3h5LVFmbVIybDZsY3lQTk91R2xBR1I4YlhUalZmdmVDUUZqOEdFZzJleU9Kc3Jqd3FnWlJOX0N3dlNwb1RqeEQzOVh2Mm52VGotanZXT051TGhmcjhjSExuVHBTZ0IxS0Mzb1ljMGsyN3c1UzBqbVowWHNpbW4ycElYQWFxenZYM25kWkNLVzRGbmdYalJ3cTI3TjZwV3Z5bkNPdVlfNjI1T2VEVzBOdW9ueDJBTUJYWnV1M2NxZGtNM0pTeENjMU5EamxKV2lUajJOYTEtQTZ1S25HMUJ4amZHWUc1Y1ZUZHQ0QmJCcjFYdm9tLWt5OHRzOWZVT0Rab3ptUnlEY0o4a2llengyS1RHRTRUVk1EdG90QzRUckUwcGZXd29vMlVzQ2kwU0ZtdlNBNVZ2eTlnRkhUSWk5UHNrbXNZcTVlamtuNlE= 解密获取到的加密tonken 12$ echo ZXlKaGJHY2lPaUpTVXpJMU5pSXNJbXRwWkNJNklpSjkuZXlKcGMzTWlPaUpyZFdKbGNtNWxkR1Z6TDNObGNuWnBZMlZoWTJOdmRXNTBJaXdpYTNWaVpYSnVaWFJsY3k1cGJ5OXpaWEoyYVdObFlXTmpiM1Z1ZEM5dVlXMMyVmpjbVYwTG01aGJXVWlPaUprWVhOb1ltOWhjbVF0WVdSdGFXNHRkRzlyWlc0dGFEZzBkamtpTENKcmRXSmxjbTVsZEdWekxtbHZMM05sY25acFkyVmhZMk52ZFc1MEwzTmxjblpwWTJVdFlXTmpiM1Z1ZEM1dVlXMWxJam9p1ZFdsa0lqb2lZekF6WmpWa1pXTXRPV013TlMweE1XVTVMV0ZsWXpBdE1EQXdZekk1WWpRek9HSXpJaXdpYzNWaUlqb2ljM2x6ZEdWdE9uTmxjblpwWTJWaFkyTnZkVzUwT210MVltVXRjM2x6ZEdWdE9tUmhjMmhpYjJGeVpDMdlNwb1RqeEQzOVh2Mm52VGotanZXT051TGhmcjhjSExuVHBTZ0IxS0Mzb1ljMGsyN3c1UzBqbVowWHNpbW4ycElYQWFxenZYM25kWkNLVzRGbmdYalJ3cTI3TjZwV3Z5bkNPdVlfNjI1T2VEVzBOdW9ueDJBTUJYWnV1M2NxZGTRUVk1EdG90QzRUckUwcGZXd29vMlVzQ2kwU0ZtdlNBNVZ2eTlnRkhUSWk5UHNrbXNZcTVlamtuNlE= | base64 -deyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkYXNoYm9hcmQtYWRtaW4tdG9rZW4taDg0djkiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGFzaGJvYXJkLWFkbWluIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiYzAzZjVkZWMtOWMwNS0xMWU5LWFlYzAtMDAwYzI5YjQzOGIzIiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOmRhc2hib2FyZC1hZG1pbiJ9.JFxxIEltq8AiYdTTKxy-QfmR2l6lcyPNOuGlAGR8bXTjVfveCQFj8GEg2eyOJsrjwqgZRN_CwvSpoTjxD39Xv2nvTj-jvWONuLhfr8cHLnTpSgB1KC3oYc0k27w5S0jmZ0Xsimn2pIXAaqzvX3ndZCKW4FngXjRwq27N6pWvynCOuY_625OeDW0Nuonx2AMBXZuu3cqdkM3JSxCc1NDjlJWiTj2Na1-A6uKnG1BxjfGYG5cVTdt4BbBr1Xvom-ky8ts9fUODZozmRyDcJ8kiezx2KTGE4TVMDtotC4TrE0pfWwoo2UsCi0SFmvSA5Vvy9gFHTIi9PskmsYq5ejkn6Q 配置dashboard-admin的集群信息 1$ kubectl config set-cluster kubernetes --certificate-authority=/etc/kubernetes/pki/ca.crt --server="https://192.168.20.222:6443" --embed-certs=true --kubeconfig=/root/dashboard-admin.conf 配置用户token信息 1$ kubectl config set-credentials dashboard-admin --token=eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkYXNoYm9hcmQtYWRtaW4tdG9rZW4taDg0djkiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGFzaGJvYXJkLWFkbWluIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiYzAzZjVkZWMtOWMwNS0xMWU5LWFlYzAtMDAwYzI5YjQzOGIzIiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOmRhc2hib2FyZC1hZG1pbiJ9.JFxxIEltq8AiYdTTKxy-QfmR2l6lcyPNOuGlAGR8bXTjVfveCQFj8GEg2eyOJsrjwqgZRN_CwvSpoTjxD39Xv2nvTj-jvWONuLhfr8cHLnTpSgB1KC3oYc0k27w5S0jmZ0Xsimn2pIXAaqzvX3ndZCKW4FngXjRwq27N6pWvynCOuY_625OeDW0Nuonx2AMBXZuu3cqdkM3JSxCc1NDjlJWiTj2Na1-A6uKnG1BxjfGYG5cVTdt4BbBr1Xvom-ky8ts9fUODZozmRyDcJ8kiezx2KTGE4TVMDtotC4TrE0pfWwoo2UsCi0SFmvSA5Vvy9gFHTIi9PskmsYq5ejkn6Q --kubeconfig=/root/dashboard-admin.conf 配置上下文和当前上下文 1$ kubectl config set-context dashboard-admin@kubernetes --cluster=kubernetes --user=dashboard-admin --kubeconfig=/root/dashboard-admin.conf 配置当前使用的上下文 1$ kubectl config use-context dashboard-admin@kubernetes --kubeconfig=/root/dashboard-admin.conf 查看dashboard-admin.config配置12345678910111213141516171819$ kubectl config view --kubeconfig=/root/dashboard-admin.conf apiVersion: v1clusters:- cluster: certificate-authority-data: DATA+OMITTED server: https://192.168.20.222:6443 name: kubernetescontexts:- context: cluster: kubernetes user: dashboard-admin name: dashboard-admin@kubernetescurrent-context: dashboard-admin@kuberneteskind: Configpreferences: &#123;&#125;users:- name: dashboard-admin user: token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkYXNoYm9hcmQtYWRtaW4tdG9rZW4taDg0djkiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGFzaGJvYXJkLWFkbWluIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiYzAzZjVkZWMtOWMwNS0xMWU5LWFlYzAtMDAwYzI5YjQzOGIzIiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOmRhc2hib2FyZC1hZG1pbiJ9.JFxxIEltq8AiYdTTKxy-QfmR2l6lcyPNOuGlAGR8bXTjVfveCQFj8GEg2eyOJsrjwqgZRN_CwvSpoTjxD39Xv2nvTj-jvWONuLhfr8cHLnTpSgB1KC3oYc0k27w5S0jmZ0Xsimn2pIXAaqzvX3ndZCKW4FngXjRwq27N6pWvynCOuY_625OeDW0Nuonx2AMBXZuu3cqdkM3JSxCc1NDjlJWiTj2Na1-A6uKnG1BxjfGYG5cVTdt4BbBr1Xvom-ky8ts9fUODZozmRyDcJ8kiezx2KTGE4TVMDtotC4TrE0pfWwoo2UsCi0SFmvSA5Vvy9gFHTIi9PskmsYq5ejkn6Q 确认没有错误后，我们将此文件copy出来到本机，然后访问dashboard页面，使用kubeconfig认证登陆即可。]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubeadm安装Kubernetes1.14.3多master集群]]></title>
    <url>%2F2019%2F06%2F15%2Fkubeadm-multi-cluster.html</url>
    <content type="text"><![CDATA[前面我们已经使用kubeadm安装Kubernetes1.14.3单master集群，该架构部署的kubernetes集群比较适合开发、测试环境，并不太适用于生产环境，因为master存在单点故障，从而导致整个集群不可用。鉴于此问题我们现在需要部署一套多master节点的kubernetes集群。此种架构的kubernetes master各个组件是通过选举leader进行高可用的。 集群环境本次构建kubernetes集群是在ESXI主机上创建11个VM虚拟机进行演示的，docker-ce版本为18.9.7，kubernetes组件均为1.14.3版本，环境信息如表所示： 系统 节点角色 ip Hostname 安装组件 centos7.4 master 192.168.20.210 master1 docker-ce、 kubeadm、 kubelet centos7.4 master 192.168.20.211 master2 docker-ce、 kubeadm、 kubelet centos7.4 master 192.168.20.212 master3 docker-ce、 kubeadm、 kubelet centos7.4 worker 192.168.20.213 node1 docker-ce、 kubeadm、 kubelet centos7.4 worker 192.168.20.214 node2 docker-ce、 kubeadm、 kubelet centos7.4 worker 192.168.20.215 node3 docker-ce、 kubeadm、 kubelet centos7.4 etcd 192.168.20.216 etcd1 etcd centos7.4 etcd 192.168.20.217 etcd2 etcd centos7.4 etcd 192.168.20.218 etcd3 etcd centos7.4 ha 192.168.20.219 HA1 keepalived、haproxy centos7.4 ha 192.168.20.220 HA2 keepalived、haproxy none SLB vip 192.168.20.222 none none 安装前准备 注： 在所有节点上进行以下操作，除非另有说明。 设置本地解析12345678910111213$ cat &gt;&gt; /etc/hosts &lt;&lt; EOF192.168.20.210 master1.ipyker.com master1192.168.20.211 master2.ipyker.com master2192.168.20.212 master3.ipyker.com master3192.168.20.213 node1.ipyker.com node1192.168.20.214 node2.ipyker.com node2192.168.20.215 node3.ipyker.com node3192.168.20.216 etcd1.ipyker.com etcd1192.168.20.217 etcd2.ipyker.com etcd2192.168.20.218 etcd3.ipyker.com etcd3192.168.20.219 ha1.ipyker.com ha1192.168.20.220 ha2.ipyker.com ha2EOF 配置SSH免密钥登陆12345678910111213# master1上生成密钥对$ ssh-keygen -t rsa $ ssh-copy-id -i ~/.ssh/id_rsa.pub master1$ ssh-copy-id -i ~/.ssh/id_rsa.pub master2$ ssh-copy-id -i ~/.ssh/id_rsa.pub master3$ ssh-copy-id -i ~/.ssh/id_rsa.pub node1$ ssh-copy-id -i ~/.ssh/id_rsa.pub node2$ ssh-copy-id -i ~/.ssh/id_rsa.pub node3$ ssh-copy-id -i ~/.ssh/id_rsa.pub etcd1$ ssh-copy-id -i ~/.ssh/id_rsa.pub etcd2$ ssh-copy-id -i ~/.ssh/id_rsa.pub etcd3$ ssh-copy-id -i ~/.ssh/id_rsa.pub ha1$ ssh-copy-id -i ~/.ssh/id_rsa.pub ha2 禁用防火墙123# 禁用iptables和firewalld开机自启动$ systemctl disable iptables$ systemctl disable firewalld 时间同步12345$ yum install -y ntp ntpdate ntp-doc # 安装ntp服务$ ntpdate ntp1.aliyun.com # 使用阿里云的时间同步服务器$ clock -w # 系统时间写入blos时间$ crontab -e # 时间同步计划任务*/5 * * * * /usr/sbin/ntpdate ntp1.aliyun.com 禁用Selinux1$ sed -i "s/^SELINUX=enforcing/SELINUX=disabled/g" /etc/selinux/config 关闭Swap12$ swapoff -a$ sed -i 's/.*swap.*/#&amp;/' /etc/fstab 如果你的集群有其他业务不能关闭swap时，需要在安装kubelet后，修改/etc/sysconfig/kubelet内容为：KUBELET_EXTRA_ARGS=--fail-swap-on=false 添加kubernetes内核参数 该参数只在kubernetes的master和node上配置即可。 123456$ cat &gt; /etc/sysctl.d/k8s.conf &lt;&lt; EOFnet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1net.ipv4.ip_forward = 1vm.swappiness = 0EOF 加载ipvs模块 该参数只在kubernetes的master和node上配置即可。 12345678$ vi /etc/sysconfig/modules/ipvs.modules#!/bin/shipvs_mods_dir="/usr/lib/modules/$(uname -r)/kernel/net/netfilter/ipvs/"for mod in `ls $&#123;ipvs_mods_dir&#125; | grep -o "^[^.]*"`; do /sbin/modprobe $moddone$ chmod +x /etc/sysconfig/modules/ipvs.modules &amp;&amp; sh /etc/sysconfig/modules/ipvs.modules 检查模块是否加载生效,如果如下所示表示已经加载ipvs模版到内核了。123456789101112131415161718$ lsmod | grep ip_vsip_vs_wrr 12697 0 ip_vs_wlc 12519 0 ip_vs_sh 12688 0 ip_vs_sed 12519 0 ip_vs_rr 12600 20 ip_vs_pe_sip 12740 0 nf_conntrack_sip 33860 1 ip_vs_pe_sipip_vs_nq 12516 0 ip_vs_lc 12516 0 ip_vs_lblcr 12922 0 ip_vs_lblc 12819 0 ip_vs_ftp 13079 0 nf_nat 26787 4 ip_vs_ftp,nf_nat_ipv4,nf_nat_ipv6,nf_nat_masquerade_ipv4ip_vs_dh 12688 0 ip_vs 141432 44 ip_vs_dh,ip_vs_lc,ip_vs_nq,ip_vs_rr,ip_vs_sh,ip_vs_ftp,ip_vs_sed,ip_vs_wlc,ip_vs_wrr,ip_vs_pe_sip,ip_vs_lblcr,ip_vs_lblcnf_conntrack 133053 10 ip_vs,nf_nat,nf_nat_ipv4,nf_nat_ipv6,xt_conntrack,nf_nat_masquerade_ipv4,nf_conntrack_netlink,nf_conntrack_sip,nf_conntrack_ipv4,nf_conntrack_ipv6libcrc32c 12644 4 xfs,ip_vs,nf_nat,nf_conntrack 请确保ipset也已经安装了，如未安装请执行yum install -y ipset安装。 安装负载均衡器本次环境使用keepalived和haproxy来负载均衡kube-apiserver组件，该组件同一时刻只能启动一个，另外两个将阻塞，直到正在运行的kube-apiserver宕机，此时再通过选举策略选举出另外一个kube-apiserver。 只在HA1和HA2主机上操作。 安装工具包1$ yum install -y wget lrzsz vim epel-release 安装keepalive和haproxy软件包1$ yum install -y keepalived haproxy psmisc haproxy配置123456789101112131415161718192021222324252627282930313233343536373839# listen kube-master指定的为master地址和apiserver的6443端口$ cat &gt; /etc/haproxy/haproxy.cfg &lt;&lt; EOFglobal log 127.0.0.1 local0 log 127.0.0.1 local1 notice chroot /var/lib/haproxy stats socket /var/run/haproxy-admin.sock mode 660 level admin stats timeout 30s user haproxy group haproxy daemon nbproc 1defaults log global timeout connect 5000 timeout client 10m timeout server 10mlisten admin_stats bind 0.0.0.0:10080 mode http log 127.0.0.1 local0 err stats refresh 30s stats uri /status stats realm welcome login\ Haproxy stats auth admin:123456 stats hide-version stats admin if TRUElisten kube-master bind 0.0.0.0:6443 mode tcp option tcplog balance roundrobin server 192.168.20.210 192.168.20.210:6443 check inter 2000 fall 2 rise 2 weight 1 server 192.168.20.211 192.168.20.211:6443 check inter 2000 fall 2 rise 2 weight 1 server 192.168.20.212 192.168.20.212:6443 check inter 2000 fall 2 rise 2 weight 1EOF keepalived配置123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354# HA1配置$ cat &gt; /etc/keepalived/keepalived.conf &lt;&lt; EOFglobal_defs &#123; router_id lb-master-105&#125;vrrp_script check-haproxy &#123; script "killall -0 haproxy" interval 3&#125;vrrp_instance VI-kube-master &#123; state BACKUP nopreempt priority 120 dont_track_primary interface ens192 virtual_router_id 68 advert_int 3 track_script &#123; check-haproxy &#125; virtual_ipaddress &#123; 192.168.20.222 &#125;&#125;EOF# HA2配置$ cat &gt; /etc/keepalived/keepalived.conf &lt;&lt; EOFglobal_defs &#123; router_id lb-master-105&#125;vrrp_script check-haproxy &#123; script "killall -0 haproxy" interval 3&#125;vrrp_instance VI-kube-master &#123; state BACKUP priority 100 dont_track_primary interface ens192 virtual_router_id 68 advert_int 3 track_script &#123; check-haproxy &#125; virtual_ipaddress &#123; 192.168.20.222 &#125;&#125;EOF 启动HA服务12345$ systemctl enable haproxy$ systemctl start haproxy$ systemctl enable keepalived$ systemctl start keepalived 查看VIP地址绑定123# 在HA1和HA2上查看VIP地址，这里VIP绑定在HA1上，如果HA1宕机会漂移到HA2上，结果我们最后集群搭建完成验证。$ ip addr show | grep 192.168.20.222 inet 192.168.20.222/32 scope global ens192 搭建ETCD集群ETCD用于存放kubernetes集群所有配置信息，例如：各个组件的维护着哪些主机、POD、Service以及用户执行kubectl下达的信息都会由apiserver存放在ETCD中，因此搭建kubernetes集群etcd是必不可少的。而etcd在kubernetes中有两种部署方式： 使用堆叠部署方式部署在master节点上。这种方法需要较少的基础设备，etcd成员和master节点位于同一位置。 使用外部etcd集群。这种方法需要更多的基础设备，master节点和etcd成员是分开的。 我们这里使用外部etcd集群配置。 目前版本的kubernetes均采用证书进行验证通信，为了统一因此我们这里也对etcd配置证书。 下载CFSSL签证工具123$ curl -o /usr/local/bin/cfssl https://pkg.cfssl.org/R1.2/cfssl_linux-amd64$ curl -o /usr/local/bin/cfssljson https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64$ chmod +x /usr/local/bin/cfssl* 创建CA证书和密钥123456789101112131415161718192021# 创建CA配置文件$ cat &gt; ca-config.json &lt;&lt;EOF&#123; "signing": &#123; "default": &#123; "expiry": "87600h" &#125;, "profiles": &#123; "kubernetes": &#123; "usages": [ "signing", "key encipherment", "server auth", "client auth" ], "expiry": "87600h" &#125; &#125; &#125;&#125;EOF 12345678910111213141516171819# 创建CA证书签名请求cat &gt; ca-csr.json &lt;&lt;EOF&#123; "CN": "kubernetes", "key": &#123; "algo": "rsa", "size": 2048 &#125;, "names": [ &#123; "C": "CN", "ST": "BeiJing", "L": "BeiJing", "O": "k8s", "OU": "4Paradigm" &#125; ]&#125;EOF 12345678910111213141516171819202122232425# 创建etcd证书签名请求文件cat &gt; etcd-csr.json &lt;&lt;EOF&#123; "CN": "etcd", "hosts": [ "127.0.0.1", "192.168.20.216", "192.168.20.217", "192.168.20.218" ], "key": &#123; "algo": "rsa", "size": 2048 &#125;, "names": [ &#123; "C": "CN", "ST": "BeiJing", "L": "BeiJing", "O": "k8s", "OU": "4Paradigm" &#125; ]&#125;EOF 生成 CA 证书和私钥12345$ cfssl gencert -initca ca-csr.json | cfssljson -bare ca$ cfssl gencert -ca=ca.pem \ -ca-key=ca-key.pem \ -config=ca-config.json \ -profile=kubernetes etcd-csr.json | cfssljson -bare etcd 证书分发123456789# 这里etcd1的没生成密钥并copy到各个节点，所以要手动输入密码$ mkdir -pv /etc/kubernetes/pki/etcd/ # etcd、master节点手动创建该目录$ cp *.pem /etc/kubernetes/pki/etcd/$ scp *.pem etcd2:/etc/kubernetes/pki/etcd/$ scp *.pem etcd3:/etc/kubernetes/pki/etcd/# kubernetes master 也需要etcd的证书$ scp /etc/kubernetes/pki/etcd/* master1:/etc/kubernetes/pki/etcd/$ scp /etc/kubernetes/pki/etcd/* master2:/etc/kubernetes/pki/etcd/$ scp /etc/kubernetes/pki/etcd/* master3:/etc/kubernetes/pki/etcd/ 下载etcd二进制tar包12345$ wget https://github.com/etcd-io/etcd/releases/download/v3.3.10/etcd-v3.3.10-linux-amd64.tar.gz$ tar zxf etcd-v3.3.10-linux-amd64.tar.gz$ cp etcd-v3.3.10-linux-amd64/etcd* /usr/local/bin$ scp /usr/local/bin/etcd* etcd2:/usr/local/bin/$ scp /usr/local/bin/etcd* etcd3:/usr/local/bin/ 创建工作目录 所有etcd节点都要操作 1$ mkdir -p /var/lib/etcd 创建systemctl unit文件1234567891011121314151617181920212223242526272829303132333435# 复制到对应etcd节点请注意修改 --name名称和节点ip，这里以etcd1和192.168.20.216为例$ cat &gt; /etc/systemd/system/etcd.service &lt;&lt; EOF[Unit]Description=Etcd ServerAfter=network.targetAfter=network-online.targetWants=network-online.targetDocumentation=https://github.com/coreos[Service]Type=notifyWorkingDirectory=/var/lib/etcd/ExecStart=/usr/local/bin/etcd \ --name etcd1 \ --cert-file=/etc/kubernetes/pki/etcd/etcd.pem \ --key-file=/etc/kubernetes/pki/etcd/etcd-key.pem \ --peer-cert-file=/etc/kubernetes/pki/etcd/etcd.pem \ --peer-key-file=/etc/kubernetes/pki/etcd/etcd-key.pem \ --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.pem \ --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.pem \ --initial-advertise-peer-urls https://192.168.20.216:2380 \ --listen-peer-urls https://192.168.20.216:2380 \ --listen-client-urls https://192.168.20.216:2379,http://127.0.0.1:2379 \ --advertise-client-urls https://192.168.20.216:2379 \ --initial-cluster-token etcd-cluster-0 \ --initial-cluster etcd1=https://192.168.20.216:2380,etcd2=https://192.168.20.217:2380,etcd3=https://192.168.20.218:2380 \ --initial-cluster-state new \ --data-dir=/var/lib/etcdRestart=on-failureRestartSec=5LimitNOFILE=65536[Install]WantedBy=multi-user.targetEOF 启动etcd服务123$ systemctl daemon-reload$ systemctl enable etcd$ systemctl start etcd 验证etcd集群1234567891011$ etcdctl --ca-file=/etc/kubernetes/pki/etcd/ca.pem --cert-file=/etc/kubernetes/pki/etcd/etcd.pem --key-file=/etc/kubernetes/pki/etcd/etcd-key.pem cluster-healthmember 8d6cbe5fbadf12e8 is healthy: got healthy result from https://192.168.20.216:2379member 99ca23ca7acda4bf is healthy: got healthy result from https://192.168.20.217:2379member 9c6200830b8143c8 is healthy: got healthy result from https://192.168.20.218:2379cluster is healthy# 也可以查看当前leader$ etcdctl --ca-file=/etc/kubernetes/pki/etcd/ca.pem --cert-file=/etc/kubernetes/pki/etcd/etcd.pem --key-file=/etc/kubernetes/pki/etcd/etcd-key.pem member list8d6cbe5fbadf12e8: name=etcd1 peerURLs=https://192.168.20.216:2380 clientURLs=https://192.168.20.216:2379 isLeader=false99ca23ca7acda4bf: name=etcd2 peerURLs=https://192.168.20.217:2380 clientURLs=https://192.168.20.217:2379 isLeader=true9c6200830b8143c8: name=etcd3 peerURLs=https://192.168.20.218:2380 clientURLs=https://192.168.20.218:2379 isLeader=false 显示cluster is healthy表示etcd集群搭建完成。至此，我们已经为kubernetes部署准备好了环境，下面我们来配置我们的主角kubernetes。 安装docker-ce注：只在kubernetes的master和node上安装docker-ce即可。 配置docker-ce的yum源1$ wget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo -O /etc/yum.repos.d/docker-ce.repo 安装docker-ce12# 这里默认为18.9.7最新版本$ yum install docker-ce -y 如果要安装指定版本的docker-ce，请先查看版本yum list docker-ce --showduplicates | sort -r 配置docker加速12345$ cat &gt; /etc/docker/daemon.json &lt;&lt; EOF&#123; "registry-mirrors": ["https://yuife6vp.mirror.aliyuncs.com"]&#125;EOF 配置iptables FORWARD默认情况下docker-ce将iptables的FORWARD默认规则设置为DROP，这样会引起Kubernetes集群中跨Node的Pod无法通信。我们需要将此规则设置为默认ACCEPT。12345# 命令行设置，重启失效$ iptables -P FORWARD ACCEPT# 在docker的systemctl文件[Service]中添加ExecStartPost$ vi /usr/lib/systemd/system/docker.service ExecStartPost=/usr/sbin/iptables -P FORWARD ACCEPT 配置docker科学上网1234# 在docker的systemctl文件[Service]中添加Environment$ vi /usr/lib/systemd/system/docker.service Environment="HTTP_PROXY=192.168.20.199:1080"Envrionment="NO_PROXY=127.0.0.0/8,192.168.20.0/23" 这里需要你有科学上网的代理服务器，配置科学上网的目的是使docker能够pull gcr.io上的镜像，如果没有也可以在其他地方下载，然后通过docker tag成kubernetes需要的镜像名。 启动docker123$ systemctl daemon-reload$ systemctl start docker$ systemctl enable docker 安装kubeadm 和 kubelet注： 只在master、node节点上进行以下操作。 123456789$ cat &gt; /etc/yum.repos.d/kubernetes.repo &lt;&lt; EOF[kubernetes]name=Kubernetes Repositorybaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/gpgcheck=1gpgkey=1gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpgEOF 1$ yum install -y kubelet-1.14.3 kubeadm-1.14.3 kubectl-1.14.3 12# 设置开机自启动kubelet$ systemctl enable kubelet kubelet负责与其他节点集群通信，并进行本节点Pod和容器生命周期的管理。kubeadm是Kubernetes的自动化部署工具，降低了部署难度，提高效率。kubectl是Kubernetes集群客户端管理工具。 部署kubernetes master以下内容是根据kubeadm config print init-defaults指令打印出来的，并根据自己需求手动修改后的结果。123456789101112131415161718192021222324252627282930313233$ cat &gt; /root/kubeadm-init.yaml &lt;&lt; EOFapiVersion: kubeadm.k8s.io/v1beta1kind: ClusterConfigurationkubernetesVersion: v1.14.3#imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containerscontrolPlaneEndpoint: 192.168.20.222:6443apiServer: certSANs: - master1 - master2 - master3 - 192.168.20.210 - 192.168.20.211 - 192.168.20.212 - 192.168.20.222 - 127.0.0.1etcd: external: endpoints: - "https://192.168.20.216:2379" - "https://192.168.20.217:2379" - "https://192.168.20.218:2379" caFile: /etc/kubernetes/pki/etcd/ca.pem certFile: /etc/kubernetes/pki/etcd/etcd.pem keyFile: /etc/kubernetes/pki/etcd/etcd-key.pemnetworking: podSubnet: 10.244.0.0/16---apiVersion: kubeproxy.config.k8s.io/v1alpha1kind: KubeProxyConfigurationmode: ipvsEOF 其中apiServer.certSANS中配置的是所有要和apiserver交互的地址，包括VIP。 etcd.external.endpoints 配置的是外部etcd集群，其中也指定了etcd证书路径，这就是为什么etcd的证书要复制到kubernetes所有master节点的原因。 获取kubernetes组件镜像这里手动拉取master所需要的镜像是为了等会kubeadm init时能够快些，也可以不手动拉取。在kubeadm init时也会自动拉取。1$ kubeadm config images pull --config kubeadm-init.yaml 如果你能科学上网，执行kubeadm config images pull后将自动拉取kubeadm对应版本的master节点需要的镜像，拉取完需要的镜像如下：12345678$ docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEk8s.gcr.io/kube-proxy v1.14.3 004666307c5b 8 days ago 82.1MBk8s.gcr.io/kube-controller-manager v1.14.3 ac2ce44462bc 8 days ago 158MBk8s.gcr.io/kube-apiserver v1.14.3 9946f563237c 8 days ago 210MBk8s.gcr.io/kube-scheduler v1.14.3 953364a3ae7a 8 days ago 81.6MBk8s.gcr.io/coredns 1.3.1 eb516548c180 5 months ago 40.3MBk8s.gcr.io/pause 3.1 da86e6ba6ca1 18 months ago 742kB 当你不能科学上网时，可以手动从别的镜像仓库中下载，这样也能达到我们的要求，如下操作：12345678910111213$ vi /root/kubernetes-images.sh#!/bin/shPACKAGE=(kube-apiserver:v1.14.3 kube-controller-manager:v1.14.3 kube-scheduler:v1.14.3 kube-proxy:v1.14.3 pause:3.1)for pack in "$&#123;PACKAGE[@]&#125;"; do docker pull mirrorgooglecontainers/$&#123;pack&#125; docker tag docker.io/mirrorgooglecontainers/$&#123;pack&#125; k8s.gcr.io/$&#123;pack&#125; docker rmi docker.io/mirrorgooglecontainers/$&#123;pack&#125;donedocker pull coredns/coredns:1.3.1docker tag docker.io/coredns/coredns:1.3.1 k8s.gcr.io/coredns:1.3.1docker rmi docker.io/coredns/coredns:1.3.1$ sh /root/kubernetes-images.sh 初始化集群1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374$ kubeadm init --config kubeadm-init.yaml[init] Using Kubernetes version: v1.14.3[preflight] Running pre-flight checks [WARNING IsDockerSystemdCheck]: detected "cgroupfs" as the Docker cgroup driver. The recommended driver is "systemd". Please follow the guide at https://kubernetes.io/docs/setup/cri/[preflight] Pulling images required for setting up a Kubernetes cluster[preflight] This might take a minute or two, depending on the speed of your internet connection[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"[kubelet-start] Activating the kubelet service[certs] Using certificateDir folder "/etc/kubernetes/pki"[certs] Generating "front-proxy-ca" certificate and key[certs] Generating "front-proxy-client" certificate and key[certs] Generating "etcd/ca" certificate and key[certs] Generating "etcd/server" certificate and key[certs] etcd/server serving cert is signed for DNS names [master1 localhost] and IPs [192.168.20.210 127.0.0.1 ::1][certs] Generating "etcd/healthcheck-client" certificate and key[certs] Generating "etcd/peer" certificate and key[certs] etcd/peer serving cert is signed for DNS names [master1 localhost] and IPs [192.168.20.210 127.0.0.1 ::1][certs] Generating "apiserver-etcd-client" certificate and key[certs] Generating "ca" certificate and key[certs] Generating "apiserver-kubelet-client" certificate and key[certs] Generating "apiserver" certificate and key[certs] apiserver serving cert is signed for DNS names [master1 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.20.210][certs] Generating "sa" key and public key[kubeconfig] Using kubeconfig folder "/etc/kubernetes"[kubeconfig] Writing "admin.conf" kubeconfig file[kubeconfig] Writing "kubelet.conf" kubeconfig file[kubeconfig] Writing "controller-manager.conf" kubeconfig file[kubeconfig] Writing "scheduler.conf" kubeconfig file[control-plane] Using manifest folder "/etc/kubernetes/manifests"[control-plane] Creating static Pod manifest for "kube-apiserver"[control-plane] Creating static Pod manifest for "kube-controller-manager"[control-plane] Creating static Pod manifest for "kube-scheduler"[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s[apiclient] All control plane components are healthy after 15.501532 seconds[upload-config] storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace[kubelet] Creating a ConfigMap "kubelet-config-1.14" in namespace kube-system with the configuration for the kubelets in the cluster[upload-certs] Skipping phase. Please see --experimental-upload-certs[mark-control-plane] Marking the node master1 as control-plane by adding the label "node-role.kubernetes.io/master=''"[mark-control-plane] Marking the node master1 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule][bootstrap-token] Using token: ankpak.4mjrj58zunfd79mo[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster[bootstrap-token] creating the "cluster-info" ConfigMap in the "kube-public" namespace[addons] Applied essential addon: CoreDNS[addons] Applied essential addon: kube-proxyYour Kubernetes control-plane has initialized successfully!To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/configYou should now deploy a pod network to the cluster.Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/You can now join any number of control-plane nodes by copying certificate authorities and service account keys on each node and then running the following as root: kubeadm join 192.168.20.222:6443 --token wvufw9.uzdq9vfcwgfq3lve \ --discovery-token-ca-cert-hash sha256:2cd1440d49aaf1f8b9321f71ff14f977f5fb37e80f865b136cdda372f94fabe7 \ --experimental-control-plane Then you can join any number of worker nodes by running the following on each as root:kubeadm join 192.168.20.222:6443 --token wvufw9.uzdq9vfcwgfq3lve \ --discovery-token-ca-cert-hash sha256:2cd1440d49aaf1f8b9321f71ff14f977f5fb37e80f865b136cdda372f94fabe7 执行完后结果会告诉我们如何配置kubectl用户权限以及其它master节点和node节点加入集群的命令。后面会用到这两条命令。 配置执行kubectl命令用户1234# kubernetes建议使用非root用户运行kubectl命令访问集群$ mkdir -p $HOME/.kube$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config$ sudo chown $(id -u):$(id -g) $HOME/.kube/config 配置flannel网络1$ wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml kube-flannel.yml文件下载后修改以下内容：12345678net-conf.json: | &#123; "Network": "10.244.0.0/16", "Backend": &#123; "Type": "vxlan", "Directrouting": true &#125; &#125; &quot;Directrouting&quot;: true的作用使同网段的pod通信直接走本node网卡，以静态路由表方式配置。而vxlan将虚拟网络的数据帧添加到VxLAN首部，封装在物理网络的UDP报文中，进行隧道传输到达目标主机后去掉物理网络报文的头部信息以及VxLAN首部，并交付给目的终端。该配置会优先使用host-gw，不在同一网段的主机在选择vxlan模式。12345678910$ kubectl apply -f kube-flannel.ymlclusterrole.rbac.authorization.k8s.io/flannel createdclusterrolebinding.rbac.authorization.k8s.io/flannel createdserviceaccount/flannel createdconfigmap/kube-flannel-cfg createddaemonset.extensions/kube-flannel-ds-amd64 createddaemonset.extensions/kube-flannel-ds-arm64 createddaemonset.extensions/kube-flannel-ds-arm createddaemonset.extensions/kube-flannel-ds-ppc64le createddaemonset.extensions/kube-flannel-ds-s390x created 如果你的网络不能下载flannel的镜像，请手动修改kube-flannel.yml文件的image为registry.cn-shenzhen.aliyuncs.com/pyker/flannel:v0.11.0-amd64，下载完成后，请执行：1$ docker tag registry.cn-shenzhen.aliyuncs.com/pyker/flannel:v0.11.0-amd64 quay.io/coreos/flannel:v0.11.0-amd64 然后在运行 kubectl apply -f kube-flannel.yml 此时我们的master节点已经准备好了，可以通过kubectl get nodes查看123$ kubectl get nodesNAME STATUS ROLES AGE VERSIONmaster1 Ready master 3h11m v1.14.3 master2和master3加入集群kubernetes证书分发复制master1生成的证书文件到master2和master3，因为kubernetes使用证书配置集群，所以其它节点要使用同一套证书。12345# 在master1上操作$ cd /etc/kubernetes/pki/$ scp *.crt *.key sa.pub master2:/etc/kubernetes/pki/$ scp *.crt *.key sa.pub master3:/etc/kubernetes/pki/#这里还有etcd证书，因为在配置etcd的时候我们已经复制过去了，所以这里不用在复制了。 手动载入组件镜像为了节约安装时间，我们这里手动将master1上的组件镜像载入到master2和master3上。如果不手动载入也可以，时间比较久而已。当然如果是k8s.gcr.io的仓库地址的话记得要科学上网哦。123$ docker save k8s.gcr.io/kube-proxy:v1.14.3 k8s.gcr.io/kube-controller-manager:v1.14.3 k8s.gcr.io/kube-apiserver:v1.14.3 k8s.gcr.io/kube-scheduler:v1.14.3 k8s.gcr.io/coredns:1.3.1 k8s.gcr.io/pause:3.1 quay.io/coreos/flannel:v0.11.0-amd64 -o k8s-masterimages.tar$ scp k8s-masterimages.tar master2:/root$ scp k8s-masterimages.tar master3:/root 1234567891011# 在master2和master3上手动加载镜像$ docker load -i k8s-masterimages.tar$ docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEk8s.gcr.io/kube-proxy v1.14.3 004666307c5b 3 weeks ago 82.1MBk8s.gcr.io/kube-controller-manager v1.14.3 ac2ce44462bc 3 weeks ago 158MBk8s.gcr.io/kube-apiserver v1.14.3 9946f563237c 3 weeks ago 210MBk8s.gcr.io/kube-scheduler v1.14.3 953364a3ae7a 3 weeks ago 81.6MBquay.io/coreos/flannel v0.11.0-amd64 ff281650a721 5 months ago 52.6MBk8s.gcr.io/coredns 1.3.1 eb516548c180 5 months ago 40.3MBk8s.gcr.io/pause 3.1 da86e6ba6ca1 18 months ago 742kB 加入集群使用kubeadm安装的集群要让一个新节点加入集群那是相当简单了，如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253# 在master2 和 master3执行$ kubeadm join 192.168.20.222:6443 --token wvufw9.uzdq9vfcwgfq3lve \ --discovery-token-ca-cert-hash sha256:2cd1440d49aaf1f8b9321f71ff14f977f5fb37e80f865b136cdda372f94fabe7 \ --experimental-control-plane [preflight] Running pre-flight checks [WARNING IsDockerSystemdCheck]: detected "cgroupfs" as the Docker cgroup driver. The recommended driver is "systemd". Please follow the guide at https://kubernetes.io/docs/setup/cri/[preflight] Reading configuration from the cluster...[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'[preflight] Running pre-flight checks before initializing the new control plane instance[preflight] Pulling images required for setting up a Kubernetes cluster[preflight] This might take a minute or two, depending on the speed of your internet connection[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'[certs] Using certificateDir folder "/etc/kubernetes/pki"[certs] Using the existing "front-proxy-client" certificate and key[certs] Using the existing "apiserver" certificate and key[certs] Using the existing "apiserver-kubelet-client" certificate and key[certs] Valid certificates and keys now exist in "/etc/kubernetes/pki"[certs] Using the existing "sa" key[kubeconfig] Generating kubeconfig files[kubeconfig] Using kubeconfig folder "/etc/kubernetes"[kubeconfig] Writing "admin.conf" kubeconfig file[kubeconfig] Writing "controller-manager.conf" kubeconfig file[kubeconfig] Writing "scheduler.conf" kubeconfig file[control-plane] Using manifest folder "/etc/kubernetes/manifests"[control-plane] Creating static Pod manifest for "kube-apiserver"[control-plane] Creating static Pod manifest for "kube-controller-manager"[control-plane] Creating static Pod manifest for "kube-scheduler"[check-etcd] Skipping etcd check in external mode[kubelet-start] Downloading configuration for the kubelet from the "kubelet-config-1.14" ConfigMap in the kube-system namespace[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"[kubelet-start] Activating the kubelet service[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...[control-plane-join] using external etcd - no local stacked instance added[upload-config] storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace[mark-control-plane] Marking the node master2 as control-plane by adding the label "node-role.kubernetes.io/master=''"[mark-control-plane] Marking the node master2 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]This node has joined the cluster and a new control plane instance was created:* Certificate signing request was sent to apiserver and approval was received.* The Kubelet was informed of the new secure connection details.* Control plane (master) label and taint were applied to the new node.* The Kubernetes control plane instances scaled up.To start administering your cluster from this node, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/configRun 'kubectl get nodes' to see this node join the cluster. 至此master2和master3以master身份加入集群成功，而--experimental-control-plane参数是主要参数。 配置执行kubectl命令用户1234# kubernetes建议使用非root用户运行kubectl命令访问集群$ mkdir -p $HOME/.kube$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config$ sudo chown $(id -u):$(id -g) $HOME/.kube/config 查看集群节点123456# 三台master节点都可以执行kubectl命令，因为都复制了集群的/etc/kubernetes/admin.conf认证文件到用户/.kube/config中。$ kubectl get nodesNAME STATUS ROLES AGE VERSIONmaster1 Ready master 4h2m v1.14.3master2 Ready master 3h48m v1.14.3master3 Ready master 3h47m v1.14.3 现在我们三台master都搭建好了，现在我们为集群部署node。 部署kubernetes node节点手动载入组件镜像同样为了节约时间，我们将master1上的部分node需要的组件镜像打包，然后发送到node节点进行载入。1234$ docker save k8s.gcr.io/kube-proxy:v1.14.3 k8s.gcr.io/pause:3.1 quay.io/coreos/flannel:v0.11.0-amd64 -o k8s-nodeimages.tar$ scp k8s-nodeimages.tar node1:/root$ scp k8s-nodeimages.tar node2:/root$ scp k8s-nodeimages.tar node3:/root 载入node组件镜像 以下操作3台node都要进行 1$ docker load -i k8s-nodeimages.tar node加入集群1$ kubeadm join 192.168.20.222:6443 --token wvufw9.uzdq9vfcwgfq3lve --discovery-token-ca-cert-hash sha256:2cd1440d49aaf1f8b9321f71ff14f977f5fb37e80f865b136cdda372f94fabe7 node节点加入集群就这么简单。集群部署完后，查看集群当前节点，3master和3node都已经为Ready了，集群可用。 查看节点数及状态12345678$ kubectl get nodeNAME STATUS ROLES AGE VERSIONmaster1 Ready master 4h14m v1.14.3master2 Ready master 4h v1.14.3master3 Ready master 4h v1.14.3node1 Ready &lt;none&gt; 3h30m v1.14.3node2 Ready &lt;none&gt; 3h31m v1.14.3node3 Ready &lt;none&gt; 3h31m v1.14.3 查看集群组件状态1234567$ kubectl get csNAME STATUS MESSAGE ERRORcontroller-manager Healthy ok scheduler Healthy ok etcd-2 Healthy &#123;"health":"true"&#125; etcd-1 Healthy &#123;"health":"true"&#125; etcd-0 Healthy &#123;"health":"true"&#125; 验证集群通过上面的步骤，我们已经完成了集群的配置安装，现在我们要验证高可用集群的特性，看看是否能高可用，要验证的有： 停掉当前已选举的master来验证组件是否会重新选举。 停掉某个etcd来验证etcd的集群是否可用。 停掉vip地址所在的主机服务，验证vip是否会偏移到另外一台HA上并且集群可用。 验证master选举高可用要验证masrer集群是否高可用，我们先来查看当前各组件选举在哪台master节点上。12345678910111213141516171819202122232425$ kubectl get endpoints kube-controller-manager -n kube-system -o yamlapiVersion: v1kind: Endpointsmetadata: annotations: control-plane.alpha.kubernetes.io/leader: '&#123;"holderIdentity":"master1_05043914-9bb3-11e9-bb13-000c29b438b3","leaseDurationSeconds":15,"acquireTime":"2019-07-01T03:48:05Z","renewTime":"2019-07-01T08:13:12Z","leaderTransitions":0&#125;' creationTimestamp: "2019-07-01T03:48:05Z" name: kube-controller-manager namespace: kube-system resourceVersion: "31206" selfLink: /api/v1/namespaces/kube-system/endpoints/kube-controller-manager uid: 0859c6e9-9bb3-11e9-a8cd-000c29b438b3$ kubectl get endpoints kube-scheduler -n kube-system -o yamlapiVersion: v1kind: Endpointsmetadata: annotations: control-plane.alpha.kubernetes.io/leader: '&#123;"holderIdentity":"master1_04bc7a76-9bb3-11e9-9dd4-000c29b438b3","leaseDurationSeconds":15,"acquireTime":"2019-07-01T03:48:04Z","renewTime":"2019-07-01T08:13:31Z","leaderTransitions":0&#125;' creationTimestamp: "2019-07-01T03:48:04Z" name: kube-scheduler namespace: kube-system resourceVersion: "31243" selfLink: /api/v1/namespaces/kube-system/endpoints/kube-scheduler uid: 0805a77e-9bb3-11e9-a8cd-000c29b438b3 通过holderIdentity&quot;:&quot;master1我们可以看到当前2组件都运行在master1上。所以我们现在把master1关机或者停掉kubernetes服务来验证。 为什么没kube-apiserver？ 因为kube-apiserver是阻塞的。只能存在一个在运行，而其他两个节点可以运行，但是没被调用而已。 1234# 在master1上杀掉kube所有进程,或者直接关机$ pkill kube$ ps -ef | grep kuberoot 37678 1531 0 16:17 pts/0 00:00:00 grep --color=auto kube 当前显示kube相关的进程已经没有了，那么我们在执行命令查看当前组件选举的哪个节点12345678910111213141516171819202122232425$ kubectl get endpoints kube-scheduler -n kube-system -o yamlapiVersion: v1kind: Endpointsmetadata: annotations: control-plane.alpha.kubernetes.io/leader: '&#123;"holderIdentity":"master3_11cc6547-9bb5-11e9-a0ed-000c29870b1d","leaseDurationSeconds":15,"acquireTime":"2019-07-01T08:17:17Z","renewTime":"2019-07-01T08:18:23Z","leaderTransitions":1&#125;' creationTimestamp: "2019-07-01T03:48:04Z" name: kube-scheduler namespace: kube-system resourceVersion: "31875" selfLink: /api/v1/namespaces/kube-system/endpoints/kube-scheduler uid: 0805a77e-9bb3-11e9-a8cd-000c29b438b3$ kubectl get endpoints kube-controller-manager -n kube-system -o yamlapiVersion: v1kind: Endpointsmetadata: annotations: control-plane.alpha.kubernetes.io/leader: '&#123;"holderIdentity":"master3_120c93e2-9bb5-11e9-9fca-000c29870b1d","leaseDurationSeconds":15,"acquireTime":"2019-07-01T08:17:20Z","renewTime":"2019-07-01T08:18:32Z","leaderTransitions":1&#125;' creationTimestamp: "2019-07-01T03:48:05Z" name: kube-controller-manager namespace: kube-system resourceVersion: "31893" selfLink: /api/v1/namespaces/kube-system/endpoints/kube-controller-manager uid: 0859c6e9-9bb3-11e9-a8cd-000c29b438b3 通过holderIdentity&quot;:&quot;master3我们可以看到当前2组件通过选举已经到master3上了。因此kubernetes master高可用验证完成。 验证etcd集群高可用我们已经知道，kubernetes所有操作的配置以及组件维护的状态都要存储在etcd中，当etcd不能用时，整个kubernetes集群也不能正常工作了。那么我们关掉etcd1节点来测试。查看当前组件监控状态：1234567kubectl get csNAME STATUS MESSAGE ERRORcontroller-manager Healthy ok scheduler Healthy ok etcd-0 Healthy &#123;"health":"true"&#125; etcd-2 Healthy &#123;"health":"true"&#125; etcd-1 Healthy &#123;"health":"true"&#125; # 当前该etcd为健康状态 在etcd1节点上以下操作：123$ systemctl stop etcd$ ps -ef | grep etcdroot 12919 2305 0 16:26 pts/0 00:00:00 grep --color=auto etcd 显示etcd1的etcd服务已经都被关掉了，然后我们在其他etcd节点上执行以下命令查看etcd集群状态：123456$ etcdctl --ca-file=/etc/kubernetes/pki/etcd/ca.pem --cert-file=/etc/kubernetes/pki/etcd/etcd.pem --key-file=/etc/kubernetes/pki/etcd/etcd-key.pem cluster-healthfailed to check the health of member 8d6cbe5fbadf12e8 on https://192.168.20.216:2379: Get https://192.168.20.216:2379/health: dial tcp 192.168.20.216:2379: connect: connection refusedmember 8d6cbe5fbadf12e8 is unreachable: [https://192.168.20.216:2379] are all unreachablemember 99ca23ca7acda4bf is healthy: got healthy result from https://192.168.20.217:2379member 9c6200830b8143c8 is healthy: got healthy result from https://192.168.20.218:2379cluster is degraded 已经很明显的看到了etcd1这台服务已经不可达了，而且整个集群也变成degraded，被降级了。回到kubernetes master1节点上，我们再次查看组件健康状态1234567kubectl get csNAME STATUS MESSAGE ERRORetcd-1 Unhealthy Get https://192.168.20.216:2379/health: dial tcp 192.168.20.216:2379: connect: connection refused scheduler Healthy ok controller-manager Healthy ok etcd-2 Healthy &#123;"health":"true"&#125; etcd-0 Healthy &#123;"health":"true"&#125; 我们也可以通过kubernetes查看到etcd1已经连接失败了。那么我们kubernetes会受影响吗？，我们操作一下即可！123456789$ kubectl create namespace prodnamespace/prod created[root@master1 ~]# kubectl get nsNAME STATUS AGEdefault Active 4h43mkube-node-lease Active 4h43mkube-public Active 4h43mkube-system Active 4h43mprod Active 6s 可以看到etcd1节点服务宕机后，我们的kubernetes依然可以用，因此etcd集群也是高可用的。 验证vip地址漂移我们都知道我们的集群apiserver是通过haproxy的vip地址做反向代理的。可以通过以下命令查看12345$ kubectl cluster-infoKubernetes master is running at https://192.168.20.222:6443KubeDNS is running at https://192.168.20.222:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxyTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. 当前显示的master是https://192.168.20.222:6443,也就是我们的VIP地址代理。那么我们手动停掉haproxy主机，结果会怎么样呢？我们还是来验证一下。通过开始的HA配置，我们的VIP地址在HA1主机上，现在我们关掉该服务器。1234567891011[root@HA1 ~]# shutdown -h nowPolicyKit daemon disconnected from the bus.We are no longer a registered authentication agent.Connection closing...Socket close.Connection closed by foreign host.Disconnected from remote host(HA1-192.168.20.219) at 16:36:36.Type `help' to learn how to use Xshell prompt.[C:\~]$ 那么我们现在到HA2主机上查看VIP是否漂移过来了。12[root@HA2 ~]# ip addr show | grep 192.168.20.222 inet 192.168.20.222/32 scope global ens192 可以看到VIP地址已经成功漂移到HA2节点上了，那么我们的kubernetes服务正常吗？我们依然来验证一下：12345678910111213141516171819202122232425$ kubectl get pod -n kube-systemNAME READY STATUS RESTARTS AGEcoredns-fb8b8dccf-95hs5 1/1 Running 2 4h52mcoredns-fb8b8dccf-ct7qr 1/1 Running 2 4h52mkube-apiserver-master1 1/1 Running 1 4h52mkube-apiserver-master2 1/1 Running 0 4h37mkube-apiserver-master3 1/1 Running 0 4h37mkube-controller-manager-master1 1/1 Running 1 4h51mkube-controller-manager-master2 1/1 Running 0 4h37mkube-controller-manager-master3 1/1 Running 0 4h37mkube-flannel-ds-amd64-4s5th 1/1 Running 0 4h38mkube-flannel-ds-amd64-4xcpp 1/1 Running 0 4h9mkube-flannel-ds-amd64-jn8sv 1/1 Running 0 4h45mkube-flannel-ds-amd64-p8qmf 1/1 Running 0 4h9mkube-flannel-ds-amd64-wltd6 1/1 Running 0 4h38mkube-flannel-ds-amd64-x5p77 1/1 Running 0 4h9mkube-proxy-c8lmp 1/1 Running 0 4h9mkube-proxy-drgnv 1/1 Running 0 4h9mkube-proxy-f8s5k 1/1 Running 1 4h52mkube-proxy-n6mmr 1/1 Running 0 4h9mkube-proxy-qnlsq 1/1 Running 0 4h38mkube-proxy-r4jjd 1/1 Running 0 4h38mkube-scheduler-master1 1/1 Running 1 4h51mkube-scheduler-master2 1/1 Running 0 4h37mkube-scheduler-master3 1/1 Running 0 4h37m 显示我们的kubectl客户端依然可以通过VIP访问kubernetes集群。因此HA高可用验证成功。]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubeadm安装Kubernetes1.14.3单master集群]]></title>
    <url>%2F2019%2F06%2F12%2Fkubeadm-cluster.html</url>
    <content type="text"><![CDATA[kubeadm是Kubernetes官方提供的用于快速安装Kubernetes集群的工具，伴随Kubernetes每个版本的发布都会同步更新，kubeadm会对集群配置方面的一些实践做调整，通过实验kubeadm可以学习到Kubernetes官方在集群配置上一些新的最佳实践。 集群环境本次构建kubernetes集群是在ESXI主机上创建4个VM虚拟机进行演示的，docker-ce版本为18.09.6，kubernetes组件均为1.14.3版本，环境信息如表所示： 系统 节点角色 ip Hostname 安装组件 centos7.4 master 192.168.20.210 master1 docker-ce、 kubeadm、 kubelet centos7.4 worker 192.168.20.213 node1 docker-ce、 kubeadm、 kubelet centos7.4 worker 192.168.20.214 node2 docker-ce、 kubeadm、 kubelet centos7.4 worker 192.168.20.215 node3 docker-ce、 kubeadm、 kubelet 安装前准备 注： 在所有节点上进行以下操作。 配置SSH免密钥登陆12345# master1上生成密钥对$ ssh-keygen -t rsa $ ssh-copy-id -i ~/.ssh/id_rsa.pub node1$ ssh-copy-id -i ~/.ssh/id_rsa.pub node2$ ssh-copy-id -i ~/.ssh/id_rsa.pub node3 设置主机名12345678$ cat &gt;&gt; /etc/hosts &lt;&lt; EOF192.168.20.210 master1.ipyker.com master1192.168.20.211 master2.ipyker.com master2192.168.20.212 master3.ipyker.com master3192.168.20.213 node1.ipyker.com node1192.168.20.214 node2.ipyker.com node2192.168.20.215 node3.ipyker.com node3EOF 禁用防火墙123# 禁用iptables和firewalld开机自启动$ systemctl disable iptables$ systemctl disable firewalld 时间同步12345$ yum install -y ntp ntpdate ntp-doc # 安装ntp服务$ ntpdate ntp1.aliyun.com # 使用阿里云的时间同步服务器$ clock -w # 系统时间写入blos时间$ crontab -e # 时间同步计划任务*/5 * * * * /usr/sbin/ntpdate ntp1.aliyun.com 禁用Selinux1$ sed -i "s/^SELINUX=enforcing/SELINUX=disabled/g" /etc/selinux/config 关闭Swap12$ swapoff -a$ sed -i 's/.*swap.*/#&amp;/' /etc/fstab 如果你的集群有其他业务不能关闭swap时，需要在安装kubelet后，修改/etc/sysconfig/kubelet内容为：KUBELET_EXTRA_ARGS=--fail-swap-on=false 添加kubernetes内核参数123456$ cat &gt; /etc/sysctl.d/k8s.conf &lt;&lt; EOFnet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1net.ipv4.ip_forward = 1vm.swappiness = 0EOF 加载ipvs模块12345678$ vi /etc/sysconfig/modules/ipvs.modules#!/bin/shipvs_mods_dir="/usr/lib/modules/$(uname -r)/kernel/net/netfilter/ipvs/"for mod in `ls $&#123;ipvs_mods_dir&#125; | grep -o "^[^.]*"`; do /sbin/modprobe $moddone$ chmod +x /etc/sysconfig/modules/ipvs.modules &amp;&amp; sh /etc/sysconfig/modules/ipvs.modules 检查模块是否加载生效,如果如下所示表示已经加载ipvs模版到内核了。123456789101112131415161718$ lsmod | grep ip_vsip_vs_wrr 12697 0 ip_vs_wlc 12519 0 ip_vs_sh 12688 0 ip_vs_sed 12519 0 ip_vs_rr 12600 20 ip_vs_pe_sip 12740 0 nf_conntrack_sip 33860 1 ip_vs_pe_sipip_vs_nq 12516 0 ip_vs_lc 12516 0 ip_vs_lblcr 12922 0 ip_vs_lblc 12819 0 ip_vs_ftp 13079 0 nf_nat 26787 4 ip_vs_ftp,nf_nat_ipv4,nf_nat_ipv6,nf_nat_masquerade_ipv4ip_vs_dh 12688 0 ip_vs 141432 44 ip_vs_dh,ip_vs_lc,ip_vs_nq,ip_vs_rr,ip_vs_sh,ip_vs_ftp,ip_vs_sed,ip_vs_wlc,ip_vs_wrr,ip_vs_pe_sip,ip_vs_lblcr,ip_vs_lblcnf_conntrack 133053 10 ip_vs,nf_nat,nf_nat_ipv4,nf_nat_ipv6,xt_conntrack,nf_nat_masquerade_ipv4,nf_conntrack_netlink,nf_conntrack_sip,nf_conntrack_ipv4,nf_conntrack_ipv6libcrc32c 12644 4 xfs,ip_vs,nf_nat,nf_conntrack 请确保ipset也已经安装了，如未安装请执行yum install -y ipset安装。 安装docker-ce注： 在所有节点上进行以下操作。 配置docker-ce的yum源1$ wget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo -O /etc/yum.repos.d/docker-ce.repo 安装docker-ce12# 这里默认为18.09.6最新版本$ yum install docker-ce -y 如果要安装指定版本的docker-ce，请先查看版本yum list docker-ce --showduplicates | sort -r 配置docker加速12345$ cat &gt; /etc/docker/daemon.json &lt;&lt; EOF&#123; "registry-mirrors": ["https://yuife6vp.mirror.aliyuncs.com"]&#125;EOF 配置iptables FORWARD默认情况下docker-ce将iptables的FORWARD默认规则设置为DROP，这样会引起Kubernetes集群中跨Node的Pod无法通信。我们需要将此规则设置为默认ACCEPT。12345# 命令行设置，重启失效$ iptables -P FORWARD ACCEPT# 在docker的systemctl文件[Service]中添加ExecStartPost$ vi /usr/lib/systemd/system/docker.service ExecStartPost=/usr/sbin/iptables -P FORWARD ACCEPT 配置docker科学上网1234# 在docker的systemctl文件[Service]中添加Environment$ vi /usr/lib/systemd/system/docker.service Environment="HTTP_PROXY=192.168.20.199:1080"Envrionment="NO_PROXY=127.0.0.0/8,192.168.20.0/23" 这里需要你有科学上网的代理服务器，配置科学上网的目的是使docker能够pull gcr.io上的镜像，如果没有也可以在其他地方下载，然后通过docker tag成kubernetes需要的镜像名。 启动docker123$ systemctl daemon-reload$ systemctl start docker$ systemctl enable docker 安装kubeadm 和 kubelet注： 在所有节点上进行以下操作。 123456789$ cat &gt; /etc/yum.repos.d/kubernetes.repo &lt;&lt; EOF[kubernetes]name=Kubernetes Repositorybaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/gpgcheck=1gpgkey=1gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpgEOF 1$ yum install -y kubelet kubeadm kubectl 123# 设置开机自启动kubelet$ systemctl enable kubelet$ systemctl start kubelet kubelet负责与其他节点集群通信，并进行本节点Pod和容器生命周期的管理。kubeadm是Kubernetes的自动化部署工具，降低了部署难度，提高效率。kubectl是Kubernetes集群客户端管理工具。 部署kubernetes master节点查看kubeadm初始化配置1234567891011121314151617181920212223242526272829303132333435363738394041$ kubeadm config print init-defaultsapiVersion: kubeadm.k8s.io/v1beta1bootstrapTokens:- groups: - system:bootstrappers:kubeadm:default-node-token token: abcdef.0123456789abcdef ttl: 24h0m0s usages: - signing - authenticationkind: InitConfigurationlocalAPIEndpoint: advertiseAddress: 1.2.3.4 bindPort: 6443nodeRegistration: criSocket: /var/run/dockershim.sock name: master1 taints: - effect: NoSchedule key: node-role.kubernetes.io/master---apiServer: timeoutForControlPlane: 4m0sapiVersion: kubeadm.k8s.io/v1beta1certificatesDir: /etc/kubernetes/pkiclusterName: kubernetescontrolPlaneEndpoint: ""controllerManager: &#123;&#125;dns: type: CoreDNSetcd: local: dataDir: /var/lib/etcdimageRepository: k8s.gcr.iokind: ClusterConfigurationkubernetesVersion: v1.14.0networking: dnsDomain: cluster.local podSubnet: "" serviceSubnet: 10.96.0.0/12scheduler: &#123;&#125; 这里可以查看到当执行kubeadm init初始化集群时的配置信息，其中kubernetesVersion版本为v1.14.0，并不是我们需要的v1.14.3，所以需要手动指定，serviceSubnet指定pod的网络，我们这里等会使用flannel网络，默认是10.244.0.0/16，所以也要手动指定。imageRepository是获取镜像的仓库地址，该地址需要科学上网才能访问, 也可以在kubeadm init时使用--image-repository参数指定仓库地址。 获取依赖镜像这里手动拉取master所需要的镜像是为了等会kubeadm init时能够快些，也可以不手动拉取。在kubeadm init时也会自动拉取。1$ kubeadm config images pull 如果你能科学上网，执行kubeadm config images pull后将自动拉取kubeadm对应版本的master节点需要的镜像，拉取完需要的镜像如下：12345678910$ docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEk8s.gcr.io/kube-proxy v1.14.3 004666307c5b 8 days ago 82.1MBk8s.gcr.io/kube-controller-manager v1.14.3 ac2ce44462bc 8 days ago 158MBk8s.gcr.io/kube-apiserver v1.14.3 9946f563237c 8 days ago 210MBk8s.gcr.io/kube-scheduler v1.14.3 953364a3ae7a 8 days ago 81.6MBquay.io/coreos/flannel v0.11.0-amd64 ff281650a721 4 months ago 52.6MBk8s.gcr.io/coredns 1.3.1 eb516548c180 5 months ago 40.3MBk8s.gcr.io/etcd 3.3.10 2c4adeb21b4f 6 months ago 258MBk8s.gcr.io/pause 3.1 da86e6ba6ca1 18 months ago 742kB 当你不能科学上网时，可以手动从别的镜像仓库中下载，这样也能达到我们的要求，如下操作：12345678910111213$ vi /root/kubernetes-images.sh#!/bin/shPACKAGE=(kube-apiserver:v1.14.3 kube-controller-manager:v1.14.3 kube-scheduler:v1.14.3 kube-proxy:v1.14.3 pause:3.1 etcd:3.3.10)for pack in "$&#123;PACKAGE[@]&#125;"; do docker pull mirrorgooglecontainers/$&#123;pack&#125; docker tag docker.io/mirrorgooglecontainers/$&#123;pack&#125; k8s.gcr.io/$&#123;pack&#125; docker rmi docker.io/mirrorgooglecontainers/$&#123;pack&#125;donedocker pull coredns/coredns:1.3.1docker tag docker.io/coredns/coredns:1.3.1 k8s.gcr.io/coredns:1.3.1docker rmi docker.io/coredns/coredns:1.3.1$ sh /root/kubernetes-images.sh 初始化master集群12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667$ kubeadm init --kubernetes-version="v1.14.3" --pod-network-cidr="10.244.0.0/16"[init] Using Kubernetes version: v1.14.3[preflight] Running pre-flight checks [WARNING IsDockerSystemdCheck]: detected "cgroupfs" as the Docker cgroup driver. The recommended driver is "systemd". Please follow the guide at https://kubernetes.io/docs/setup/cri/[preflight] Pulling images required for setting up a Kubernetes cluster[preflight] This might take a minute or two, depending on the speed of your internet connection[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"[kubelet-start] Activating the kubelet service[certs] Using certificateDir folder "/etc/kubernetes/pki"[certs] Generating "front-proxy-ca" certificate and key[certs] Generating "front-proxy-client" certificate and key[certs] Generating "etcd/ca" certificate and key[certs] Generating "etcd/server" certificate and key[certs] etcd/server serving cert is signed for DNS names [master1 localhost] and IPs [192.168.20.210 127.0.0.1 ::1][certs] Generating "etcd/healthcheck-client" certificate and key[certs] Generating "etcd/peer" certificate and key[certs] etcd/peer serving cert is signed for DNS names [master1 localhost] and IPs [192.168.20.210 127.0.0.1 ::1][certs] Generating "apiserver-etcd-client" certificate and key[certs] Generating "ca" certificate and key[certs] Generating "apiserver-kubelet-client" certificate and key[certs] Generating "apiserver" certificate and key[certs] apiserver serving cert is signed for DNS names [master1 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.20.210][certs] Generating "sa" key and public key[kubeconfig] Using kubeconfig folder "/etc/kubernetes"[kubeconfig] Writing "admin.conf" kubeconfig file[kubeconfig] Writing "kubelet.conf" kubeconfig file[kubeconfig] Writing "controller-manager.conf" kubeconfig file[kubeconfig] Writing "scheduler.conf" kubeconfig file[control-plane] Using manifest folder "/etc/kubernetes/manifests"[control-plane] Creating static Pod manifest for "kube-apiserver"[control-plane] Creating static Pod manifest for "kube-controller-manager"[control-plane] Creating static Pod manifest for "kube-scheduler"[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s[apiclient] All control plane components are healthy after 15.501532 seconds[upload-config] storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace[kubelet] Creating a ConfigMap "kubelet-config-1.14" in namespace kube-system with the configuration for the kubelets in the cluster[upload-certs] Skipping phase. Please see --experimental-upload-certs[mark-control-plane] Marking the node master1 as control-plane by adding the label "node-role.kubernetes.io/master=''"[mark-control-plane] Marking the node master1 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule][bootstrap-token] Using token: ankpak.4mjrj58zunfd79mo[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster[bootstrap-token] creating the "cluster-info" ConfigMap in the "kube-public" namespace[addons] Applied essential addon: CoreDNS[addons] Applied essential addon: kube-proxyYour Kubernetes control-plane has initialized successfully!To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/configYou should now deploy a pod network to the cluster.Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/Then you can join any number of worker nodes by running the following on each as root:kubeadm join 192.168.20.210:6443 --token ankpak.4mjrj58zunfd79mo \ --discovery-token-ca-cert-hash sha256:ef3875ec75bb073fe41e480c9c4d18d18de5cec5270f45fc88f3a5dcea8deeaf 此时master集群已经初始化完成。 配置执行kubectl命令用户1234# kubernetes建议使用非root用户运行kubectl命令访问集群$ mkdir -p $HOME/.kube$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config$ sudo chown $(id -u):$(id -g) $HOME/.kube/config 配置flannel网络1234567891011$ wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml$ kubectl apply -f kube-flannel.ymlclusterrole.rbac.authorization.k8s.io/flannel createdclusterrolebinding.rbac.authorization.k8s.io/flannel createdserviceaccount/flannel createdconfigmap/kube-flannel-cfg createddaemonset.extensions/kube-flannel-ds-amd64 createddaemonset.extensions/kube-flannel-ds-arm64 createddaemonset.extensions/kube-flannel-ds-arm createddaemonset.extensions/kube-flannel-ds-ppc64le createddaemonset.extensions/kube-flannel-ds-s390x created 如果你的网络不能下载flannel的镜像，请手动修改kube-flannel.yml文件的image为registry.cn-shenzhen.aliyuncs.com/pyker/flannel:v0.11.0-amd64，下载完成后，请执行：1$ docker tag registry.cn-shenzhen.aliyuncs.com/pyker/flannel:v0.11.0-amd64 quay.io/coreos/flannel:v0.11.0-amd64 然后在运行 kubectl apply -f kube-flannel.yml 此时我们的master节点已经准备好了，可以通过kubectl get nodes查看123$ kubectl get nodesNAME STATUS ROLES AGE VERSIONmaster1 Ready master 3h11m v1.14.3 部署kubernetes node节点还记得我们上文在master节点上执行kubeadm init命令最后生成的kubeadm join命令吗？那就是我们node节点加入kubernetes集群的命令。 注： 以下命令在所有node节点上操作。 node加入集群1234567891011121314$ kubeadm join 192.168.20.210:6443 --token ankpak.4mjrj58zunfd79mo --discovery-token-ca-cert-hash sha256:ef3875ec75bb073fe41e480c9c4d18d18de5cec5270f45fc88f3a5dcea8deeafpreflight] Reading configuration from the cluster...[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'[kubelet-start] Downloading configuration for the kubelet from the "kubelet-config-1.14" ConfigMap in the kube-system namespace[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"[kubelet-start] Activating the kubelet service[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...This node has joined the cluster:* Certificate signing request was sent to apiserver and a response was received.* The Kubelet was informed of the new secure connection details.Run 'kubectl get nodes' on the control-plane to see this node join the cluster. 此时node也会拉取gcr.io上的镜像，需要科学上网，或者手动获取如下镜像： 12345$ docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEk8s.gcr.io/kube-proxy v1.14.3 004666307c5b 8 days ago 82.1MBquay.io/coreos/flannel v0.11.0-amd64 ff281650a721 4 months ago 52.6MBk8s.gcr.io/pause 3.1 da86e6ba6ca1 18 months ago 742kB 此时node已经全部加入集群。我们可以在master上执行以下命令查看集群信息：1234567# 查看当前集群节点状态$ kubectl get nodesNAME STATUS ROLES AGE VERSIONmaster1 Ready master 3h21m v1.14.3node1 Ready &lt;none&gt; 3h9m v1.14.3node2 Ready &lt;none&gt; 3h8m v1.14.3node3 Ready &lt;none&gt; 3h8m v1.14.3 123456# 查看集群组件健康状态kubectl get csNAME STATUS MESSAGE ERRORscheduler Healthy ok controller-manager Healthy ok etcd-0 Healthy &#123;"health":"true"&#125; 12345678910111213141516171819202122232425262728293031323334# 查看kube-system名称空间中所有控制器信息，以下显示了pod状态信息、service状态信息、daemonset状态信息、deployment状态信息以及replicaset状态信息。$ kubectl get all -n kube-systemNAME READY STATUS RESTARTS AGEpod/coredns-fb8b8dccf-jw556 1/1 Running 0 3h22mpod/coredns-fb8b8dccf-s69bn 1/1 Running 0 3h22mpod/etcd-master1 1/1 Running 0 3h21mpod/kube-apiserver-master1 1/1 Running 0 3h21mpod/kube-controller-manager-master1 1/1 Running 0 3h21mpod/kube-flannel-ds-amd64-9r4ds 1/1 Running 0 3h10mpod/kube-flannel-ds-amd64-gk5jt 1/1 Running 0 3h9mpod/kube-flannel-ds-amd64-gpv97 1/1 Running 0 3h9mpod/kube-flannel-ds-amd64-w4c64 1/1 Running 0 3h13mpod/kube-proxy-7jxlr 1/1 Running 0 3h9mpod/kube-proxy-hwl4v 1/1 Running 0 3h10mpod/kube-proxy-m7l8g 1/1 Running 0 3h9mpod/kube-proxy-r9n7m 1/1 Running 0 3h22mpod/kube-scheduler-master1 1/1 Running 0 3h21mNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/kube-dns ClusterIP 10.96.0.10 &lt;none&gt; 53/UDP,53/TCP,9153/TCP 3h22mNAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGEdaemonset.apps/kube-flannel-ds-amd64 4 4 4 4 4 beta.kubernetes.io/arch=amd64 3h13mdaemonset.apps/kube-flannel-ds-arm 0 0 0 0 0 beta.kubernetes.io/arch=arm 3h13mdaemonset.apps/kube-flannel-ds-arm64 0 0 0 0 0 beta.kubernetes.io/arch=arm64 3h13mdaemonset.apps/kube-flannel-ds-ppc64le 0 0 0 0 0 beta.kubernetes.io/arch=ppc64le 3h13mdaemonset.apps/kube-flannel-ds-s390x 0 0 0 0 0 beta.kubernetes.io/arch=s390x 3h13mdaemonset.apps/kube-proxy 4 4 4 4 4 &lt;none&gt; 3h22mNAME READY UP-TO-DATE AVAILABLE AGEdeployment.apps/coredns 2/2 2 2 3h22mNAME DESIRED CURRENT READY AGEreplicaset.apps/coredns-fb8b8dccf 2 2 2 3h22m 配置kubernetes使用ipvs1234567891011121314151617# 1、使用参数加载ipvs (master上操作)$ cat &gt;&gt; /etc/sysconfig/kubelet &lt;&lt; EOFKUBE_PROXY_MODE=ipvsEOF# 2、通过在线修改kube-proxy配置文件$ kubectl edit configMap kube-proxy -n kube-system ipvs: excludeCIDRs: null minSyncPeriod: 0s scheduler: "" strictARP: false syncPeriod: 30s kind: KubeProxyConfiguration metricsBindAddress: 127.0.0.1:10249 mode: "ipvs" # mode改为ipvs nodePortAddresses: null oomScoreAdj: -999 master隔离默认情况下，出于安全原因，kubernetes群集不会在master服务器上分配pod。如果您希望能够在master服务器上分配pod，例如，对于用于开发的单机Kubernetes集群，请运行：1234$ kubectl taint nodes --all node-role.kubernetes.io/master-node "test-01" untaintedtaint "node-role.kubernetes.io/master:" not foundtaint "node-role.kubernetes.io/master:" not found 这将从node-role.kubernetes.io/master包含master节点的任何节点中删除，这意味着调度程序将能够在任何地方分配pod。 未来node节点加入集群节点是运行工作负载的位置。要向群集添加新节点，请为每台计算机执行以下操作：12$ kubeadm join --token &lt;token&gt; &lt;master-ip&gt;:&lt;master-port&gt; --discovery-token-ca-cert-hash sha256:&lt;hash&gt;` 如果没有令牌，可以通过在主节点上运行以下命令来获取它：123$ kubeadm token listTOKEN TTL EXPIRES USAGES DESCRIPTION EXTRA GROUPSankpak.4mjrj58zunfd79mo 20h 2019-06-15T15:29:36+08:00 authentication,signing The default bootstrap token generated by 'kubeadm init'. system:bootstrappers:kubeadm:default-node-token 默认情况下，令牌在24小时后过期。如果在当前令牌过期后将节点加入群集，则可以通过在master节点上运行以下命令来创建新令牌：12$ kubeadm token create5didvk.d09sbcov8ph2amjw 如果没有值--discovery-token-ca-cert-hash，可以通过在主节点上运行以下命令链来获取它：12$ openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2&gt;/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //'8cb2de97839780a412b93877f8507ad6c94f73add17d5d7058e91741c9d5ec78 注意：要指定IPv6的master地址&lt;master-ip&gt;:&lt;master-port&gt;，必须将IPv6地址括在方括号中，例如：[fd00::101]:2073。 几秒钟后，您应该可以看到在master服务器上运行kubectl get nodes将会输出此节点信息。也可以直接执行以下命令，获取加入集群节点的命令：12# 这条命令比刚刚通过手动生成效率更高$ kubeadm token create --print-join-command]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes 核心概念介绍]]></title>
    <url>%2F2019%2F06%2F11%2Fk8s-introduce.html</url>
    <content type="text"><![CDATA[Kubernetes是什么Kubernetes是容器集群管理系统，是一个开源的平台，可以实现容器集群的自动装箱、自动修复、水平扩展、服务发现、负载均衡、自动发布和回滚等功能。 可移植 : 支持公有云，私有云，混合云，多重云（multi-cloud） 可扩展 : 模块化, 插件化, 可挂载, 可组合 自动化 : 自动部署，自动重启，自动复制，自动伸缩/扩展 kubernetes基本架构 Kubernetes各个组件kubernetes采用master/node的拓扑结构，因此master和node节点上的组件不一样，下面我们详细说明两者节点上的组件功能。 Master节点组件Master节点上的组件提供集群的管理控制中心。 在Master节点上面主要由四个组件组成：APIServer、scheduler、controller-manager、etcd。当然etcd不属于kubernetes的自带的组件，而是属于第三方的以key/value存储形式的存储组件，该组件也可以单独部署到其他独立的节点上运行。 APIServerAPIServer负责对外提供RESTful的Kubernetes API服务，它是系统管理指令的统一入口，任何对资源进行GET、PUT 、DELETE 、POST 的操作都要交给APIServer处理后再提交给etcd。如架构图中所示，kubectl（Kubernetes提供的客户端工具，该工具内部就是对Kubernetes API的调用）是直接和APIServer交互的。 schedulerscheduler的职责很明确，就是负责调度pod到合适的Node上。如果把scheduler看成一个黑匣子，那么它的输入是pod和由多个Node组成的列表，输出是Pod和一个Node的绑定，即将这个pod部署到这个Node上。Kubernetes目前提供了调度算法，但是同样也保留了接口，用户可以根据自己的需求定义自己的调度算法。 controller-manager如果说APIServer做的是“前台”的工作的话，那controller-manager就是负责“后台”的。每个资源一般都对应有一个控制器，而controller-manager就是负责管理这些控制器的。比如我们通过APIServer创建一个pod，当这个pod创建成功后，APIServer的任务就算完成了。而后面保证Pod的状态始终和我们预期的一样的重任就由controller-manager去保证了。 etcdetcd是一个高可用的键值存储系统，Kubernetes使用它来存储各个资源的状态，从而实现了Restful的API。对于kubernetes集群来说他的客户端只有apiserver，任何资源的变动都要提交给apiserver，由apiserver当etcd的客户端写入etcd。 Node节点组件每个Node节点主要由三个模块组成：kubelet、kube-proxy、runtime。 runtimeruntime指的是容器运行环境，目前Kubernetes最主流的runtime就是docker了。 kube-proxy该模块实现了Kubernetes中的服务发现和反向代理功能。反向代理方面：kube-proxy支持TCP和UDP连接转发，默认基于Round Robin算法将客户端流量转发到与service对应的一组后端pod。服务发现方面，kube-proxy使用etcd的watch机制，监控集群中service和endpoint对象数据的动态变化，并且维护一个service到endpoint的映射关系，从而保证了后端pod的IP变化不会对访问者造成影响。另外kube-proxy还支持session affinity。 kubeletKubelet是Master在每个Node节点上面的agent，是Node节点上面最重要的模块，它负责维护容器的生命周期，同时也负责 Volume（CVI）和网络（CNI）的管理，并将状态信息反馈给master。但是如果容器不是通过Kubernetes创建的，它并不会管理。 插件（Addons）部署 Kubernetes 集群后，还需要部署一系列的附加组件（addons），这些组件通常是保证集群功能正常运行必不可少的。通常使用addon-manager来管理集群中的附加组件。它运行在 Kubernetes集群Master节点中，管理着所有扩展，保证它们始终运行在期望状态。必须安装的插件组件有： coredns: 他为整个kubernetes集群提供DNS解析，而且是动态的DNS解析，他会随着pod或者service的配置变动而动态解析。 flannel和calio： 比较主流的两款为kubernetes提供跨主机节点的网络通信的CNI (container network interface)。 kubernetes-dashboard：为kubernetes提供web界面的管理方式。 metrics-server: 监控整个kubernetes集群资源状态数据信息，代替早期1.11版本前的heapster。 Kubernetes对象资源对象资源是Kubernetes集群中的管理操作单元。基本的资源对象有：Pod 、Service 、Namespace 、Job，更高级的资源对象有：Deployment 、DaemonSet 、StatefulSet 、ConfigMap、 Volume 等。下面我们将对这些资源进行说明。 在kubernetes中所有的配置定义都应该是声明式（Declarative）的而不是命令式（Imperative）的。 PodPod 是Kubernetes的最小基本操作单元，也是应用运行的载体。整个Kubernetes系统都是围绕着Pod展开的，比如如何部署运行Pod、如何保证Pod的数量、如何访问Pod等。另外，一个Pod是一个或多个紧密关联容器的集合，一个Pod也是一个隔离体，而Pod内部包含的一组容器又是共享的（包括PID、Network、IPC、UTS）。除此之外，一个Pod中的容器可以访问共同的数据卷来实现文件系统的共享。 创建Pod通常把创建Pod分为两类： 自主式Pod： 这种Pod本身是不能自我修复的，当Pod被创建后，会被Kuberentes调度到集群的Node上。直到Pod的进程终止、被删掉、因为缺少资源而被驱逐、Node故障或者调度器本身故障时，这个Pod就会被删除，不会自动重建。 控制器管理的Pod： Kubernetes使用更高级的称为Controller的抽象层，来管理Pod实例。Controller可以创建和管理多个Pod，提供副本管理、滚动升级和集群级别的自愈能力。例如，如果一个Node故障，Controller就能自动将该节点上的Pod调度到其他健康的Node上。 注: 每个Pod都有一个基础的Pause容器，Pause容器对应的镜像属于Kubernetes平台的一部分，而Pod中的业务容器正是共享了该基础容器的PID UTS Network IPC。 Pod生命周期Pod被分配到一个Node上之后，就不会离开这个Node，直到被删除。当某个Pod失败，首先会被Kubernetes清理掉，之后Pod对应的控制器将会在其它机器上（或本机）重建Pod，重建之后Pod的ID发生了变化，那将会是一个新的Pod。所以，Kubernetes中Pod的迁移，实际指的是在新Node上重建Pod。它生命周期有如下阶段(phase)： Pending: API Server创建了Pod资源对象并已经存入了etcd中，但是它并未被调度完成，或者仍然处于从仓库下载镜像的过程中。 Running: Pod已经被调度到了某个节点，所有容器已经被创建完成。 Successded: Pod中的所有容器成功终止，也不会重启。 Failed: 所有容器终止，至少有一个容器以失败方式终止。也就是说，这个容器要么已非 0 状态退出，要么被系统终止。 Unknown: 由于一些原因，Pod 的状态无法获取，通常是与 Pod 通信时出错导致的。 下图是Pod的生命周期示意图，从图中可以看到Pod状态的变化。 另外在pod生命周期中可以做的一些事情。主容器启动前可以完成初始化容器，初始化容器可以有多个，他们是串行执行的，执行完成后就退出了，在主容器刚刚启动的时候可以指定一个post start 主容器启动开始后执行一些操作，在主容器结束前可以指定一个pre stop 表示主容器结束前执行的一些操作。在容器启动后可以做两类检测 livenessprobe（存活性探测）和 readinessprobe（就绪性探测）。如下图： Service为了适应快速的业务需求，微服务架构已经逐渐成为主流，微服务架构的应用需要有非常好的服务编排支持。Kubernetes中的核心要素Service便提供了一套简化的服务发现和负载均衡，天然适应微服务架构。在Kubernetes中，当Deployment创建pod后，Pod副本是变化的，对于的Pod IP也是变化的，比如发生迁移或者伸缩的时候。这对于Pod的访问者来说是不可接受的。Kubernetes中的Service是一种抽象概念，它定义了一个Pod逻辑集合以及访问它们的策略，Service同Pod的关联同样是居于Label标签来完成的。Service的目标是提供一种桥梁， 它会为访问者提供一个固定访问地址，我们称它为ClusterIP或者ServiceIP，该IP是Kubernetes通过iptables规则做转发分配给Service的一个虚拟IP，用于Kube-proxy组件来实现的虚拟IP路由及负载均衡访问后端相应的endPoint。外部访问Pod，Kubernetes提供了NodePort、LoadBalancer、Ingress三种转发方式： NodePort: Kubernetes会在每一个Node上暴露出一个端口：nodePort，与之关联serviceIP，外部网络可以通过（任一Node）[NodeIP]:[NodePort]访问到后端的Service。 LoadBalancer： 在NodePort基础上，Kubernetes可以请求底层云平台创建一个负载均衡器，将每个Node作为后端，进行服务分发。该模式需要底层云平台（例如GCE）支持。 Ingress: Ingress通过http代理服务器将外部的http请求转发到集群内部的后端服务。Ingress Controller实时监控Kubernetes API，实时更新HTTP代理服务器的转发规则。 DeploymentDeployment中描述你所期望的集群状态，用于管理Pod，其实真正控制Pod的是ReplicaSet，而Deployment控制的是ReplicaSet。也就是说 Deployment 是通过 ReplicaSet 来管理 Pod 的多个副本，我们通常不需要直接使用 ReplicaSet。 当更新时Deployment Controller会将该控制器管理的Pod逐步更新成你所期望的集群状态。Deployment主要职责同样是为了保证pod的数量和健康，90%的功能与Replication Controller（弃用）完全一样，可以看做新一代的Replication Controller。但是，它又具备了Replication Controller之外的新特性： Replication Controller全部功能：Deployment继承了Replication Controller全部功能。 事件和状态查看：可以查看Deployment的升级详细进度和状态。 回滚：当升级pod镜像或者相关参数的时候发现问题，可以使用回滚操作回滚到上一个稳定的版本或者指定的版本。 版本记录: 每一次对Deployment的操作，都能保存下来，给予后续可能的回滚使用。 暂停和启动：对于每一次升级，都能够随时暂停和启动。比如金丝雀发布。 多种升级方案：Recreate—-删除所有已存在的pod,重新创建新的; RollingUpdate—-滚动升级，逐步替换的策略，同时滚动升级时，支持更多的附加参数，例如设置最大不可用pod数量，最小升级间隔时间等等。 ReplicaSetReplicaSet是用来来取代 ReplicationController。ReplicaSet 跟 ReplicationController 没有本质的不同，只是名字不一样，并且 ReplicaSet 支持集合式的 selector（ReplicationController 仅支持等式），虽然 ReplicaSet也可以独立使用，但是我们日常都使用 Deployment 来自动管理 ReplicaSet。 而当对Pod进行升级时，旧的Replicaset也一直存在，Deployment默认可以管理10个Replicaset用于回滚操作。 DaemonSetDaemonSet确保所有（或某些标记为污点）节点都只运行一个Pod副本。 Pod随着节点添加而自动添加一个Pod到该节点。 随着节点从群集中删除而自动从该节点删除。 删除DaemonSet将清除它创建的所有Pod。它和Deployment一样也是一个无状态的控制器。DaemonSet的一些典型用法是： 运行集群存储后台守护程序，在每个节点上运行如：glusterd，ceph。 在每个节点上运行日志收集守护程序，例如：fluentd或logstash。 在每个节点上运行节点监视守护程序，例如：Prometheus Node Exporter，Sysdig Agent，collectd，Dynatrace OneAgent，AppDynamics Agent，Datadog proxy，New Relic proxy，Ganglia gmond或Instana Agent。 StatefuleSet在云原生应用的体系里，有下面两组近义词；第一组是无状态（stateless）、牲畜（cattle）、无名（nameless）、可丢弃（disposable）；第二组是有状态（stateful）、宠物（pet）、有名（having name）、不可丢弃（non-disposable）。Deployment和RS主要是控制提供无状态服务的，其所控制的Pod的名字是随机设置的，一个Pod出故障了就被丢弃掉，在另一个地方重启一个新的Pod，名字变了、名字和启动在哪儿都不重要，重要的只是Pod总数。而StatefuleSet是用来控制有状态服务，能够保证 Pod 的每个副本在整个生命周期中名称是不变的。且StatefuleSet 会保证副本在启动时按照固定的顺序启动，在更新或者删除会按照逆序进行。 那么适用于StatefulSet业务包括数据库服务MySQL和PostgreSQL，集群化管理服务Zookeeper、etcd等有状态服务。StatefuleSet的另一种典型应用场景是作为一种比普通容器更稳定可靠的模拟虚拟机的机制。传统的虚拟机正是一种有状态的宠物，运维人员需要不断地维护它，容器刚开始流行时，我们用容器来模拟虚拟机使用，所有状态都保存在容器里，而这已被证明是非常不安全、不可靠的。使用StatefuleSet，Pod仍然可以通过漂移到不同节点提供高可用，而存储也可以通过外挂的存储来提供高可靠性，StatefuleSet做的只是将确定的Pod与确定的存储关联起来保证状态的连续性。 ConfigMap在很多业务环境中的应用程序配置较为复杂，可能需要多个配置文件、命令行参数和环境变量的组合。并且，这些配置信息应该从应用程序镜像中解耦出来，以保证镜像的可移植性以及配置信息不被泄露。社区引入ConfigMap这个API资源来满足这一需求。ConfigMap包含了一系列的键值对，用于存储被Pod或者系统组件（如controller）访问的信息。这与secret的设计理念有异曲同工之妙，它们的主要区别在于ConfigMap通常不用于存储敏感信息，而只存储简单的文本信息。 SecretKubernetes的Secret对象允许您存储和管理敏感信息，例如：密码，OAuth令牌和ssh密钥。 将此信息存放于Secret中比将其逐字地放入Pod定义或容器映像中更安全，更灵活。 目前secret支持创建以下三种类型的secret： docker-resgistry: 创建一个给 Docker registry 使用的 secret generic: 从本地 file, directory 或者 literal value 创建一个 secret tls: 创建一个 TLS secret JobJob是创建一个或多个Pod并确保该pod成功运行后终止。 当pod成功完成后，Job会跟踪成功的完成情况。 达到指定数量的成功完成时，Job（即作业）完成。 该Pod将自动删除。通常运行一次性的脚本或者备份数据可以使用该类型。 Volume在Docker的设计实现中，容器中的数据是临时的，即当容器被销毁时，其中的数据将会丢失。如果需要持久化数据，需要使用Docker数据卷挂载宿主机上的文件或者目录到容器中。在Kubernetes中，当Pod重建的时候，数据也是会丢失的，Kubernetes也是通过数据卷挂载来提供Pod数据的持久化的。Kubernetes数据卷是对Docker数据卷的扩展，Kubernetes数据卷是Pod级别的，可以用来实现Pod中容器的文件共享。kubernetes支持的挂载卷使用以下命令查看：1$ kubectl explain pod.spec.volumes EmptyDir如果Pod配置了EmpyDir数据卷，在Pod的生命周期内都会存在，当Pod被分配到 Node上的时候，会在Node上创建EmptyDir数据卷，并挂载到Pod的容器中。只要Pod 存在，EmpyDir数据卷都会存在, 但是如果Pod的生命周期终结（Pod被删除），EmpyDir数据卷也会被删除，并且永久丢失。 HostPathHostPath数据卷允许将容器宿主机上的文件系统挂载到Pod中。如果Pod需要使用宿主机上的某些文件，可以使用HostPath，此时Pod被删除数据不会被丢失，只有对应的Node节点挂掉，数据才丢失。或Pod重建时被分配到其他节点上后，该数据也不能使用。 网络数据卷Kubernetes提供了很多类型的数据卷以集成第三方的存储系统，包括一些非常流行的分布式文件系统，也有在IaaS平台上提供的存储支持，这些存储系统都是分布式的，通过网络共享文件系统，因此我们称这一类数据卷为网络数据卷。网络数据卷能够满足数据的持久化需求，Pod通过配置使用网络数据卷，每次Pod创建的时候都会将存储系统的远端文件目录挂载到容器中，数据卷中的数据将被水久保存，即使Pod被删除，只是除去挂载数据卷，数据卷中的数据仍然保存在存储系统中，且当新的Pod被创建的时候，仍是挂载同样的数据卷。网络数据卷包含以下几种：NFS、iSCISI、GlusterFS、RBD（Ceph Block Device）、Flocker、AWS Elastic Block Store、GCE Persistent Disk。 PersistentVolume（PV）和PersistentVolumeClaim(PVC)理解每个存储系统是一件复杂的事情，特别是对于普通用户来说，有时候并不需要关心各种存储实现，只希望能够安全可靠地存储数据。Kubernetes中提供了Persistent Volume和Persistent Volume Claim机制，这是存储消费模式。Persistent Volume是由系统管理员配置创建的一个数据卷，它代表了某一类存储插件实现；而对于普通用户来说，通过Persistent Volume Claim可请求并获得合适的Persistent Volume，而无须感知后端的存储实现。Persistent Volume和Persistent Volume Claim的关系其实类似于Pod和Node，Pod消费Node资源，Persistent Volume Claim则消费Persistent Volume资源。Persistent Volume和Persistent Volume Claim相互关联，有着完整的生命周期管理。]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch7.0详解]]></title>
    <url>%2F2019%2F05%2F20%2Felasticsearch.html</url>
    <content type="text"><![CDATA[什么是ElasticsearchElasticsearch是一个高度可扩展的开源全文搜索和分析引擎。 它允许您快速，近乎实时地存储，搜索和分析大量数据。 它通常用作底层引擎/技术，本身扩展性很好，可以扩展到上百台服务器，处理PB级别的数据。 为具有复杂搜索特性和需求的应用程序提供支持。Elasticsearch也使用Java开发并使用Lucene作为其核心来实现所有索引和搜索的功能，但是它的目的是通过简单的RESTful API来隐藏Lucene的复杂性，从而让全文搜索变得简单。 Elasticsearch能干什么 您运行在线网上商店，允许您的客户搜索您销售的产品。 在这种情况下，您可以使用Elasticsearch存储整个产品目录和库存，并为它们提供搜索和自动填充建议。 您希望收集日志或交易数据，并且希望分析和挖掘此数据以查找趋势，统计信息，摘要或异常。 在这种情况下，您可以使用Logstash（Elasticsearch / Logstash / Kibana堆栈的一部分）来收集，聚合和解析数据，然后让Logstash将此数据提供给Elasticsearch。 一旦数据在Elasticsearch中，您就可以运行搜索和聚合来挖掘您感兴趣的任何信息。 您运行价格提醒平台，允许对价格明确的客户指定一条规则，例如“我有兴趣购买特定的电子产品，如果某个供应商的电子产品价格在下个月内跌破X美元，我希望得到通知”。在这种情况下，您可以抓取供应商价格，将其推入Elasticsearch并使用其反向搜索（Percolator）功能来匹配价格变动与客户查询，并最终在发现匹配后将警报推送给客户。 您有分析/商业智能需求，并希望快速调查，分析，可视化并询问有关大量数据的特定问题（想想数百万或数十亿条记录）。 在这种情况下，您可以使用Elasticsearch存储数据，然后使用Kibana（Elasticsearch / Logstash / Kibana堆栈的一部分）构建自定义仪表板，该仪表板可以可视化对您来说重要的数据。 此外，您可以使用Elasticsearch聚合功能针对您的数据执行复杂的商业智能查询。 Elasticsearch核心概念Near-realtime(NRT)： 近实时 Elasticsearch是一个近实时搜索平台。 这意味着从索引文档到可搜索文档的时间有一点延迟（通常是一秒）。 Cluster：集群 cluster是一个或多个节点（服务器）的集合，它们共同保存您的整个数据，并提供跨所有节点的联合索引和搜索功能。 群集由唯一名称标识，默认情况下为“elasticsearch”。 这个标识很重要，节点会根据这个标识加入到一个集群。 确保不要在不同的环境中使用相同的群集名称，这会导致节点加入错误的集群。例如，您可以将logging-dev，logging-stage和logging-prod用于开发，测试和生产集群。 请注意，如果一个集群只有一个节点也是完全可行的，此外你也可以建立多个独立的集群，每个集群有自己的唯一标识。 Node：节点 节点是作为群集一部分的单个服务器，用于存储集群数据并为集群提供索引和搜索能力。 与集群一样，每个节点也有一个唯一名称标识，在节点启动的时候会默认分配给它一个随机通用唯一标识(UUID)。 你也可以自定义节点标识，以代替默认标识。在集群的管理中，可以使用节点标识来对应网络中的服务器名称。 节点可以根据配置的集群标识加入指定的集群。一般来说，每个节点在启动之后会默认加入一个叫elasticsearch的集群。假如在一个节点可以相互发现对方的网络环境中，启动多个节点，他们会自动加入到一个标识为elasticsearch的集群。 在一个独立的集群中，你可以建立任意个数的节点。此外，如果当前网络环境中没有其他的elasticsearch节点在运行，那么单独启动一个节点会形成一个单节点的集群，其集群标识为elasticsearch Index：索引索引是具有某些类似特征的文档（Document）集合。 例如，可以为客户数据建立索引，为产品目录建立另一个索引，为订单数据建立另一个索引。索引由名称标识(必须全部为小写)，当对其中的文档执行索引、搜索、更新和删除操作时，需要引用索引名称。 Type：类型warning :在6.0.0后弃用 它曾经是索引的逻辑类别/分区，允许您在同一索引中存储不同类型的文档，例如，一种用户类型，另一种用于博客帖子。 不再可能在索引中创建多个类型。在Elasticsearch 7.0.0或更高版本中创建的索引不再接受default映射。 在6.x中创建的索引将继续像以前一样在Elasticsearch 6.x中运行。 在7.0中的API中不推荐使用type，对索引创建，放置映射，获取映射，放置模板，获取模板和获取字段映射API进行重大更改。详见Removal of mapping types Document：文档 Document是可以被索引的基本信息单元。 例如，您可以为单个客户提供文档，为单个产品提供另一个文档，为单个订单提供另一个文档。 该文档以JSON格式表示。在索引（index）或者类型（type）中可以存储任意多的文档。尽管文档实际存储在索引中，但是使用中必须把文档编入或者分配到一个索引的类型中。 Shard：分片 一个索引index可能会存储大量的数据，大到超过一个节点的存储能力。例如：一个占用1T磁盘存储空间的索引，对单节点来说可能会太大，或者造成查询请求太慢。 为了解决这个问题，Elasticsearch提供了将索引细分为多个称为分片(shard)的功能。 创建索引时，只需定义所需的分片数即可。 每个分片本身都是一个功能齐全且独立的“索引”，每个分片可以放到不同的服务器上。 当你查询的索引分布在多个分片上时，Elasticsearch会把查询发送给每个相关的分片，并将结果组合在一起，而应用程序并不知道分片的存在。即：这个过程对用户来说是透明的。 Replicas：副本 为提高查询吞吐量或实现高可用性，可以使用分片副本。 副本是一个分片的精确复制，每个分片可以有零个或多个副本。Elasticsearch会中可以有许多相同的分片，其中之一被选择更改索引操作，这种特殊的分片称为主分片。 当主分片丢失时，如：该分片所在的数据不可用时，集群将副本提升为新的主分片。 Full-text-search：全文检索全文检索就是对一篇文章进行索引，可以根据关键字搜索，类似于mysql里的like语句。 全文索引就是把内容根据词的意义进行分词，然后分别创建索引，例如“你们的激情是因为什么事情来的” 可能会被分词成：“你们“，”激情“，“什么事情“，”来“ 等token，这样当你搜索“你们” 或者 “激情” 都会把这句搜出来。 Elasticsearch的安装由于上篇文章已经对elasticsearch7.0.1版本进行了安装，用户可以参考该安装方式，进行多节点集群安装部署。 Restful API接口能干什么Elasticsearch提供了一个非常全面和强大的REST API，可以使用它与集群进行交互。使用该API可以做一些事情如下: 检查群集，节点和索引运行状况，状态和统计信息 管理您的群集，节点和索引数据和元数据 对索引执行CRUD（创建，读取，更新和删除）和搜索操作 执行高级搜索操作，例如分页，排序，过滤，脚本编写，聚合等等 下面我们将通过使用Restful API接口风格来使用Elasticsearch。 集群信息检查通过上面对elasticsearch的了解后，我们现在可以从检查集群基本运行状况开始，elasticsearch支持curl、HTTP\REST以及Kibana开发控制台上来操作elasticsearch。我们这里使用curl方式进行操作,并且使用Elastic stack之EFK安装的elasticsearch集群服务器。 集群健康123456789101112131415161718192021222324# 通过响应我们可以观察到集群 efk-cluster除于green状态，节点总数2个，节点存储2个，分片数38，主分片19$ curl -XGET "http://192.168.20.211:9200/_cat/health?v"epoch timestamp cluster status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent1559096592 02:23:12 efk-cluster green 2 2 38 19 0 0 0 0 - 100.0%# 也可以这样操作获取集群信息状态$ curl -XGET http://192.168.20.211:9200/_cluster/health?pretty&#123; "cluster_name" : "efk-cluster", "status" : "green", "timed_out" : false, "number_of_nodes" : 2, "number_of_data_nodes" : 2, "active_primary_shards" : 19, "active_shards" : 38, "relocating_shards" : 0, "initializing_shards" : 0, "unassigned_shards" : 0, "delayed_unassigned_shards" : 0, "number_of_pending_tasks" : 0, "number_of_in_flight_fetch" : 0, "task_max_waiting_in_queue_millis" : 0, "active_shards_percent_as_number" : 100.0&#125; green — 一切都很好（集群功能齐全） yellow — 所有数据都可用，但尚未分配一些副本（群集功能齐全） red — 某些数据由于某种原因不可用（群集部分功能） 集群详细信息12345678910111213141516171819202122232425262728# 这将获得集群和节点的名称以及UUID，及一些其他属性$ curl -XGET http://192.168.20.211:9200/_cluster/state/nodes?pretty&#123; "cluster_name" : "efk-cluster", "cluster_uuid" : "93hPHHs9SGu6EqtOJCpQbA", "nodes" : &#123; "eKK8iLHjSx2pW9yCfIwacA" : &#123; "name" : "efk-node2", "ephemeral_id" : "Goc8GevQThakZ7y3GY4oLA", "transport_address" : "192.168.20.212:9300", "attributes" : &#123; "ml.machine_memory" : "8353046528", "ml.max_open_jobs" : "20", "xpack.installed" : "true" &#125; &#125;, "fK9VZWuJTWKBhFlXgHLkRA" : &#123; "name" : "efk-node1", "ephemeral_id" : "483CBL_zRwyqKI3QUTwyUg", "transport_address" : "192.168.20.211:9300", "attributes" : &#123; "ml.machine_memory" : "8353046528", "xpack.installed" : "true", "ml.max_open_jobs" : "20" &#125; &#125; &#125;&#125; 集群master状态123456789101112# 和查看集群信息类似，这里只查看集群master信息$ curl -XGET http://192.168.20.211:9200/_cluster/state/master_node?pretty&#123; "cluster_name" : "efk-cluster", "cluster_uuid" : "93hPHHs9SGu6EqtOJCpQbA", "master_node" : "fK9VZWuJTWKBhFlXgHLkRA"&#125;#也可以这样获取mster id ip信息$ curl -XGET http://192.168.20.211:9200/_cat/master?vid host ip nodefK9VZWuJTWKBhFlXgHLkRA 192.168.20.211 192.168.20.211 efk-node1 集群节点状态12345# 可以查询到集群的节点数，堆、运存、cpu、平均负载等信息，其中efk-node1为* 表示为ES主节点$ curl -XGET http://192.168.20.211:9200/_cat/nodes?vip heap.percent ram.percent cpu load_1m load_5m load_15m node.role master name192.168.20.212 57 56 0 0.07 0.09 0.07 mdi - efk-node2192.168.20.211 50 56 0 0.05 0.07 0.05 mdi * efk-node1 查看集群索引1234# 这里没有粘贴出.开头的索引，只列出自建的alading索引，这样方便看出索引健康、状态、主/副本数等信息$ curl -XGET http://192.168.20.211:9200/_cat/indices?vhealth status index uuid pri rep docs.count docs.deleted store.size pri.store.sizegreen open alading JEG96RCIRkOgzC-JQudF5Q 1 1 1 0 7kb 3.5kb Elasticsearch的CRUD操作通过上面的介绍，相信大家已经知道如何和elasticsearch数据通信了。下面我们将简单介绍如何在ES中进行增删改查操作。而Elasticsearch的Restful API对应的增删改查为：GET、POST、PUT、DELETE、HEAD，他们的含义分别为： GET：获取请求对象的当前状态。 POST：改变对象的当前状态。 PUT：创建一个对象。 DELETE：删除对象。 HEAD：请求获取对象的基础信息。 如果Elasticsearch的CRUD和mysql的概念关系对比的话，可以理解为下表： Mysql Elasticsearch Database Index Table Type Row Document Column Field Schema Mapping Index Everything is indexed SQL Query DSL Select * from table … GET http://… Update table Set … PUT http://… 创建索引1234567# 例如我们创建一个customer的索引$ curl -XPUT http://192.168.20.211:9200/customer?pretty&#123; "acknowledged" : true, "shards_acknowledged" : true, "index" : "customer"&#125; pretty为以美观的json格式输出。 此时我们查看当前已建立的索引可以发现customer12345# alading为我们之前建立的，customer为我们刚刚建立的$ url -XGET http://192.168.20.211:9200/_cat/indices?vhealth status index uuid pri rep docs.count docs.deleted store.size pri.store.sizegreen open alading JEG96RCIRkOgzC-JQudF5Q 1 1 1 0 7kb 3.5kbgreen open customer aayvk0MRTP2OMaBOcSF0DQ 1 1 0 0 566b 283b 添加索引文档12345678910111213141516# 我们在customer索引中建立一个id为1的文档$ curl -H "Content-Type: application/json" -XPUT http://192.168.20.211:9200/customer/_doc/1?pretty -d '&#123;"name":"pyker"&#125;'&#123; "_index" : "customer", "_type" : "_doc", "_id" : "1", "_version" : 1, "result" : "created", "_shards" : &#123; "total" : 2, "successful" : 2, "failed" : 0 &#125;, "_seq_no" : 1, "_primary_term" : 1&#125; 查询索引文档1234567891011121314# 这里查询上一步骤建立的索引为customer，类型为_doc，id为1的文档$ curl -XGET http://192.168.20.211:9200/customer/_doc/1?pretty&#123; "_index" : "customer", "_type" : "_doc", "_id" : "1", "_version" : 1, "_seq_no" : 0, "_primary_term" : 1, "found" : true, "_source" : &#123; "name" : "pyker" &#125;&#125; _version每一次操作都会加1，found为true表示找到了该文档，_source返回我们从索引中检出的完整JSON文档。 更改索引文档内容12345678910111213141516# 执行方式和前面一样，指定要操作的索引、类型、文档id，然后修改需要修改的字段名和值。$ curl -H "Content-Type: application/json" -XPOST http://192.168.20.211:9200/customer/_doc/1?pretty -d '&#123;"name":"pyker zhang"&#125;'&#123; "_index" : "customer", "_type" : "_doc", "_id" : "1", "_version" : 2, "result" : "updated", "_shards" : &#123; "total" : 2, "successful" : 2, "failed" : 0 &#125;, "_seq_no" : 2, "_primary_term" : 1&#125; 1234567891011121314# 更新后我们在查询，发现name字段值已经被更改了，并且_version的值也加1了。$ curl -XGET http://192.168.20.211:9200/customer/_doc/1?pretty&#123; "_index" : "customer", "_type" : "_doc", "_id" : "1", "_version" : 2, "_seq_no" : 2, "_primary_term" : 1, "found" : true, "_source" : &#123; "name" : "pyker zhang" &#125;&#125; 删除索引文档按照上面的步骤,我们在建立一个id为2的文档123456789101112131415$ curl -H "Content-Type: application/json" -XPUT http://192.168.20.211:9200/customer/_doc/2?pretty -d '&#123;"name":"jack"&#125;'&#123; "_index" : "customer", "_type" : "_doc", "_id" : "2", "_version" : 1, "result" : "created", "_shards" : &#123; "total" : 2, "successful" : 2, "failed" : 0 &#125;, "_seq_no" : 1, "_primary_term" : 1&#125; 12345678910111213$ curl -XGET http://192.168.20.211:9200/customer/_doc/2?pretty&#123; "_index" : "customer", "_type" : "_doc", "_id" : "2", "_version" : 1, "_seq_no" : 1, "_primary_term" : 1, "found" : true, "_source" : &#123; "name" : "jack" &#125;&#125; 那么此时如果我们要删除这个文档进行如下操作12345678910111213141516# 可以发现_version为第二次操作，result结果为deleted，表示该文档已经被删除$ curl -XDELETE http://192.168.20.211:9200/customer/_doc/2?pretty&#123; "_index" : "customer", "_type" : "_doc", "_id" : "2", "_version" : 2, "result" : "deleted", "_shards" : &#123; "total" : 2, "successful" : 2, "failed" : 0 &#125;, "_seq_no" : 3, "_primary_term" : 1&#125; 此时你在去customer中查找id为2的文档就会显示没找到，如下12345678# found为false表示该文档没有找到$ curl -XGET http://192.168.20.211:9200/customer/_doc/2?pretty&#123; "_index" : "customer", "_type" : "_doc", "_id" : "2", "found" : false&#125; ES批处理操作通过上面的操作都是基于单个文档进行的，Elasticsearch也提供了使用 _bulk API 批量执行上述任何操作的功能。这个功能是非常重要的，因为它提供了一个非常有效的机制来尽可能快地进行多个操作，并且尽可能减少网络的往返行程。简单举个例子，下面会在一个 bulk操作中索引两个文档：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# 通过_bulk我们同时创建了id为3和4 name为John Doe和Jane Doe的两个文档$ curl -H "Content-Type: application/json" -XPOST http://192.168.20.211:9200/customer/_doc/_bulk?pretty -d '&#123;"index":&#123;"_id":"3"&#125;&#125;&#123;"name": "John Doe" &#125;&#123;"index":&#123;"_id":"4"&#125;&#125;&#123;"name": "Jane Doe" &#125;'&#123; "took" : 4, "errors" : false, "items" : [ &#123; "index" : &#123; "_index" : "customer", "_type" : "_doc", "_id" : "3", "_version" : 1, "result" : "created", "_shards" : &#123; "total" : 2, "successful" : 2, "failed" : 0 &#125;, "_seq_no" : 6, "_primary_term" : 1, "status" : 201 &#125; &#125;, &#123; "index" : &#123; "_index" : "customer", "_type" : "_doc", "_id" : "4", "_version" : 1, "result" : "created", "_shards" : &#123; "total" : 2, "successful" : 2, "failed" : 0 &#125;, "_seq_no" : 7, "_primary_term" : 1, "status" : 201 &#125; &#125; ]&#125; 下面语句批处理执行增加id为2的数据然后执行删除id为3的数据，并且更新id为4的数据1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465# 可以清楚的看到各_id的result值对应着我们的增加、删除、更新需求。$ curl -H "Content-Type: application/json" -XPOST http://192.168.20.211:9200/customer/_doc/_bulk?pretty -d '&#123;"index":&#123;"_id":"2"&#125;&#125;&#123;"name": "jack" &#125;&#123;"delete":&#123;"_id":"3"&#125;&#125;&#123;"update": &#123;"_id":"4"&#125;&#125;&#123;"doc":&#123;"name":"John Doe becomes Jane Doe"&#125;&#125; '&#123; "took" : 7, "errors" : false, "items" : [ &#123; "index" : &#123; "_index" : "customer", "_type" : "_doc", "_id" : "2", "_version" : 1, "result" : "created", "_shards" : &#123; "total" : 2, "successful" : 2, "failed" : 0 &#125;, "_seq_no" : 12, "_primary_term" : 1, "status" : 201 &#125; &#125;, &#123; "delete" : &#123; "_index" : "customer", "_type" : "_doc", "_id" : "3", "_version" : 2, "result" : "deleted", "_shards" : &#123; "total" : 2, "successful" : 2, "failed" : 0 &#125;, "_seq_no" : 13, "_primary_term" : 1, "status" : 200 &#125; &#125;, &#123; "update" : &#123; "_index" : "customer", "_type" : "_doc", "_id" : "4", "_version" : 2, "result" : "updated", "_shards" : &#123; "total" : 2, "successful" : 2, "failed" : 0 &#125;, "_seq_no" : 14, "_primary_term" : 1, "status" : 200 &#125; &#125; ]&#125; 通过上面的操作，现在我们来验证一下结果1234567891011121314151617181920212223242526272829303132333435363738# _id为2 name为jack的文档我们创建成功了$ curl -XGET http://192.168.20.211:9200/customer/_doc/2?pretty&#123; "_index" : "customer", "_type" : "_doc", "_id" : "2", "_version" : 1, "_seq_no" : 12, "_primary_term" : 1, "found" : true, "_source" : &#123; "name" : "jack" &#125;&#125;# _id为3的文档我们进行了删除操作，所以found为false了$ curl -XGET http://192.168.20.211:9200/customer/_doc/3?pretty&#123; "_index" : "customer", "_type" : "_doc", "_id" : "3", "found" : false&#125;# _id为4的文档name值已经被我们更新成John Doe becomes Jane Doe$ curl -XGET http://192.168.20.211:9200/customer/_doc/4?pretty&#123; "_index" : "customer", "_type" : "_doc", "_id" : "4", "_version" : 2, "_seq_no" : 14, "_primary_term" : 1, "found" : true, "_source" : &#123; "name" : "John Doe becomes Jane Doe" &#125;&#125; 值得注意点是_Bulk API 不会因其中一个操作失败而失败。 如果单个操作因任何原因失败，它将继续处理其后的其余操作。 _bulk API返回时，它将为每个操作提供一个状态（按照发送的顺序），以便您可以检查特定操作是否失败。 列出索引中所有文档123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384# 通过索引名/_search 可以查看当前索引下所有类型的文档$ curl -XGET "http://192.168.20.211:9200/customer/_search?pretty"&#123; "took" : 0, "timed_out" : false, "_shards" : &#123; "total" : 1, "successful" : 1, "skipped" : 0, "failed" : 0 &#125;, "hits" : &#123; "total" : &#123; "value" : 7, "relation" : "eq" &#125;, "max_score" : 1.0, "hits" : [ &#123; "_index" : "customer", "_type" : "_doc", "_id" : "1", "_score" : 1.0, "_source" : &#123; "name" : "pyker zhang" &#125; &#125;, &#123; "_index" : "customer", "_type" : "_doc", "_id" : "en", "_score" : 1.0, "_source" : &#123; "language" : "english" &#125; &#125;, &#123; "_index" : "customer", "_type" : "_doc", "_id" : "cn", "_score" : 1.0, "_source" : &#123; "language" : "chinese" &#125; &#125;, &#123; "_index" : "customer", "_type" : "_doc", "_id" : "6", "_score" : 1.0, "_source" : &#123; "name" : "lily" &#125; &#125;, &#123; "_index" : "customer", "_type" : "_doc", "_id" : "5", "_score" : 1.0, "_source" : &#123; "name" : "lucy" &#125; &#125;, &#123; "_index" : "customer", "_type" : "_doc", "_id" : "2", "_score" : 1.0, "_source" : &#123; "name" : "jack" &#125; &#125;, &#123; "_index" : "customer", "_type" : "_doc", "_id" : "4", "_score" : 1.0, "_source" : &#123; "name" : "John Doe becomes Jane Doe" &#125; &#125; ] &#125;&#125; 以上就是ES的增删改查的基本操作，如果仔细学习了以上命令，应该会发现 elasticsearch 访问数据所使用的模式，概括如下：curl -X&lt;REST Verb&gt; &lt;Node&gt;:&lt;Port&gt;/&lt;Index&gt;/&lt;Type&gt;/&lt;ID&gt; Search API (查询API)运行搜索有两种基本方法:一种是通过REST请求URI发送搜索参数，另一种是通过REST请求体发送搜索参数。请求体方法允许您更富表现力，还可以以更可读的JSON格式定义搜索。我们将尝试请求URI方法的一个示例，但是在本教程的其余部分中，我们将只使用请求体方法。 为了方便操作，我们加载示例数据集,可以从这里下载示例数据集(accounts.json)。 将其解压缩到当前目录，并将其加载到集群中，如下所示:123456$ curl -H "Content-Type: application/json" -XPOST "http://192.168.20.211:9200/bank/_bulk?pretty&amp;refresh" --data-binary "@accounts.json"# 加载完后查看索引清楚的看到bank索引有1000条数据文档$ curl -XGET "http://192.168.20.211:9200/_cat/indices?v"health status index uuid pri rep docs.count docs.deleted store.size pri.store.sizegreen open bank J5R8uBBoQ4ie-x3eNf8LpQ 1 1 1000 0 828.7kb 414.3kb 通过REST请求URI发送搜索参数123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263$ curl -X GET "http://192.168.20.211:9200/bank/_search?q=*&amp;sort=account_number:asc&amp;pretty"&#123; "took" : 1, "timed_out" : false, "_shards" : &#123; "total" : 1, "successful" : 1, "skipped" : 0, "failed" : 0 &#125;, "hits" : &#123; "total" : &#123; "value" : 1000, "relation" : "eq" &#125;, "max_score" : null, "hits" : [ &#123; "_index" : "bank", "_type" : "_doc", "_id" : "0", "_score" : null, "_source" : &#123; "account_number" : 0, "balance" : 16623, "firstname" : "Bradshaw", "lastname" : "Mckenzie", "age" : 29, "gender" : "F", "address" : "244 Columbus Place", "employer" : "Euron", "email" : "bradshawmckenzie@euron.com", "city" : "Hobucken", "state" : "CO" &#125;, "sort" : [ 0 ] &#125;, &#123; "_index" : "bank", "_type" : "_doc", "_id" : "1", "_score" : null, "_source" : &#123; "account_number" : 1, "balance" : 39225, "firstname" : "Amber", "lastname" : "Duke", "age" : 32, "gender" : "M", "address" : "880 Holmes Lane", "employer" : "Pyrami", "email" : "amberduke@pyrami.com", "city" : "Brogan", "state" : "IL" &#125;, "sort" : [ 1 ] &#125;, ... ... 示例返回所有bank中的索引数据。其中 q=* 表示匹配索引中所有的数据, 按照account_number值进行升序排序。 注意：如果siez不指定，则默认搜索返回10条文档数据。 took: Elasticsearch执行搜索的时间（以毫秒为单位） time_out：告诉我们搜索是否超时 _shards：告诉我们搜索了多少个分片，以及搜索成功/失败分片的计数 hits：搜索结果 hits.total： 包含与搜索条件匹配的文档总数的信息的对象，默认值为10000，可以显示通过track_total_hits设置为true或整数 hits.hits：实际的搜索结果数组（默认为前10个文档） hits.sort：按哪个字段排序的结果（如果按分数排序则丢失） hits._score and max_score：暂时忽略这些字段 通过REST请求体发送搜索参数123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176$ curl -XGET "http://192.168.20.211:9200/bank/_search?pretty" -H 'Content-Type: application/json' -d '&#123;"query": &#123;"match_all": &#123;&#125;&#125;, "from": 370, "size": 7, "sort": [&#123;"account_number": &#123;"order": "asc"&#125;&#125;]&#125;'&#123; "took" : 0, "timed_out" : false, "_shards" : &#123; "total" : 1, "successful" : 1, "skipped" : 0, "failed" : 0 &#125;, "hits" : &#123; "total" : &#123; "value" : 1000, "relation" : "eq" &#125;, "max_score" : null, "hits" : [ &#123; "_index" : "bank", "_type" : "_doc", "_id" : "370", "_score" : null, "_source" : &#123; "account_number" : 370, "balance" : 28499, "firstname" : "Oneill", "lastname" : "Carney", "age" : 25, "gender" : "F", "address" : "773 Adelphi Street", "employer" : "Bedder", "email" : "oneillcarney@bedder.com", "city" : "Yorklyn", "state" : "FL" &#125;, "sort" : [ 370 ] &#125;, &#123; "_index" : "bank", "_type" : "_doc", "_id" : "371", "_score" : null, "_source" : &#123; "account_number" : 371, "balance" : 19751, "firstname" : "Barker", "lastname" : "Allen", "age" : 32, "gender" : "F", "address" : "295 Wallabout Street", "employer" : "Nexgene", "email" : "barkerallen@nexgene.com", "city" : "Nanafalia", "state" : "NE" &#125;, "sort" : [ 371 ] &#125;, &#123; "_index" : "bank", "_type" : "_doc", "_id" : "372", "_score" : null, "_source" : &#123; "account_number" : 372, "balance" : 28566, "firstname" : "Alba", "lastname" : "Forbes", "age" : 24, "gender" : "M", "address" : "814 Meserole Avenue", "employer" : "Isostream", "email" : "albaforbes@isostream.com", "city" : "Clarence", "state" : "OR" &#125;, "sort" : [ 372 ] &#125;, &#123; "_index" : "bank", "_type" : "_doc", "_id" : "373", "_score" : null, "_source" : &#123; "account_number" : 373, "balance" : 9671, "firstname" : "Simpson", "lastname" : "Carpenter", "age" : 21, "gender" : "M", "address" : "837 Horace Court", "employer" : "Snips", "email" : "simpsoncarpenter@snips.com", "city" : "Tolu", "state" : "MA" &#125;, "sort" : [ 373 ] &#125;, &#123; "_index" : "bank", "_type" : "_doc", "_id" : "374", "_score" : null, "_source" : &#123; "account_number" : 374, "balance" : 19521, "firstname" : "Blanchard", "lastname" : "Stein", "age" : 30, "gender" : "M", "address" : "313 Bartlett Street", "employer" : "Cujo", "email" : "blanchardstein@cujo.com", "city" : "Cascades", "state" : "OR" &#125;, "sort" : [ 374 ] &#125;, &#123; "_index" : "bank", "_type" : "_doc", "_id" : "375", "_score" : null, "_source" : &#123; "account_number" : 375, "balance" : 23860, "firstname" : "Phoebe", "lastname" : "Patton", "age" : 25, "gender" : "M", "address" : "564 Hale Avenue", "employer" : "Xoggle", "email" : "phoebepatton@xoggle.com", "city" : "Brule", "state" : "NM" &#125;, "sort" : [ 375 ] &#125;, &#123; "_index" : "bank", "_type" : "_doc", "_id" : "376", "_score" : null, "_source" : &#123; "account_number" : 376, "balance" : 44407, "firstname" : "Mcmillan", "lastname" : "Dunn", "age" : 21, "gender" : "F", "address" : "771 Dorchester Road", "employer" : "Eargo", "email" : "mcmillandunn@eargo.com", "city" : "Yogaville", "state" : "RI" &#125;, "sort" : [ 376 ] &#125; ] &#125;&#125; 命令中query是REST请求体的查询语句，match_all为匹配所有文档，from为从第370个文档开始， size为查询文档的个数，sort为按照account_number的值进行升序排序。 在例如以下请求将查询bank索引 state为NE的文档且只显示account_number, balance, state三个字段信息，显示结果以account_number字段进行升序排序。1234567891011121314$ curl -XGET "http://192.168.20.211:9200/bank/_search?pretty" -H 'Content-Type: application/json' -d '&#123; "query": &#123;"match": &#123; "state": "NE" &#125;&#125; , "_source": ["account_number", "balance","state"] , "sort": [ &#123; "account_number": &#123; "order": "asc" &#125; &#125; ]&#125;' 查询单个字段匹配 为了方便演示，下面的操作都是在kibana 开发工具上操作的 1234567# 查询匹配account_number值为200的文档GET /bank/_search&#123; "query": &#123;"match": &#123; "account_number": "200" &#125;&#125; &#125; 包含短句匹配1234567# 匹配address字段包含值为mill lane短句的文档GET /bank/_search&#123; "query": &#123;"match_phrase": &#123; "address": "mill lane" &#125;&#125;&#125; 与关系匹配123456789101112# 匹配address字段值中有mill也有lane的文档GET /bank/_search &#123; "query": &#123;"bool": &#123;"must": [ &#123;"match": &#123; "address": "mill" &#125;&#125;, &#123;"match": &#123; "address": "lane" &#125;&#125; ]&#125;&#125;&#125; 或关系匹配1234567891011121314# 匹配address字段值中有mill或者有lane的文档GET /bank/_search&#123; "query": &#123;"bool": &#123;"should": [ &#123;"match": &#123; "address": "mill" &#125;&#125;,&#123; "match": &#123; "address": "lane" &#125; &#125; ]&#125; &#125;&#125; 非关系匹配123456789101112# 匹配address字段值中不能有mill和lane的文档GET /bank/_search&#123; "query": &#123;"bool": &#123;"must_not": [ &#123;"match": &#123; "address": "mill" &#125;&#125;, &#123;"match": &#123; "address": "lane" &#125;&#125; ]&#125;&#125;&#125; 与和非匹配12345678910111213141516# 匹配age字段值为40，但是state不能是ID的文档GET /bank/_search&#123; "query": &#123;"bool": &#123;"must": [ &#123;"match": &#123; "age": "40" &#125;&#125; ], "must_not": [ &#123;"match": &#123; "state": "ID" &#125;&#125; ] &#125; &#125;&#125; 范围匹配1234567891011121314# 全文匹配balance字段值大于10000小于20000的文档GET /bank/_search&#123; "query": &#123;"bool": &#123;"must": [ &#123;"match_all": &#123;&#125;&#125; ], "filter": &#123;"range": &#123; "balance": &#123; "gte": 10000, "lte": 20000 &#125; &#125;&#125; &#125;&#125;&#125; 聚合匹配123456789101112# 统计state对应的值的合计数量，size=0表示不显示搜索匹配的结果只显示聚合结果。GET /bank/_search&#123; "size": 0, "aggs": &#123; "group_by_state": &#123; "terms": &#123; "field": "state.keyword" &#125; &#125; &#125;&#125; 嵌套聚合匹配12345678910111213141516171819# 统计state对应值的合计数量以及对应state文档中balance字段平均值GET /bank/_search&#123; "size": 0 , "aggs": &#123; "group_by_state": &#123; "terms": &#123; "field": "state.keyword" &#125; , "aggs": &#123; "average_balance": &#123; "avg": &#123; "field": "balance" &#125; &#125; &#125; &#125; &#125;&#125;]]></content>
      <categories>
        <category>elastic</category>
      </categories>
      <tags>
        <tag>elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo博客Next7.X版本主题配置]]></title>
    <url>%2F2019%2F05%2F01%2Fhexo-next.html</url>
    <content type="text"><![CDATA[前言 Hexo安装过后，默认的主题是landscape，如果不喜欢，我们也可以进行更换。本文主要讲解NexT主题的使用。 Hexo主题相关网址hexo官网nexT7.x 目录我们主要对next主题进行了如下配置操作，该配置后的next主题获取地址：https://github.com/ipyker/hexo-next-theme 设置中文语言设置博主文字描述设置博客文章连接为year/month/day/title.html格式Menu增加关于、标签、分类、互动、搜索菜单禁用关于、标签、分类菜单评论功能添加添加RSS设置背景图片。默认禁用，可以在themes/nexT/source/css/_custom/custon.styl文件中启用设置Canvas_nest动态背景图片快速加载设置微信支付宝打赏功能点击出现桃心效果主页文章添加阴影效果设置代码高亮顶栏背景色底栏背景色在右上角实现红色的fork me on github。默认禁用，可以在themes/nexT/_config.yml文件github_banner键中启用添加添加RSS修改文章内链接文本样式修改文章底部标签样式在文章末尾添加“文章结束”标记设置头像网站底部加上访问量网站底部字数统计网站底部添加网站运行时间网站底部添加动态桃心网站底部添加备案信息底部隐藏由Hexo强力驱动、主题–NexT.Mist设置网站的图标Favicon实现文章文字统计功能和阅读时长添加来必力云跟帖功能。（需要自己注册获取ID）去掉底部重复字数统计修改字体大小添加DaoVoice在线联系。（需要自己注册获取ID）侧边栏社交小图标设置添加侧栏推荐阅读修改侧边栏背景图片修改侧边栏文字颜色在文章底部增加版权信息Hexo博客添加站内搜索修改选中字符的颜色添加aplay音乐播放添加博客右下角卡通动漫]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elastic stack之EFK安装]]></title>
    <url>%2F2019%2F03%2F15%2Finstall-efk.html</url>
    <content type="text"><![CDATA[试验环境本次试验的elastic stack软件都为7.0.1版本，现在官网最新的为7.1版本，用户可在官方下载下载地址。 主机名 ip地址 服务 efk-node1 192.168.20.211 elasticsearch、kibana、jdk8 efk-node2 192.168.20.212 elasticsearch、cerebro、jdk8 real-server 192.168.20.250 filebeat 环境准备安装JDKelasticsearch需要jdk环境的支持，7.0.1版本默认已经自带JDK了，但是为了日后扩展问题，我们还是手动配置一边JDK环境。以下操作在efk-node1和efk-node2主机上进行：123456789101112131415161718# 下面下载链接因授权问题，需用户前往JDK官网下载$ wget https://download.oracle.com/otn/java/jdk/8u171-b12/478a62b7d4e34b78b671c754eaaf38ab/jdk-8u171-linux-x64.tar.gz$ tar zxvf jdk-8u171-linux-x64.tar.gz -C /usr/local/$ cat &gt;&gt; /etc/profile &lt;&lt; EOFexport JAVA_HOME=/usr/local/jdk1.8.0_171export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jarexport PATH=$JAVA_HOME/bin:$PATHEOF$ source /etc/profile# 验证JDK, JDK配置完成$ java -versionjava version "1.8.0_171"Java(TM) SE Runtime Environment (build 1.8.0_171-b11)Java HotSpot(TM) 64-Bit Server VM (build 25.171-b11, mixed mode) 设置服务器的最大文件数123456$ cat &gt;&gt; /etc/security/limits.conf &lt;&lt; EOF * soft nofile 655350 * hard nofile 655350 * soft nproc 655350 * hard nproc 655350EOF 设置服务器打开的最大进程数1234$ cat &gt; /etc/security/limits.d/20-nproc.conf &lt;&lt; EOF* soft nproc 4096root soft nproc unlimitedEOF 设置nmap数量对虚拟内存的支持12345$ cat &gt;&gt; /etc/sysctl.conf &lt;&lt; EOFvm.max_map_count=262144EOF$ sysctl -p 本地host解析12345cat &gt;&gt; /etc/hosts &lt;&lt; EOFefk-node1 192.168.20.211efk-node2 192.168.20.212real-server 192.168.20.250EOF 安装elasticsearch以下操作在efk-node1和efk-node2主机上进行： 下载和解压12345678# 创建elastic工作目录$ mkdir /elastic &amp;&amp; cd /elastic# 下载elasticsearch tar包$ wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.0.1-linux-x86_64.tar.gz# 解压到当前/elastic目录$ tar zxvf elasticsearch-7.0.1-linux-x86_64.tar.gz 修改配置文件下面只列出已配置的参数1234567891011$ cat elasticsearch-7.0.1/config/elasticsearch.yml | egrep -v "^$|^#" cluster.name: efk-cluster # 集群名称 node.name: efk-node1 # 节点名称，212服务器改成efk-node2 path.data: /var/lib/elasticsearch # elasticsearch数据目录 path.logs: /var/log/elasticsearch # elasticsearch日志目录 network.host: 0.0.0.0 # elasticsearch绑定的地址 http.port: 9200 # elasticsearch绑定的端口 discovery.seed_hosts: ["efk-node1", "efk-node2"] #集群发现 cluster.initial_master_nodes: ["efk-node1", "efk-node2"] #第一次访问初始的集群节点 xpack.security.enabled: true # ssl\tls安全参数 xpack.security.transport.ssl.enabled: true # ssl\tls安全参数 创建运行elasticsearch用户 elasticsearch默认是不运行root用户运行的，因此我们得创建一个新用户来运行elasticsearch。 12$ useradd elastic$ passwd elastic 创建依赖文件12345678910$ mkdir /var/log/elasticsearch # 创建日志目录$ mkdir /var/lib/elasticsearch # 创建数据目录$ touch /var/run/elasticsearch.pid # 创建进程文件$ chown -R elastic.elastic /elastic # 修改elastic工作目录所有者$ chown -R elastic.elastic /var/log/elasticsearch &amp;&amp; \chown -R elastic.elastic /var/lib/elasticsearch &amp;&amp; \chown -R elastic.elastic /var/run/elasticsearch.pid # 相关目录所有者都修改成运行elasticsearch服务的用户 准备elasticsearch systemctl文件12345678910111213141516171819[Unit]Description=ElasticsearchDocumentation=http://www.elastic.coWants=network-online.targetAfter=network-online.target[Service]Environment=JAVA_HOME=/usr/local/jdk1.8.0_171User=elasticGroup=elasticExecStart=/elastic/elasticsearch-7.0.1/bin/elasticsearch -p /var/run/elasticsearch.pidLimitNOFILE=65535LimitNPROC=65535LimitAS=infinityLimitFSIZE=infinity[Install]WantedBy=multi-user.target 启动elasticsearch12345$ systemctl daemon-reload # 加载systemctl配置文件$ systemctl start elasticsearch # 启动elasticsearch$ systemctl enable elasticsearch # 设置开机启动elasticsearch Tips: elasticsearch7.0.1安装完默认自带基础版的X-Pack功能，使用铂金版的需要购买或者参考破解X-pack进行破解。 验证集群状态通过上面的配置，我们的elasticsearch服务器已经启动，并且默认监听在9200和9300端口。9200端口: 为web访问提供服务；9300端口：为集群节点提供通信。12345678910# 验证集群节点数，其中master为*的代表该节点为主节点$ curl http://192.168.20.211:9200/_cat/nodes?vip heap.percent ram.percent cpu load_1m load_5m load_15m node.role master name192.168.20.212 10 39 0 0.00 0.01 0.05 mdi - efk-node2192.168.20.211 18 37 0 0.00 0.01 0.05 mdi * efk-node1# 验证集群健康状态，status为green表示正常$ curl http://192.168.20.211:9200/_cat/health?vepoch timestamp cluster status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent1558517253 09:27:33 efk-cluster green 2 2 6 3 0 0 0 0 - 100.0% 由上可知，我们的elasticsearch集群已经正常工作了，并且在2台集群节点下/var/lib/elasticsearch目录下都有相同的数据。更多关于elasticsearch REST API请参考官方说明。cluster apis _cat apis Search apis Document apis 安装 Kibana以下操作只在efk-node1主机上进行： 下载和解压12345678# 进入elastic工作目录$ cd /elastic# 下载kibana tar包$ wget https://artifacts.elastic.co/downloads/kibana/kibana-7.0.1-linux-x86_64.tar.gz# 解压到当前/elastic目录$ tar zxvf kibana-7.0.1-linux-x86_64.tar.gz 修改配置文件下面只列出已配置的参数123456789101112$ cat kibana-7.0.1-linux-x86_64/config/kibana.yml | egrep -v "^$|^#" server.port: 5601 # kibana服务端口 server.host: "0.0.0.0" # kibana服务地址 elasticsearch.hosts: ["http://192.168.20.201:9200"] # es访问地址，kibana需要和它通信 elasticsearch.username: "elastic" # es帐号，如果是X-pack铂金版可配 elasticsearch.password: "pyker123456" # es密码，如果是X-pack铂金版可配 elasticsearch.logQueries: true # 查询日志是否发送到ES，配合logging.json: true pid.file: /var/run/kibana.pid # kibana的进行id文件 logging.dest: /var/log/kibana.log # kibana日志文件 logging.json: true # 以json格式输出日志 logging.verbose: true # 记录所有事件，包括系统使用信息和所以请求 i18n.locale: "zh-CN" # 设置kibana为中文 创建依赖文件1234567$ touch /var/log/kibana.log # 创建日志文件$ touch /var/run/kibana.pid # 创建进程文件$ chown -R elastic.elastic /elastic &amp;&amp; \chown -R elastic.elastic /var/log/kibana.log &amp;&amp; \chown -R elastic.elastic /var/run/kibana.pid # 修改elastic工作目录及相关工作文件所有者 Tips：默认Kibana是支持root运行的，我这里为了统一elasticsearch运行环境所以打算elastic用户运行。 准备kibana systemctl文件1234567891011121314[Unit]Description=KibanaDocumentation=http://www.elastic.coWants=network-online.targetAfter=network-online.target[Service]User=elasticGroup=elasticExecStart=/elastic/kibana-7.0.1-linux-x86_64/bin/kibanaRestart=always[Install]WantedBy=multi-user.target 启动kibana12345$ systemctl daemon-reload # 加载systemctl配置文件$ systemctl start kibana # 启动kibana$ systemctl enable kibana # 设置开机启动kibana Kibana默认监控在5601端口上，此时可以通过http://192.168.20.211:5601访问kibana。 安装cerebro可视化集群管理工具cerebro是一个使用Scala，Play Framework，AngularJS和Bootstrap构建的开源（MIT许可）elasticsearch web可视化的监控工具，Github项目以下操作只在efk-node2主机上进行： 下载和解压请前往https://github.com/lmenezes/cerebro/releases 地址下载cerebro工具12345678910111213141516$ tar zxvf cerebro-0.8.3.tgz$ vim cerebro-0.8.3/conf/application.conf # 设置cerebro密码登陆认证auth=&#123; type: basic settings: &#123; username="admin" password="pyker123456" &#125;&#125;# 文件最后配置elasticsearch地址hosts = [ &#123; host = "http://192.168.20.211:9200" name = "efk-cluster" &#125;, 准备cerebro systemctl文件123456789101112131415[Unit]Description=CerebroAfter=network.target[Service]Environment=JAVA_HOME=/usr/local/jdk1.8.0_171User=elasticGroup=elasticLimitNOFILE=65535ExecStart=/elastic/cerebro-0.8.3/bin/cerebro -Dconfig.file=/elastic/cerebro-0.8.3/conf/application.conf -Dhttp.port=1234Restart=on-failureWorkingDirectory=/elastic/cerebro-0.8.3[Install]WantedBy=multi-user.target 命令启动cerebro12# 默认9000端口$ nohup ./bin/cerebro -Dhttp.port=1234 -Dhttp.address=192.168.20.212 &amp; systemctl方式启动cerebro12345$ systemctl daemon-reload # 加载systemctl配置文件$ systemctl start cerebro # 启动cerebro$ systemctl enable cerebro # 设置开机启动cerebro 访问cerebro通过浏览器访问http://192.168.20.212:1234就可以cerebro工具了。该工具详细的显示了es集群状态、节点数、索引数、分片数、文档数以及数据大小等参数。 安装filebeat此前文档我们已经说明了关于filebeat的原理，以及一些配置文件参数。现在我们只初略的说明我们已使用的配置参数。以下操作只在real-server主机上进行：（也就是我们业务所跑的服务器，我们要抓取的是业务日志，所以当然是安装在业务服务器上咯） 下载和解压12345678# 进入elastic工作目录$ mkdir /elastic &amp;&amp;&amp; cd /elastic# 下载kibana tar包$ wget https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-7.0.1-linux-x86_64.tar.gz# 解压到当前/elastic目录$ tar zxvf filebeat-7.0.1-linux-x86_64.tar.gz 本例我们使用filebeat监控tomcat日志和nginx日志 修改配置文件以配置文件方式支持tomcat日志输入下面只列出已配置的参数，关于参数说明，请参考此前文档123456789101112131415161718192021222324252627282930313233343536$ cat filebeat-7.0.1-linux-x86_64/filebeat.ymlfilebeat.inputs:- type: log enabled: true paths: - /usr/local/tomcat/logs/catalina.out # 监控tomcat控制台catalina.out日志文件 fields: log_topic: tomcat_access_logs exclude_lines: ['收到ping的消息'] multiline.pattern: '^[[:space:]]+|^Caused by:' multiline.negate: false multiline.match: after ignore_older: 0 close_inactive: 2m- type: log enabled: true paths: - /usr/local/tomcat/logs/localhost_access.* # 监控tomcat访问localhost_access日志文件 fields: log_topic: tomcat_catalina_logsfilebeat.config.modules: path: $&#123;path.config&#125;/modules.d/*.yml reload.enabled: truesetup.template.settings: index.number_of_shards: 1setup.kibana: host: "192.168.20.211:5601"output.elasticsearch: # 我们这里直接输出到ES，当然也可以输出到logstash、kafka等中间件 hosts: ["192.168.20.211:9200"] #protocol: "https" username: "elastic" password: "pyker123456"processors: - add_host_metadata: ~ - add_cloud_metadata: ~ catalina.out和localhost_access都需要使用一定的格式。方便filebeat处理多行事件日志。 以模版方式支持nginx日志输入默认filebeat自带诸多服务日志模版，如：nginx、redis、apache、IIS、kafka等等。默认都在filebeat解压后module、modules.d目录中。12345678$ cat /elastic/filebeat-7.0.1-linux-x86_64/modules.d/nginx.yml.disabled- module: nginx access: enabled: true var.paths: ["/data/wwwlogs/access.log*"] # 配置nginx实际的访问日志路径，多个文件逗号分开 error: enabled: true var.paths: ["/data/wwwlogs/error_nginx.log*"] # 配置nginx实际的错误日志路径 启动nginx模版123456789101112$ cd /elastic/filebeat-7.0.1-linux-x86_64$ ./filebeat modules enable nginx $ ./filebeat modules list # 列出已启动和未启动的模版Enabled:nginxsystemDisabledapacheauditd... 准备filebeat systemctl文件123456789101112[Unit]Description=Filebeat sends log files to Logstash or directly to Elasticsearch.Documentation=https://www.elastic.co/products/beats/filebeatWants=network-online.targetAfter=network-online.target[Service]ExecStart=/elastic/filebeat-7.0.1-linux-x86_64/filebeat -c /elastic/filebeat-7.0.1-linux-x86_64/filebeat.ymlRestart=always[Install]WantedBy=multi-user.target 启动filebeat12345$ systemctl daemon-reload # 加载systemctl配置文件$ systemctl start filebeat # 启动filebeat$ systemctl enable filebeat # 设置开机启动filebeat 至此简单的EFK集群搭建完成。在生成环境中 我们还会在filebeat和elasticsearch中间加入kafka和logstash来提高日志数据的高效传输和更强的日志过滤功能，而kafka又可以配置成集群模式。 在当前文档中我们并未说明kibana如何使用，请参考官方使用教程]]></content>
      <categories>
        <category>elastic</category>
      </categories>
      <tags>
        <tag>elasticsearch</tag>
        <tag>filebeat</tag>
        <tag>kibana</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elastic Stack之Filebeat7.0详解]]></title>
    <url>%2F2019%2F03%2F14%2Ffilebeat.html</url>
    <content type="text"><![CDATA[一：认识Filebeat Filebeat是beats其中一个组件，它是一个轻量型日志采集器，可以直接（或者通过Logstash）将数据发送到Elasticsearch，在那里你可以进一步处理和增强数据，然后在Kibana中将其可视化。 当您要面对成百上千、甚至成千上万的服务器、虚拟机和容器生成的日志时，请告别 SSH 吧。Filebeat 将为您提供一种轻量型方法，用于转发和汇总日志与文件，让简单的事情不再繁杂。 它性能稳健，不错过任何检测信号，无论在任何环境中，随时都潜伏着应用程序中断的风险。Filebeat 能够读取并转发日志行，如果出现中断，还会在一切恢复正常后，从中断前停止的位置继续开始。 Filebeat 内置有多种模块（auditd、Apache、NGINX、System、MySQL 等等），可针对常见格式的日志大大简化收集、解析和可视化过程，只需一条命令即可。之所以能实现这一点，是因为它将自动默认路径（因操作系统而异）与 Elasticsearch 采集节点管道的定义和 Kibana 仪表板组合在一起。不仅如此，数个 Filebeat 模块还包括预配置的 Machine Learning 任务。 二：Filebeat工作原理Filebeat由两个主要组件组成：inputs 和 harvesters（直译：收割机，采集器）。这些组件一起工作以跟踪文件，并将事件数据发送到你指定的输出。 2.1 harvester是什么一个harvester负责读取一个单个文件的内容。harvester逐行读取每个文件（一行一行地读取每个文件），并把这些内容发送到输出。每个文件启动一个harvester负责打开和关闭这个文件，这就意味着在harvester运行时文件描述符保持打开状态。而在harvester正在读取文件内容的时候，文件被删除或者重命名了，那么Filebeat会继续读这个文件。这就有一个问题了，就是只要负责这个文件的harvester没用关闭，那么磁盘空间就不会释放。默认情况下，Filebeat保存文件打开直到close_inactive到达。 2.2 input是什么一个input负责管理当前的harvesters，并找到所有要读取的源。如果input类型是log，则input查找paths已定义的glob路径匹配的所有日志文件，并为每个文件启动一个harvester。下面的例子配置Filebeat从所有匹配指定的glob模式的文件中读取行：12345filebeat.inputs:- type: log paths: - /var/log/*.log - /var/path2/*.log 2.3 Filebeat如何保持文件状态Filebeat保存每个文件的状态，并经常刷新状态到磁盘上的注册文件（registry）。状态用于记住harvester读取文件的最后一个偏移量（行），并确保所有日志行被发送到输出（如Elasticsearch 或者 Logstash等），当输出无法访问时，那么Filebeat会跟踪已经发送的最后一行，并只要输出再次变得可用时继续读取文件。当Filebeat运行时，会将每个文件的状态新保存在内存中。当Filebeat重新启动时，将使用注册文件中的数据重新构建状态，Filebeat将在最后一个已知行位置继续每个harvester。对于每个输入，Filebeat保存它找到的每个文件的状态。因为文件可以重命名或移动，所以文件名和路径不足以标识文件。对于每个文件，Filebeat存储惟一标识符，以检测文件是否以前读取过。如果你的情况涉及每天创建大量的新文件，你可能会发现注册表文件变得太大了。为了减小注册表文件的大小，有两个配置选项可用：clean_remove和clean_inactive。对于你不再访问且被忽略的旧文件，建议您使用clean_inactive。如果想从磁盘上删除旧文件，那么使用clean_remove选项。 2.4 Filebeat如何确保至少投递一次（at-least-once）Filebeat保证事件将被投递到配置的输出中至少一次，并且不会丢失数据。Filebeat能够实现这种行为，因为它将每个事件的投递状态存储在注册表文件中。在定义的输出被阻塞且没有确认所有事件的情况下，Filebeat将继续尝试发送事件，直到输出确认收到事件为止。如果Filebeat在发送事件的过程中关闭了，则在关闭之前它不会等待输出确认所有事件。当Filebeat重新启动时，发送到输出（但在Filebeat关闭前未确认）的任何事件将再次发送。这确保每个事件至少被发送一次，但是你最终可能会将重复的事件发送到输出。你可以通过设置shutdown_timeout选项，将Filebeat配置为在关闭之前等待特定的时间。 三：配置Filebeat为了配置Filebeat，你可以编辑配置文件filebeat.yml。此外同级目录下还有一个完整的配置文件示例filebeat.reference.yml 3.1 指定运行哪个模块Filebeat提供了几种启用模块的不同方式： 用modules.d目录下的配置启用模块 运行Filebea命令的时候启用模块 配置filebeat.yml文件启用模块配置 1234567891011# 用modules.d目录启用模块配置$ ./filebeat modules enable apache2 mysql# 运行Filebea命令的时候启用模块$ ./filebeat --modules nginx,mysql,system# 配置`filebeat.yml`文件启用模块配置filebeat.modules: - module: nginx - module: mysql - module: system 3.2 配置inputs为了手动配置Filebeat（代替用模块），你可以在filebeat.yml中的filebeat.inputs区域下指定一个inputs列表。列表是一个YMAL数组，并且你还可以指定多个inputs，相同input类型也可以指定多个。例如：1234567891011filebeat.inputs:- type: log paths: - /var/log/system.log - /var/log/wifi.log- type: log paths: - "/var/log/apache2/*" fields: apache: true fields_under_root: true 上面案例是从日志文件中读取行，类型为log的input需要指定一个paths列表，列表中的每一项必须能够定位并抓取到日志行。此外你还可以配置其它额外的配置项（比如，fields, include_lines, exclude_lines, multiline等等）来从这些文件中过滤读取行。你设置的这些配置对当前所有这种类型的input在获取日志行的时候都生效。为了对不同的文件应用不同的配置，你需要定义多个input区域。 3.3 其他配置项 paths：从预定义的目录级别下抓取所有文件。例如：/var/log//.log 将会抓取/var/log子目录目录下所有.log文件。它不会抓取/var/log本身目录下的日志文件。也不会抓取子目录的子目录下的日志文件。如果你应用recursive_glob: true设置的话，它将递归地抓取所有子目录下的所有.log文件。 recursive_glob.enabled允许将**扩展为递归glob模式。启用这个特性后，每个路径中最右边的**被扩展为固定数量的glob模式。例如：/foo/**扩展到/foo， /foo/*， /foo/**，等等。如果启用，它将单个**扩展为8级深度*模式。这个特性默认是启用的，设置recursive_glob.enabled: false可以禁用它。 encoding按照W3C推荐的HTML5读取的文件的编码。例如：plain, latin1, utf-8, utf-16be-bom, utf-16be, utf-16le, big5, gb18030, gbk, hz-gb-2312。plain编码是特殊的，因为它不校验或者转换任何输入。 exclude_lines一组正则表达式，用于匹配你想要排除的行。Filebeat会删除这组正则表达式匹配的行。默认情况下，没有行被删除。空行被忽略。如果指定了multiline，那么在用exclude_lines过滤之前会将每个多行消息合并成一个单行。（PS：也就是说，多行合并成单行后再支持排除行的过滤）例如配置Filebeat删除以DBG开头的行： 1234filebeat.inputs:- type: log ... exclude_lines: ['^DBG'] include_lines一组正则表达式，用于匹配你想要包含的行。Filebeat只会导出匹配这组正则表达式的行。默认情况下，所有行都被导出。空行被忽略。如果指定了multipline设置，每个多行消息先被合并成单行后再执行include_lines过滤。例如配置Filebeat导出以ERR或者WARN开头的行： 1234filebeat.inputs:- type: log ... include_lines: ['^ERR', '^WARN'] 如果include_lines 和 exclude_lines 都被定义了，那么Filebeat先执行include_lines后执行exclude_lines，而与这两个选项被定义的顺序没有关系。例如导出那些除了以DGB开头的所有包含sometext的行： 12345filebeat.inputs:- type: log ... include_lines: ['sometext'] exclude_lines: ['^DBG'] exclude_files需要排除的日志文件。 要匹配的正则表达式列表。 Filebeat从列表中删除与任何正则表达式匹配的文件。 默认情况下，不会删除任何文件。例如忽略.gz的文件: 1234filebeat.inputs:- type: log ... exclude_files: ['\.gz$'] fields可选的附加字段。 可以自由选择这些字段，以便将其他信息添加到已抓取到的日志文件中进行过滤。 fields_under_root设置为true可将其他字段存储为顶级字段，而不是“字段”子字典下。 如果名称与Filebeat本身添加的字段冲突，则自定义字段会覆盖默认字段。 harvester_buffer_size当抓取一个文件时每个harvester使用的buffer的字节数。默认是16384。 max_bytes单个日志消息允许的最大字节数。超过max_bytes的字节将被丢弃且不会发送到输出。对于多行日志消息来说这个设置是很有用的，因为它们往往很大。默认是10MB（10485760）。 json json.keys_under_root 默认情况下，解码后的JSON被放置在一个以”json”为key的输出文档中。如果你启用这个设置，那么这个key在文档中被复制为顶级。默认是false。 json.overwrite_keys 如果启用了keys_under_root和该设置，则解码的JSON对象中的值将覆盖Filebeat通常添加的字段（类型，来源，偏移等），以防发生冲突。 json.add_error_key 如果启用次设置，则当JSON解析出现错误的时候Filebeat添加 error.message和 error.type: json两个key，或者当没有使用message_key的时候。 json.message_key 一个可选的配置，用于在应用行过滤和多行设置的时候指定一个JSON key。指定的这个key必须在JSON对象中是顶级的，而且其关联的值必须是一个字符串，否则没有过滤或者多行聚集发送。 ignore_decoding_error 一个可选的配置，用于指定是否JSON解码错误应该被记录到日志中。如果设为true，错误将被记录。默认是false。 123json.keys_under_root: truejson.add_error_key: truejson.message_key: log multiline multiline.pattern 指定要匹配的正则表达式模式。根据您配置其他多行选项的方式，与指定正则表达式匹配的行被视为前一行的延续或新多行事件的开始。 您可以设置negate选项来否定模式。 multiline.negate 定义模式是否被否定。 默认值为false。 multiline.match 指定Filebeat如何将匹配行组合到事件中。 设置是在之前或之后。 这些设置的行为取决于您为negate指定的内容。 123multiline.pattern: '^\['multiline.negate: truemultiline.match: after multiline.negate multiline.match Result false after 与模式匹配的连续行和前面不匹配的行合并成一条完整日志 false before 与模式匹配的连续行和后面不匹配的行合并成一条完整日志 true after 与模式匹配的行和后面不匹配的行组成一条完整日志 true before 与模式匹配的行和前面不匹配的行组成一条完整日志 例如Java堆栈跟踪由多行组成，在初始行之后的每一行都以空格开头，例如下面这样：1234Exception in thread "main" java.lang.NullPointerException at com.example.myproject.Book.getTitle(Book.java:16) at com.example.myproject.Author.getBookTitles(Author.java:25) at com.example.myproject.Bootstrap.main(Bootstrap.java:14) 为了把这些行合并成单个事件，用写了多行配置：123multiline.pattern: '^[[:space:]]'multiline.negate: falsemultiline.match: after 下面是一个稍微更复杂的例子1234567Exception in thread "main" java.lang.IllegalStateException: A book has a null property at com.example.myproject.Author.getBookIds(Author.java:38) at com.example.myproject.Bootstrap.main(Bootstrap.java:14)Caused by: java.lang.NullPointerException at com.example.myproject.Book.getId(Book.java:22) at com.example.myproject.Author.getBookIds(Author.java:35) ... 1 more 为了合并这个，用下面的配置：123multiline.pattern: '^[[:space:]]+(at|\.&#123;3&#125;)\b|^Caused by:'multiline.negate: falsemultiline.match: after 在这个例子中，模式匹配下列行： 以空格开头，后面跟 at 或者 … 的行 以 Caused by: 开头的行 ignore_older如果启用，那么Filebeat会忽略在指定的时间跨度之前被修改的文件。如果你想要保留日志文件一个较长的时间，那么配置ignore_older是很有用的。例如，如果你想要开始Filebeat，但是你只想发送最近一周最新的文件，这个情况下你可以配置这个选项。你可以用时间字符串，比如2h（2小时），5m（5分钟）。默认是0，意思是禁用这个设置。你必须设置ignore_older比close_inactive更大。 close_*close_*配置项用于在一个确定的条件或者时间点之后关闭harvester。关闭harvester意味着关闭文件处理器。如果在harvester关闭以后文件被更新，那么在scan_frequency结束后改文件将再次被拾起。然而，当harvester关闭的时候如果文件被删除或者被移动，那么Filebeat将不会被再次拾起，并且这个harvester还没有读取的数据将会丢失。 close_inactive 当启用此选项时，如果文件在指定的持续时间内未被获取，则Filebeat将关闭文件句柄。当harvester读取最后一行日志时，指定周期的计数器就开始工作了。它不基于文件的修改时间。如果关闭的文件再次更改，则会启动一个新的harvester，并且在scan_frequency结束后，将获得最新的更改。 推荐给close_inactive设置一个比你的日志文件更新频率更大一点儿的值。例如，如果你的日志文件每隔几秒就会更新，你可以设置close_inactive为1m。如果日志文件的更新速率不固定，那么可以用多个配置。将close_inactive设置为更低的值意味着文件句柄可以更早关闭。然而，这样做的副作用是，如果harvester关闭了，新的日志行不会实时发送。 关闭文件的时间戳不依赖于文件的修改时间。代替的，Filebeat用一个内部时间戳来反映最后一次读取文件的时间。例如，如果close_inactive被设置为5分钟，那么在harvester读取文件的最后一行以后，这个5分钟的倒计时就开始了。你可以用时间字符串，比如2h（2小时），5m（5分钟）。默认是5m。 close_renamed 当启用此选项时，Filebeat会在重命名文件时关闭文件处理器。默认情况下，harvester保持打开状态并继续读取文件，因为文件处理器不依赖于文件名。如果启用了close_rename选项，并且重命名或者移动的文件不再匹配文件模式的话，那么文件将不会再次被选中。Filebeat将无法完成文件的读取。 close_removed 当启用此选项时，Filebeat会在删除文件时关闭harvester。通常，一个文件只有在它在由close_inactive指定的期间内不活跃的情况下才会被删除。但是，如果一个文件被提前删除，并且你不启用close_removed，则Filebeat将保持文件打开，以确保harvester已经完成。如果由于文件过早地从磁盘中删除而导致文件不能完全读取，请禁用此选项。 close_eof 抓取到达文件末尾后立即关闭文件处理程序。默认情况下，此选项被禁用。 注意：潜在的数据丢失。 请务必阅读并理解此选项的文档。 close_timeout 当启用此选项是，Filebeat会给每个harvester一个预定义的生命时间。无论读到文件的什么位置，只要close_timeout周期到了以后就会停止读取。当你想要在文件上只花费预定义的时间时，这个选项对旧的日志文件很有用。尽管在close_timeout时间以后文件就关闭了，但如果文件仍然在更新，则Filebeat将根据已定义的scan_frequency再次启动一个新的harvester。这个harvester的close_timeout将再次启动，为超时倒计时。 scan_frequencyFilebeat多久检查一次指定路径下的新文件（PS：检查的频率）。例如，如果你指定的路径是 /var/log/* ，那么会以指定的scan_frequency频率去扫描目录下的文件（PS：周期性扫描）。指定1秒钟扫描一次目录，默认值10秒。如果你需要近实时的发送日志行的话，不要设置scan_frequency为一个很低的值，而应该调整close_inactive以至于文件处理器保持打开状态，并不断地轮询你的文件。 更多关于类型为log的输入配置，请参考官方配置文档 四：加载外部配置文件Filebeat可以为输入和模块加载外部配置文件，允许您将配置分成多个较小的配置文件。 4.1 配置input对于输入配置，请在filebeat.yml文件的filebeat.config.inputs部分中指定path选项。 例如：123filebeat.config.inputs: enabled: true path: configs/*.yml 每一个在path下的文件都必须包含一个或多个input定义，例如：123456789- type: log paths: - /var/log/mysql.log scan_frequency: 10s- type: log paths: - /var/log/apache.log scan_frequency: 5s 4.2 模块配置对于模块配置，请在filebeat.yml文件的filebeat.config.modules部分中指定path选项。 默认情况下，Filebeat会加载modules.d目录中启用的模块配置。 例如：123filebeat.config.modules: enabled: true path: $&#123;path.config&#125;/modules.d/*.yml 每个被发现的配置文件必须包含一个或多个模块定义，例如：1234567- module: apache2 access: enabled: true var.paths: [/var/log/apache2/access.log*] error: enabled: true var.paths: [/var/log/apache2/error.log*] 4.3 配置output您可以通过在filebeat.yml配置文件的输出部分中设置选项来配置Filebeat以写入特定输出。 只能定义一个输出。支持的输出如：Elasticsearch, Logstash, Kafka, Redis, File, Console, Cloud。 4.3.1 配置Elasticsearch输出为输出指定Elasticsearch时，Filebeat会使用Elasticsearch HTTP API将事务直接发送到Elasticsearch。例如：123456output.elasticsearch: hosts: ["https://localhost:9200"] index: "filebeat-%&#123;[beat.version]&#125;-%&#123;+yyyy.MM.dd&#125;" ssl.certificate_authorities: ["/etc/pki/root/ca.pem"] ssl.certificate: "/etc/pki/client/cert.pem" ssl.key: "/etc/pki/client/cert.key" 为了启用SSL，只需要在hosts下的所有URL添加https即可1234output.elasticsearch: hosts: ["https://localhost:9200"] username: "filebeat_internal" password: "YOUR_PASSWORD" 如果Elasticsearch节点是用IP:PORT的形式定义的，那么添加protocol:https。12345output.elasticsearch: hosts: ["localhost"] protocol: "https" username: "&#123;beatname_lc&#125;_internal" password: "&#123;pwd&#125;" 4.3.2 Elasticsearch输出配置项enabled启用或禁用该输出。默认true。 hostsElasticsearch节点列表。事件以循环顺序发送到这些节点。如果一个节点变得不可访问，那么自动发送到下一个节点。每个节点可以是URL形式，也可以是IP:PORT形式。如果端口没有指定，用9200。1234output.elasticsearch: hosts: ["10.45.3.2:9220", "10.45.3.1:9230"] protocol: https path: /elasticsearch username用于认证的用户名 password用户认证的密码 protocol可选值是：http 或者 https。默认是http。 pathHTTP API调用前的HTTP路径前缀。这对于Elasticsearch监听HTTP反向代理的情况很有用。 headers将自定义HTTP头添加到Elasticsearch输出的每个请求。 index索引名字。（PS：意思是要发到哪个索引中去）。默认是&quot;filebeat-%{[beat.version]}-%{+yyyy.MM.dd}&quot;（例如，”filebeat-6.3.2-2017.04.26”）。如果你想改变这个设置，你需要配置 setup.template.name 和 setup.template.pattern 选项。如果你用内置的Kibana dashboards，你也需要设置setup.dashboards.index选项。 indices索引选择器规则数组，支持条件、基于格式字符串的字段访问和名称映射。如果索引缺失或没有匹配规则，将使用index字段。例如：12345678910output.elasticsearch: hosts: ["http://localhost:9200"] index: "logs-%&#123;[beat.version]&#125;-%&#123;+yyyy.MM.dd&#125;" indices: - index: "critical-%&#123;[beat.version]&#125;-%&#123;+yyyy.MM.dd&#125;" when.contains: message: "CRITICAL" - index: "error-%&#123;[beat.version]&#125;-%&#123;+yyyy.MM.dd&#125;" when.contains: message: "ERR" timeout请求超时时间。默认90秒。 更多关于elasticsearch输出配置请参考官方文档这里只以elasticsearch输出类型为例，更多关于output 输出类型配置，请参考官方配置输出类型 4.4 加载索引模板在filebeat.yml配置文件的setup.template区域指定索引模板，用来设置在Elasticsearch中的映射。如果模板加载是启用的（默认的），Filebeat在成功连接到Elasticsearch后自动加载索引模板。你可以调整下列设置或者覆盖一个已经存在的模板。 setup.template.enabled设为false表示禁用模板加载 setup.template.name模板的名字。默认是filebeat。Filebeat的版本总是跟在名字后面，所以最终的名字是 filebeat-%{[beat.version]} setup.template.pattern模板的模式。默认模式是filebeat-*。例如：12setup.template.name: "filebeat"setup.template.pattern: "filebeat-*" setup.template.fields描述字段的YAML文件路径。默认是 fields.yml。 setup.template.overwrite是否覆盖存在的模板。默认false。 setup.template.settings._source12345setup.template.name: "filebeat"setup.template.fields: "fields.yml"setup.template.overwrite: falsesetup.template.settings: _source.enabled: false 有关filebeat完整的配置文档，请参考(https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-getting-started.html)]]></content>
      <categories>
        <category>elastic</category>
      </categories>
      <tags>
        <tag>filebeat</tag>
        <tag>efk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elastic Stack之X-Pack7.0破解]]></title>
    <url>%2F2019%2F03%2F13%2Felastic-x-pack.html</url>
    <content type="text"><![CDATA[说明： elastic官方在elastic stack 6.4.2版本后就在elasticsearch中内置了X-Pack工具，因此下文破解X-Pack7.0.1的版本也是对应elastic stack7.0.1的版本。而X-Pack内置在elasticsearch包中，以下所有操作都是针对elasticsearch7.0.1包中进行的。 X-Pack是什么X-pack是elasticsearch的一个扩展包，将安全，警告，监视，图形和报告功能捆绑在一个易于安装的软件包中，虽然x-pack被设计为一个无缝的工作，但是你可以轻松的启用或者关闭一些功能。 目前6.2及以下版本只能使用免费版，然而免费版的功能相当少。X-pack 的破解基本思路是先安装正常版本，之后替换破解的jar包来实现，目前只能破解到白金版，但已经够用了。更多版本功能介绍请查看官方版本订阅文档 下载elasticsearch上面提到X-Pack自6.4.2版本后已经内置到elasticsearch中，因此我们需要下载elasticsearch7.0.1最新版。123456# 下载elasticsearch.tar.gz$ mkdir /elk &amp;&amp; cd /elk$ wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.0.1-linux-x86_64.tar.gz# 解压elasticsearch.tar.gz$ tar zxvf elasticsearch-7.0.1-linux-x86_64.tar.gz 下载完成并且解压后，我们可以查看自带x-pack的模版。12345678910111213141516# 查看x-pack相关的模块$ cd elasticsearch-7.0.1$ ls modules/ | grep x-packx-pack-ccrx-pack-corex-pack-deprecationx-pack-graphx-pack-ilmx-pack-logstashx-pack-mlx-pack-monitoringx-pack-rollupx-pack-securityx-pack-sqlx-pack-watcher 我们需要破解的x-pack-core.7.0.1.jar也就位于x-pack-core目录下12$ ls /elk/elasticsearch-7.0.1/modules/x-pack-core | grep x-packx-pack-core-7.0.1.jar 下载反编译工具Luyten破解X-Pack-core-7.0.1.jar需要反编译工具Luyten，我们可以前往下载地址下载Luyten工具。我们这里下载Luyten.exe windows版本，下载下来后打开，并将x-pack-core.7.0.1.jar文件拖进去，即可展开jar包的源代码了。 修改X-Pack源码文件在Luyten工具中我们需要把2个文件提取出来进行修改。org.elasticsearch.license.LicenseVerifier和org.elasticsearch.xpack.core.XPackBuild。 修改LicenseVerifier.javaLicenseVerifier中有两个静态方法，这就是验证授权文件是否有效的方法，我们把它修改为全部返回true.12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091/*如下代码为修改完后的代码,我们这里使用注释将不需要的代码注释掉*/package org.elasticsearch.license;import java.nio.*;import org.elasticsearch.common.bytes.*;import java.security.*;import java.util.*;import org.elasticsearch.common.xcontent.*;import org.apache.lucene.util.*;import org.elasticsearch.core.internal.io.*;import java.io.*;public class LicenseVerifier&#123; public static boolean verifyLicense(final License license, final byte[] publicKeyData) &#123; /* byte[] signedContent = null; byte[] publicKeyFingerprint = null; try &#123; final byte[] signatureBytes = Base64.getDecoder().decode(license.signature()); final ByteBuffer byteBuffer = ByteBuffer.wrap(signatureBytes); final int version = byteBuffer.getInt(); final int magicLen = byteBuffer.getInt(); final byte[] magic = new byte[magicLen]; byteBuffer.get(magic); final int hashLen = byteBuffer.getInt(); publicKeyFingerprint = new byte[hashLen]; byteBuffer.get(publicKeyFingerprint); final int signedContentLen = byteBuffer.getInt(); signedContent = new byte[signedContentLen]; byteBuffer.get(signedContent); final XContentBuilder contentBuilder = XContentFactory.contentBuilder(XContentType.JSON); license.toXContent(contentBuilder, (ToXContent.Params)new ToXContent.MapParams((Map)Collections.singletonMap("license_spec_view", "true"))); final Signature rsa = Signature.getInstance("SHA512withRSA"); rsa.initVerify(CryptUtils.readPublicKey(publicKeyData)); final BytesRefIterator iterator = BytesReference.bytes(contentBuilder).iterator(); BytesRef ref; while ((ref = iterator.next()) != null) &#123; rsa.update(ref.bytes, ref.offset, ref.length); &#125; return rsa.verify(signedContent); &#125; catch (IOException ex) &#123;&#125; catch (NoSuchAlgorithmException ex2) &#123;&#125; catch (SignatureException ex3) &#123;&#125; catch (InvalidKeyException e) &#123; throw new IllegalStateException(e); &#125; finally &#123; if (signedContent != null) &#123; Arrays.fill(signedContent, (byte)0); &#125; &#125;*/ return true; &#125; public static boolean verifyLicense(final License license) &#123; /* byte[] publicKeyBytes; try &#123; final InputStream is = LicenseVerifier.class.getResourceAsStream("/public.key"); try &#123; final ByteArrayOutputStream out = new ByteArrayOutputStream(); Streams.copy(is, (OutputStream)out); publicKeyBytes = out.toByteArray(); if (is != null) &#123; is.close(); &#125; &#125; catch (Throwable t) &#123; if (is != null) &#123; try &#123; is.close(); &#125; catch (Throwable t2) &#123; t.addSuppressed(t2); &#125; &#125; throw t; &#125; &#125; catch (IOException ex) &#123; throw new IllegalStateException(ex); &#125; //return verifyLicense(license, publicKeyBytes); */ return true; &#125;&#125; 修改XPackBuild.javaXPackBuild中最后一个静态代码块中 try的部分全部删除，这部分会验证jar包是否被修改.1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677/*如下代码为修改完后的代码,我们这里使用注释将不需要的代码注释掉*/package org.elasticsearch.xpack.core;import org.elasticsearch.common.io.*;import java.net.*;import org.elasticsearch.common.*;import java.nio.file.*;import java.io.*;import java.util.jar.*;public class XPackBuild&#123; public static final XPackBuild CURRENT; private String shortHash; private String date; @SuppressForbidden(reason = "looks up path of xpack.jar directly") static Path getElasticsearchCodebase() &#123; final URL url = XPackBuild.class.getProtectionDomain().getCodeSource().getLocation(); try &#123; return PathUtils.get(url.toURI()); &#125; catch (URISyntaxException bogus) &#123; throw new RuntimeException(bogus); &#125; &#125; XPackBuild(final String shortHash, final String date) &#123; this.shortHash = shortHash; this.date = date; &#125; public String shortHash() &#123; return this.shortHash; &#125; public String date() &#123; return this.date; &#125; static &#123; final Path path = getElasticsearchCodebase(); String shortHash = null; String date = null; Label_0109: &#123;/* if (path.toString().endsWith(".jar")) &#123; try &#123; final JarInputStream jar = new JarInputStream(Files.newInputStream(path, new OpenOption[0])); try &#123; final Manifest manifest = jar.getManifest(); shortHash = manifest.getMainAttributes().getValue("Change"); date = manifest.getMainAttributes().getValue("Build-Date"); jar.close(); &#125; catch (Throwable t) &#123; try &#123; jar.close(); &#125; catch (Throwable t2) &#123; t.addSuppressed(t2); &#125; throw t; &#125; break Label_0109; &#125; catch (IOException e) &#123; throw new RuntimeException(e); &#125; &#125;*/ shortHash = "Unknown"; date = "Unknown"; &#125; CURRENT = new XPackBuild(shortHash, date); &#125;&#125; 生成.class文件上述LicenseVerifier.java和XPackBuild.java两个文件在本地电脑windows修改完成后，我们需要将其复制到elasticsearch服务器上并编译成class文件，然后打包到x-pack-core-7.0.1.jar中。我们这里将这2个文件放到了/root目录下。12345678910# 编译LicenseVerifier.java$ javac -cp "/elk/elasticsearch-7.0.1/lib/elasticsearch-7.0.1.jar:/elk/elasticsearch-7.0.1/lib/lucene-core-8.0.0.jar:/elk/elasticsearch-7.0.1/modules/x-pack-core/x-pack-core-7.0.1.jar:/elk/elasticsearch-7.0.1/modules/x-pack-core/netty-common-4.1.32.Final.jar:/elk/elasticsearch-7.0.1/lib/elasticsearch-core-7.0.1.jar" /root/LicenseVerifier.java# 编译XPackBuild.java$ javac -cp "/elk/elasticsearch-7.0.1/lib/elasticsearch-7.0.1.jar:/elk/elasticsearch-7.0.1/lib/lucene-core-8.0.0.jar:/elk/elasticsearch-7.0.1/modules/x-pack-core/x-pack-core-7.0.1.jar:/elk/elasticsearch-7.0.1/modules/x-pack-core/netty-common-4.1.32.Final.jar:/elk/elasticsearch-7.0.1/lib/elasticsearch-core-7.0.1.jar" /root/XPackBuild.java# 查看编译后的文件$ ls /root | grep .classLicenseVerifier.classXPackBuild.class 替换LicenseVerifier.class和XPackBuild.class我们把/elk/elasticsearch-7.0.1/modules/x-pack-core目录下的x-pack-core-7.0.1.jar提取出来，放到一个临时的/elk/x-pack目录中。12345678$ cp /elk/elasticsearch-7.0.1/modules/x-pack-core/x-pack-core-7.0.1.jar /elk/x-pack$ cd /elk/x-pack# 解压x-pack-core-7.0.1.jar$ jar -xvf x-pack-core-7.0.1.jar# 替换.class文件$ cp /root/XPackBuild.class /elk/x-pack/org/elasticsearch/xpack/core/$ cp /root/LicenseVerifier.class /elk/x-pack/org/elasticsearch/license/ 打包新x-pack-core-7.0.1.jar文件123$ cd /elk/x-pack$ rm -rf x-pack-core-7.0.1.jar # 删除临时拷贝过来的源文件$ jar cvf x-pack-core-7.0.1.jar . 至此在/elk/x-pack目录下会新生成一个x-pack-core-7.0.1.jar文件。也就是破解后的文件。 替换x-pack-core-7.0.1.jar文件我们将新生成的x-pack-core-7.0.1.jar文件文件替换掉源x-pack-core-7.0.1.jar文件。12345678910111213141516171819cp /elk/x-pack/x-pack-core-7.0.1.jar /elk/elasticsearch-7.0.1/modules/x-pack-core/rm -rf /elk/x-pack # 完成文件替换后该目录既可以删除了``` ## 申请License完成以上步骤后，我们还需要去elastic官网申请一个license, [License申请地址](https://license.elastic.co/registration)，申请完成后，下载下来的License格式为json格式。并将该License的`type`、`expiry_date_in_millis`、`max_nodes`分别修改成`platinum`、`2524579200999`、`1000`。如下：```json&#123;"license": &#123; "uid":"537c5c48-c1dd-43ea-ab69-68d209d80c32", "type":"platinum", "issue_date_in_millis":1558051200000, "expiry_date_in_millis":2524579200999, "max_nodes":1000, "issued_to":"pyker", "issuer":"Web Form", "signature":"AAAAAwAAAA3fIq7NLN3Blk2olVjbAAABmC9ZN0hjZDBGYnVyRXpCOW5Bb3FjZDAxOWpSbTVoMVZwUzRxVk1PSmkxaktJRVl5MUYvUWh3bHZVUTllbXNPbzBUemtnbWpBbmlWRmRZb25KNFlBR2x0TXc2K2p1Y1VtMG1UQU9TRGZVSGRwaEJGUjE3bXd3LzRqZ05iLzRteWFNekdxRGpIYlFwYkJiNUs0U1hTVlJKNVlXekMrSlVUdFIvV0FNeWdOYnlESDc3MWhlY3hSQmdKSjJ2ZTcvYlBFOHhPQlV3ZHdDQ0tHcG5uOElCaDJ4K1hob29xSG85N0kvTWV3THhlQk9NL01VMFRjNDZpZEVXeUtUMXIyMlIveFpJUkk2WUdveEZaME9XWitGUi9WNTZVQW1FMG1DenhZU0ZmeXlZakVEMjZFT2NvOWxpZGlqVmlHNC8rWVVUYzMwRGVySHpIdURzKzFiRDl4TmM1TUp2VTBOUlJZUlAyV0ZVL2kvVk10L0NsbXNFYVZwT3NSU082dFNNa2prQ0ZsclZ4NTltbU1CVE5lR09Bck93V2J1Y3c9PQAAAQCjNd8mwy8B1sm9rGrgTmN2Gjm/lxqfnTEpTc+HOEmAgwQ7Q1Ye/FSGVNIU/enZ5cqSzWS2mY8oZ7FM/7UPKVQ4hkarWn2qye964MW+cux54h7dqxlSB19fG0ZJOJZxxwVxxi8iyJPUSQBa+QN8m7TFkK2kVmP+HnhU7mGUrqXt3zTk5d3pZw3QBQ/Rr3wmSYC5pxV6/o2UHFgu1OPDcX+kEb+UZtMrVNneR+cEwyx7o5Bg3rbKC014T+lMtt69Y080JDI5KfHa7e9Ul0c3rozIL975fP45dU175D4PKZy98cvHJgtsCJF3K8XUZKo2lOcbsWzhK2mZ5kFp0BMXF3Hs", "start_date_in_millis":1558051200000 &#125;&#125; 我们将过期时间写到2050年，type改为platinum 白金版，这样我们就会拥有全部的x-pack功能。 配置elasticsearch安全协议完成以上所有操作在启动elasticsearch前，我们需要配置elasticsearch的SSL/TLS安全协议,如果不配置的话，需要禁止security才能配置License。当License配置完成后我们需要再开启security，并开启SSL\TLS。12345678# 加载License到elasticsearch之前操作$ echo "xpack.security.enabled: false" &gt;&gt; /elk/elasticsearch-7.0.1/config/elasticsearch.yml$ ./bin/elasticsearch -d # 后台方式启动elasticsearch# 加载License到elasticsearch之后操作$ echo "xpack.security.transport.ssl.enabled: true" &gt;&gt; /elk/elasticsearch-7.0.1/config/elasticsearch.yml$ sed -i 's/xpack.security.enabled: false/xpack.security.enabled: true/g' /elk/elasticsearch-7.0.1/config/elasticsearch.yml$ kill -9 13023 &amp;&amp; ./bin/elasticsearch -d # 重启elasticsearch 加载License到elasticsearch123$ curl -XPUT -u elastic 'http://192.168.20.210:9200/_xpack/license' -H "Content-Type: application/json" -d @license.jsonEnter host password for user 'elastic': # 提示输入elastic用户密码，当前无密码，所以直接回车&#123;"acknowledged":true,"license_status":"valid"&#125; # license写入成功 查看License12345678910111213141516$ curl -XGET -u elastic:tWbWZc7NE3wYqS6DvSu4 http://192.168.20.210:9200/_license&#123; "license" : &#123; "status" : "active", "uid" : "537c5c48-c1dd-43ea-ab69-68d209d80c32", "type" : "platinum", "issue_date" : "2019-05-17T00:00:00.000Z", "issue_date_in_millis" : 1558051200000, "expiry_date" : "2049-12-31T16:00:00.999Z", "expiry_date_in_millis" : 2524579200999, "max_nodes" : 1000, "issued_to" : "pyker", "issuer" : "Web Form", "start_date_in_millis" : 1558051200000 &#125;&#125; 由结果可以看出x-pack到期时间为2049-12-31，破解完成。也可以在kibana web页面管理中查看破解详情。 设置密码现在我们可以使用x-pack铂金版的所有功能了，例如密码安全验证功能。1234567891011121314151617181920212223$ ./bin/elasticsearch-setup-passwords autoInitiating the setup of passwords for reserved users elastic,apm_system,kibana,logstash_system,beats_system,remote_monitoring_user.The passwords will be randomly generated and printed to the console.Please confirm that you would like to continue [y/N]yChanged password for user apm_systemPASSWORD apm_system = 24UtJKbNI1UqHUQkKPZYChanged password for user kibanaPASSWORD kibana = 8SSZMisIY0NZFMCS6wv9Changed password for user logstash_systemPASSWORD logstash_system = rFhWkYzayIUZVl8VIunJChanged password for user beats_systemPASSWORD beats_system = U1B4O5SKrSEatqDQRsQzChanged password for user remote_monitoring_userPASSWORD remote_monitoring_user = zdpj7HqO02yRXZR9Bwa2Changed password for user elasticPASSWORD elastic = tWbWZc7NE3wYqS6DvSu4]]></content>
      <categories>
        <category>elastic</category>
      </categories>
      <tags>
        <tag>x-pack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[下一代监控Prometheus初识]]></title>
    <url>%2F2019%2F01%2F12%2Fprometheus.html</url>
    <content type="text"><![CDATA[Prometheus介绍Prometheus 是一套开源的系统监控报警框架。它启发于 Google 的 borgmon 监控系统，由工作在 SoundCloud 的 google 前员工在 2012 年创建，作为社区开源项目进行开发，并于 2015 年正式发布。2016 年，Prometheus 正式加入 Cloud Native Computing Foundation，成为受欢迎度仅次于 Kubernetes 的项目。 什么是TSDBTSDB(Time Series Database)时序列数据库，我们可以简单的理解为一个优化后用来处理时间序列数据的软件，并且数据中的数组是由时间进行索引的。 Prometheus作为TSDB具有以下特点： 具有由指标名称和键/值对标识的时间序列数据的多维度数据模型。 PromQL灵活的查询语言。 不依赖分布式存储，单个服务器节点是自主的。 通过基于HTTP的pull方式采集时序数据。 可以通过中间网关（Pushgateway）进行时序列数据推送。 通过服务发现或者静态配置来发现目标服务对象。 支持多种多样的图表和界面展示，比如Grafana等。 Prometheus组成和架构Prometheus 生态圈中包含了多个组件，其中许多组件是可选的： Prometheus Server：它是Prometheus组件中的核心部分，负责实现对监控数据的获取，存储及查询。Prometheus Server可以通过静态配置管理监控目标，也可以配合使用Service Discovery的方式动态管理监控目标，并从这些监控目标中获取数据。其次Prometheus Sever需要对采集到的数据进行存储，Prometheus Server本身就是一个实时数据库，将采集到的监控数据按照时间序列的方式存储在本地磁盘当中。Prometheus Server对外提供了自定义的PromQL，实现对数据的查询以及分析。另外Prometheus Server的联邦集群能力可以使其从其他的Prometheus Server实例中获取数据。 Exporters：Exporter将监控数据采集的端点通过HTTP服务的形式暴露给Prometheus Server，将其转化为Prometheus支持的格式，Prometheus Server通过访问该Exporter提供的Endpoint端点，即可以获取到需要采集的监控数据。可以将Exporter分为2类： 直接采集：这一类Exporter直接内置了对Prometheus监控的支持，比如cAdvisor，Kubernetes，Etcd，Gokit等，都直接内置了用于向Prometheus暴露监控数据的端点。 间接采集：原有监控目标并不直接支持Prometheus，因此需要通过Prometheus提供的Client Library编写该监控目标的监控采集程序。如：Mysql Exporter，JMX Exporter，Consul Exporter等。 AlertManager：在Prometheus Server中支持基于Prom QL创建告警规则，如果满足Prom QL定义的规则，则会产生一条告警。当AlertManager从 Prometheus server 端接收到 alerts后，会进行去除重复数据，分组，并路由到对收的接受方式，发出报警。常见的接收方式有：电子邮件，pagerduty，webhook 等。 PushGateway: 主要用于短期的jobs。由于这类 jobs 存在时间较短，可能在 Prometheus 来 pull 之前就消失了。为此，这次 jobs 可以直接向 Prometheus中间网关推送它们的 metrics。这种方式主要用于服务层面的 metrics，对于机器层面的 metrices，需要使用 node exporter。（PushGatway类似zabbix proxy） Prometheus 架构图从上图可以看出，Prometheus 的主要模块包括：Prometheus server, exporters, Pushgateway, PromQL, Alertmanager 以及图形界面。Prometheus的基本原理是通过HTTP协议周期性抓取被监控组件的状态，任意组件只要提供对应的HTTP接口就可以接入监控。不需要任何SDK或者其他的集成过程。这样做非常适合做虚拟化环境监控系统，比如VM、Docker、Kubernetes等。输出被监控组件信息的HTTP接口被叫做exporter 。目前互联网公司常用的组件大部分都有exporter可以直接使用，比如Varnish、Haproxy、Nginx、MySQL、Linux系统信息(包括磁盘、内存、CPU、网络等等)。 Prometheus工作流程 Prometheus server 定期从配置好的 jobs 或者 exporters 中拉取 metrics，或者从Pushgateway 拉取metrics，或者从其他的 Prometheus server 中拉 metrics。 Prometheus server 在本地存储收集到的 metrics，并运行已定义好的 alert.rules，通过一定规则进行清理和整理数据，并把得到的结果存储到新的时间序列中。记录新的时间序列或者向 Alertmanager 推送警报。 Prometheus通过PromQL和其他API可视化地展示收集的数据。Prometheus支持很多方式的图表可视化，例如Grafana、自带的Promdash以及自身提供的模版引擎等等。Prometheus还提供HTTP API的查询方式，自定义所需要的输出。 Prometheus相关概念数据模型Prometheus 中存储的数据为时间序列，是由 metric 的名字和一系列的标签（键值对）唯一标识的，不同的标签则代表不同的时间序列。给定度量标准名称和一组标签，通常使用此表示法标识时间序列：1&lt;metric name&gt;&#123;&lt;label name&gt;=&lt;label value&gt;, ...&#125; 例如：度量名称的时间序列api_http_requests_total和标签method=”POST”，并handler=”/messages”可以这样写：1api_http_requests_total&#123;method="POST", handler="/messages"&#125; 四种Metric类型 Counter 一种累加的 metric，计数器可以用于记录只会增加不会减少的指标类型,比如记录：应用请求的总数，cpu使用时间，结束的任务数， 出现的错误数等。 对于Counter类型的指标，只包含一个inc()方法，用于计数器+1。 Gauge 一种常规的 metric，可以任意加减。 常用于反应应用的当前状态。典型的应用如：温度，运行的 goroutines 的个数。对于Gauge指标的对象则包含两个主要的方法inc()以及dec(),用户添加或者减少计数。 Histogram 自带buckets区间用于统计分布直方图，主要用于在指定分布范围内(Buckets)记录大小或者事件发生的次数。典型的应用如：请求持续时间，响应大小。 Summary Summary和Histogram非常类型相似，都可以统计事件发生的次数或者大小，以及其分布情况。Summary和Histogram都提供了对于事件的计数_count以及值的汇总_sum。 因此使用_count,和_sum时间序列可以计算出相同的内容，例如http每秒的平均响应时间：rate(basename_sum[5m]) / rate(basename_count[5m])。 同时Summary和Histogram都可以计算和统计样本的分布情况，比如中位数，9分位数等等。其中 0.0&lt;= 分位数Quantiles &lt;= 1.0。 不同在于Histogram可以通过histogram_quantile函数在服务器端计算分位数。 而Sumamry的分位数则是直接在客户端进行定义。因此对于分位数的计算。 Summary在通过PromQL进行查询时有更好的性能表现，而Histogram则会消耗更多的资源。 Jobs 和 Instances在Prometheus术语中，您可以scrape的目标称为instance，通常对应于单个进程。具有相同目的的instance集合，例如，为可伸缩性或可靠性而复制的流程称为Job。例如：12345job: api-server instance 1: 1.2.3.4:5670 instance 2: 1.2.3.4:5671 instance 3: 5.6.7.8:5670 instance 4: 5.6.7.8:5671 当Prometheus进行scrape目标时，它会自动将job标签和instance标签附加到scrape到的时间序列中，用于识别被抓取的目标，如果这些标签中的任何一个已存在于已删除数据中，则行为取决于honor_labels配置选项。 表达式语言类型Prometheus表达式或子表达式可以评估为一下四种类型之一： 即时向量（Instant vector） - 包含每个时间序列单个样品的一组时间序列，共享相同的时间戳 范围向量（Range vector） - 包含一个范围内数据点的一组时间序列 标量（Scalar） - 一个简单的数字浮点值 字符串（String） - 一个简单的字符串值；当前未使用 时间序列选择器即时向量选择即时向量选择器允许选择一组时间序列，或者某个给定的时间戳的样本数据。下面这个例子选择了具有http_requests_total的时间序列：1http_requests_total 你可以通过附加一组标签，并用{}括起来，来进一步筛选这些时间序列。下面这个例子只选择有http_requests_total名称的、有prometheus工作标签的、有canary组标签的时间序列：1http_requests_total&#123;job="prometheus",group="canary"&#125; 另外，也可以也可以将标签值反向匹配，或者对正则表达式匹配标签值。下面列举匹配操作符： =：选择正好相等的字符串标签 !=：选择不相等的字符串标签 =~：选择匹配正则表达式的标签（或子标签） !~：选择不匹配正则表达式的标签（或子标签） 例如，选择staging、testing、development环境下的，GET之外的HTTP方法的http_requests_total的时间序列： 1http_requests_total&#123;environment=~"staging|testing|development",method!="GET"&#125; 范围向量选择范围向量表达式正如即时向量表达式一样运行，但是前者返回从当前时刻开始的一定时间范围的时间序列集合回来。语法是，在一个向量表达式之后添加[]来表示时间范围，持续时间用数字表示，后接下面单元之一： s：seconds m：minutes h：hours d：days w：weeks y：years 在下面这个例子中，我们选择最后5分钟的记录，metric名称为http_requests_total、作业标签为prometheus的时间序列的所有值：1http_requests_total&#123;job="prometheus"&#125;[5m] 偏移量Offset所述offset可以改变时间为查询中的个别时刻和范围矢量偏移。例如，以下表达式返回http_requests_total相对于当前查询评估时间的过去5分钟值 ：1http_requests_total offset 5m 同样适用于范围向量。这将返回http_requests_total一周前的5分钟费率 ：1rate(http_requests_total[5m] offset 1w) 操作符关于操作符的说明，请参考官网操作符说明 函数关于函数的说明，请参考官网函数解释 Prometheus的安装下载prometheus二进制包1$ wget https://github.com/prometheus/prometheus/releases/download/v2.9.1/prometheus-2.9.1.linux-amd64.tar.gz 解压prometheus压缩包1$ tar zxvf prometheus-2.9.1.linux-amd64.tar.gz 将prometheus放到/usr/local目录下1$ mv prometheus-2.9.1.linux-amd64 /usr/local/prometheus 验证版本12345$ /usr/local/prometheus/prometheus --version prometheus, version 2.9.1 (branch: HEAD, revision: ad71f2785fc321092948e33706b04f3150eee44f) build user: root@09f919068df4 build date: 20190416-17:50:04 go version: go1.12.4 配置prometheus用户12$ groupadd prometheus$ useradd -g prometheus -s /sbin/nologin prometheus 创建prometheus数据目录1$ mkdir -p /var/lib/prometheus 创建prometheus的systemctl文件123456789101112131415$ cat &gt; /usr/lib/systemd/system/prometheus.service &lt;&lt; EOF[Unit]Description=PrometheusDocumentation=https://prometheus.io/After=network.target[Service]Type=simpleUser=prometheusExecStart=/usr/local/prometheus/prometheus --config.file=/usr/local/prometheus/prometheus.yml --storage.tsdb.path=/var/lib/prometheusRestart=on-failure[Install]WantedBy=multi-user.targetEOF prometheus权限配置12345678# prometheus主程序$ chown -R prometheus:prometheus /usr/local/prometheus/# prometheus数据目录$ chown -R prometheus:prometheus /var/lib/prometheus/# prometheus启动文件$ chown prometheus:prometheus /usr/lib/systemd/system/prometheus.service 启动服务并设为开机启动12$ systemctl start prometheus$ systemctl enable prometheus Prometheus.yml配置说明Prometheus通过命令行参数和配置文件进行配置。虽然命令行参数配置了不可变的系统参数（例如存储位置，保留在磁盘和内存中的数据量等），但配置文件定义了与抓取job及其instance相关的所有内容，以及哪些规则文件的加载。Prometheus可以在运行时重新加载其配置。如果新配置格式不正确，则不会应用更改。它有两种方式： 通过向prometheus进程发送SIGHUP信号 向Prometheus进程发送/-/reload的POST请求，curl -X POST http://localdns:9090/-/reload 配置文件要指定要加载的配置文件，请使用该--config.file标志。该文件以YAML格式编写，由下面描述的方案定义。括号表示参数是可选的。对于非列表参数，该值设置为指定的默认值。 以下为一个简单的prometheus.yml示例： 123456789101112131415161718192021222324252627282930# Prometheus全局配置项global: scrape_interval: 15s # 设定抓取数据的周期，默认为1min evaluation_interval: 15s # 设定更新rules文件的周期，默认为1min scrape_timeout: 15s # 设定抓取数据的超时时间，默认为10s external_labels: # 额外的属性，会添加到拉取得数据并存到数据库中 monitor: 'codelab_monitor'# Alertmanager配置alerting: alertmanagers: - static_configs: - targets: ["localhost:9093"] # 设定alertmanager和prometheus交互的接口，即alertmanager监听的ip地址和端口# rule配置，首次读取默认加载，之后根据evaluation_interval设定的周期加载rule_files: - "alertmanager_rules.yml" - "prometheus_rules.yml"# scrape配置scrape_configs:- job_name: 'prometheus' # job_name默认写入timeseries的labels中，可以用于查询使用 scrape_interval: 15s # 抓取周期，默认采用global配置 static_configs: # 静态配置 - targets: ['localdns:9090'] # prometheus所要抓取数据的地址，即instance实例项- job_name: 'example-random' static_configs: - targets: ['localhost:8080'] 更多关于prometheus配置文件参数以及自动发现规则，请参考官方prometheus配置]]></content>
      <categories>
        <category>prometheus</category>
      </categories>
      <tags>
        <tag>prometheus</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tomcat8配置APR模式]]></title>
    <url>%2F2018%2F12%2F14%2Ftomcat-apr.html</url>
    <content type="text"><![CDATA[tomcat三种模式Tomcat Connector运行有三种模式： bio默认的模式,同步阻塞，性能非常低下,没有经过任何优化处理和支持. nio同步非阻塞，利用java的异步io护理技术,noblocking IO技术,想运行在该模式下，直接修改server.xml里的Connector节点,修改protocol为protocol=&quot;org.apache.coyote.http11.Http11NioProtocol&quot;启动后,就可以生效。 apr安装起来最困难,但是从操作系统级别来解决异步的IO问题,大幅度的提高性能. Tomcat apr也是在Tomcat上运行高并发应用的首选模式。必须要安装apr、apr-util和native，nio修改模式，修改protocol为org.apache.coyote.http11.Http11AprProtocol，直接启动就支持apr。 安装配置APR模式本文所有步骤的前提是已经可以正常运行tomcat程序，JDK已安装的环境。 Tomcat配置apr模式依赖以下包,（ 版本根据自己需求选择 ）123apr-1.6.2apr-util-1.6.0openssl-1.0.2l 下载依赖包123$ wget http://mirror.bit.edu.cn/apache//apr/apr-1.6.2.tar.gz$ wget http://mirror.bit.edu.cn/apache//apr/apr-util-1.6.0.tar.gz$ wget https://www.openssl.org/source/openssl-1.0.2l.tar.gz 安装各个依赖包123456789101112#安装apr$ tar zxvf apr-1.6.2.tar.gz$ cd apr-1.6.2$ ./configure --prefix=/usr/local/apr &amp;&amp; make &amp;&amp; make install#安装apr-util$ tar zxvf apr-util-1.6.0.tar.gz$ cd apr-util-1.6.0$ ./configure --prefix=/usr/local/apr-util --with-apr=/usr/local/apr &amp;&amp; make &amp;&amp; make install#安装openss-1.0.2l$ tar zxvf openssl-1.0.2l.tar.gz$ cd openssl-1.0.2l$ ./config --prefix=/usr/local/openssl shared zlib &amp;&amp; make &amp;&amp; make install 1、安装apr-util前请确认是否安装了 expat-devel 包，如没安装请安装，不然会报错。yum install expat-devel 2、检查openssl是否安装成功 /usr/local/openssl/bin/openssl version -a 显示1.0.2l版本为成功 安装tomcat-native123$ tar zxvf /usr/local/tomcat8/bin/tomcat-native.tar.gz$ cd /usr/local/tomcat8/bin/tomcat-native-1.2.12-src/native$ ./configure --with-apr=/usr/local/apr --with-java-home=/usr/local/java8/ --with-ssl=/usr/local/openssl/ &amp;&amp; make &amp;&amp; make install 配置tomcat支持apr配置apr库文件123456#方式1：配置坏境变量：$ echo "export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/apr/lib" &gt;&gt; /etc/profile$ echo "export LD_RUN_PATH=$LD_RUN_PATH:/usr/local/apr/lib" &gt;&gt; /etc/profile &amp;&amp; source /etc/profile##方式2：catalina.sh脚本文件：在注释行# Register custom URL handlers下添加一行 $ JAVA_OPTS="$JAVA_OPTS -Djava.library.path=/usr/local/apr/lib" 修改tomcat server.xml文件&lt;Connector port=&quot;8080&quot; protocol=&quot;org.apache.coyote.http11.Http11AprProtocol&quot; connectionTimeout=&quot;20000&quot; redirectPort=&quot;8443&quot; /&gt; 启动Tomcat12$ cd /usr/local/tomcat8/bin$ ./startup.sh 查看Tomcat模式运行1$ cat /usr/local/tomcat8/logs/catalina.out 如果有显示[http-apr-8080] 说明配置APR模式成功。 java.net.ConnectException异常处理有时候在安装完tomcat后，停止tomcat会的会有如下异常，该异常可能和JDK有关系。 解决办法：进入JDK目录，编辑java.security文件,(注释掉原来的securerandom.source行，新增此行，保存即可 )12$ vi /usr/local/java8/jre/lib/security/java.securitysecurerandom.source=file:/dev/./urandom]]></content>
      <categories>
        <category>web</category>
      </categories>
      <tags>
        <tag>tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git 常 用 命 令]]></title>
    <url>%2F2018%2F11%2F30%2Fgit-command.html</url>
    <content type="text"><![CDATA[以下总结了日常常用的git指令方便读者学习和测试，如果想了解更多git指令相关的命令和参数，请参考Git指令指南1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889git init # 初始化本地git仓库（创建新仓库）git config --global user.name "xxx" # 配置用户名git config --global user.email "xxx@xxx.com" # 配置邮件git config --global color.ui true # git status等命令自动着色git config --global color.status autogit config --global color.diff autogit config --global color.branch autogit config --global color.interactive autogit clone git+ssh://git@192.168.53.168/VT.git # clone远程仓库git status # 查看当前版本状态（是否修改）git add xyz # 添加xyz文件至暂存区git add . # 增加当前子目录下所有更改过的文件至暂存区git commit -m 'xxx' # 提交git commit --amend -m 'xxx' # 合并上一次提交（用于反复修改）git commit -am 'xxx' # 将add和commit合为一步git rm xxx # 删除暂存区中的文件git rm -r * # 递归删除git log # 显示提交日志git log -1 # 显示最近1次commit的日志 -n为最近n次git log -5 --oneline # 极简显示最近5次的commit日志git log --stat # 显示提交日志及相关变动文件git log --graph --pretty=oneline --abbrev-commit # 以图形表示分支合并历史git show dfb02e6e4f2f7b573337763e5c0013802e392818 # 显示某个提交的详细内容git show dfb02 # 可只用commitid的前几位git show HEAD # 显示HEAD提交日志git show HEAD^ # 显示HEAD的父的提交日志git show HEAD~10 # 显示前面第10次的提交日志git show -s --pretty=raw 2be7fcb476git tag # 显示已存在的taggit tag -a v2.0 -m 'xxx' # 增加v2.0的taggit show v2.0 # 显示v2.0的日志及详细内容git log v2.0 # 显示v2.0的日志git diff # 显示所有未添加至暂存区的变更git diff --cached # 显示所有已添加index但还未commit的变更git diff HEAD^ # 比较与上一个版本的差异git diff HEAD -- ./lib # 比较与HEAD版本lib目录的差异git diff origin/master..master # 比较远程分支master与本地master分支的区别git diff origin/master..master --stat # 只显示差异的文件，不显示具体内容git remote add origin git+ssh://git@192.168.53.168/VT.git # 增加远程地址定义（用于push/pull/fetch）git branch # 显示本地分支git branch --contains 50089 # 显示包含提交50089的分支git branch -a # 显示所有分支git branch -r # 显示所有原创分支，包括本地和远程跟踪分支git branch --merged # 显示所有已合并到当前分支的分支git branch --no-merged # 显示所有未合并到当前分支的分支git branch -m master master_copy # 本地分支重命名git checkout -b master_copy # 从当前分支创建新分支master_copy并检出git checkout -b dev origin/dev # 拉取远程dev分支到本地dev分支git checkout features/performance # 检出已存在的features/performance分支git checkout --track hotfix/BJP93 # 检出远程分支hotfix/BJP93并创建本地跟踪分支git checkout v2.0 # 检出版本v2.0git checkout -b devel origin/develop # 从远程分支develop创建新本地分支devel并检出git checkout -- README # 将README文件回退到更新前(用于修改错误回退)git merge --no-ff origin/master # 合并远程master分支至当前分支git cherry-pick ff44785404a8e # 合并提交ff44785404a8e的修改git push origin master # 将当前master分支push到远程master分支git push origin :hotfixes/BJVEP933 # 删除远程仓库的hotfixes/BJVEP933分支git push --tags # 把所有tag推送到远程仓库git push --set-upstream origin dev # 推送本地当前分支到远程dev分支git fetch # 获取所有远程分支（不更新本地分支，需merge）git fetch --prune # 获取所有原创分支并清除服务器上已删掉的分支git fetch origin dev:tmp # 拉取远程dev分支到本地tmp分支git pull origin master # 获取远程分支master并merge到当前分支git mv README README2 # 重命名文件README为README2git reset --hard 2f3c # 将当前版本重置为2f3c（常用于merge回退）git rebasegit branch -d hotfix # 删除分支hotfix (该分支已合并到其他分支)git branch -D hotfix/BJP93 # 强制删除分支hotfix/BJP93git ls-files # 列出git 暂存区包含的文件git show-branch # 图示当前分支历史git show-branch --all # 图示所有分支历史git whatchanged # 显示提交历史对应的文件修改git revert dfb02e6e4 # 撤销提交dfb02e6e4git ls-tree HEAD # 内部命令：显示某个git对象git rev-parse v2.0 # 内部命令：显示某个ref对于的SHA1 HASHgit reflog # 显示历史所有提交，包括孤立节点git show HEAD@&#123;5&#125;git show master@&#123;yesterday&#125; # 显示master分支昨天的状态git log --pretty=format:'%h %s' --graph # 图示提交日志git stash # 暂存当前修改，将所有至为HEAD状态git stash list # 查看所有暂存git stash show -p stash@&#123;0&#125; # 参考第一次暂存git stash apply stash@&#123;0&#125; # 应用第一次暂存git stash drop # 删除stash记录git stash pop # 等价于 git stash apply和git stash dropgit grep "delete from" # 文件中搜索文本“delete from”git grep -e '#define' --and -e SORT_DIRENTgit gcgit fsck git fetch 和git pull 的差别 git fetch 相当于是从远程获取最新到本地，不会自动merge，如下指令： 1234git fetch # 将远程仓库的当前分支下载到本地当前branch中git diff origin/master # 比较本地的master分支和origin/master分支的差别git diff HEAD FETCH_HEAD # 和上条命令一样效果git merge origin/master # 进行合并 也可以用以下指令： 123git fetch origin master:tmp # 从远程仓库master分支获取最新，在本地建立tmp分支git diff tmp # 将当前分支和tmp進行對比git merge tmp # 合并tmp分支到当前分支 git pull：相当于是从远程获取最新版本并merge到本地 1git pull origin master git pull 相当于从远程获取最新版本并merge到本地 在实际使用中，git fetch更安全一些]]></content>
      <categories>
        <category>CI/CD</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Gitlab-CE社区版安装]]></title>
    <url>%2F2018%2F11%2F29%2Finstall-gitlab-ce.html</url>
    <content type="text"><![CDATA[常见的git环境有git web版和gitolite版本，gitolite权限控制强大，但功能不完善，而git web(gitlab)虽然权限没有像gitolite分的那么细，但是功能异常强大，不仅支持SSH KEY、web密码方式还有CI/CD，PIPELINE功能。所以下文将介绍如何在公司内网搭建私有的gitlab环境。 GitLab官网强烈建议安装Omnibus软件包，因为它安装更快，更易于升级，并且包含增强其他方法所没有的可靠性的功能。我们还强烈建议至少4GB的可用内存` 来运行GitLab。 GitLab的安装Omnibus方式安装安装并配置必要的依赖项在CentOS 7（和RedHat / Oracle / Scientific Linux 7）上，执行以下命令。1sudo yum install -y curl policycoreutils-python openssh-server 更新GitLab国内yum源1234567$ cat &gt; /etc/yum.repos.d/gitlab-ce.repo &lt;&lt; EOF[gitlab-ce]name=Gitlab CE Repositorybaseurl=https://mirrors.tuna.tsinghua.edu.cn/gitlab-ce/yum/el$releasever/gpgcheck=0enabled=1 EOF 安装GitLab-CE包12$ yum makecache$ yum install gitlab-ce -y 配置GitLab-CE域名和邮件通知配置文件为/etc/gitlab/gitlab.rb123456789101112131415#修改external_url 为你gitlab的访问域名$ vim /etc/gitlab/gitlab.rbexternal_url 'http://git.ipyker.com'#添加gitlab邮件通知配置（大约在/etc/gitlab/gitlab.rb文件的517行）gitlab_rails['smtp_enable'] = true gitlab_rails['smtp_address'] = "smtp.ipyker.com"gitlab_rails['smtp_port'] = 465gitlab_rails['smtp_user_name'] = "gitlab@ipyker.com"gitlab_rails['smtp_password'] = "mypasswd"gitlab_rails['smtp_authentication'] = "login"gitlab_rails['smtp_enable_starttls_auto'] = true gitlab_rails['smtp_tls'] = true gitlab_rails['gitlab_email_from'] = 'gitlab@ipyker.com'gitlab_rails['smtp_domain'] = "ipyker.com" GitLab安装完后，工作目录默认为：/var/opt/gitlab、/opt/gitlab，配置文件目录为：/etc/gitlab GitLab管理重载配置信息1$ gitlab-ctl reconfigure 启动1$ gitlab-ctl start 停止1$ gitlab-ctl stop 重启1$ gitlab-ctl restart 登陆第一次用域名登录gitlab，需要为root用户修改密码，root用户也是gitlab的超级管理员，gitlab也支持修改中文界面，如下图所示 更多关于gitlab配置信息参数，请参考官网GitLab配置文件 Docker方式安装GitLab docker 镜像gitlab-ce镜像存放在docker官方仓库中。 GitLab Docker镜像是GitLab的单个镜像，在单个容器上运行所有必要的服务。在以下示例中，我们使用GitLab CE的镜像。如果要使用最新的RC映像，请使用gitlab/gitlab-ce:rc 用于GitLab CEGitLab Docker镜像可以多种方式运行： 在Docker Engine中运行映像 将GitLab安装到群集中 使用docker-compose安装GitLab 本文档以第一种方式在docker中运行，如你需要高可用可以用第二种，甚至是通过kubenetes deplyment部署pod方式安装。 先决条件需要先安装docker，请参考docker-ce安装 运行gitlab-ce容器123456789$ docker run -d \ --hostname gitlab.example.com \ --publish 443:443 --publish 80:80 --publish 22:22 \ --name gitlab \ --restart always \ --volume /srv/gitlab/config:/etc/gitlab \ --volume /srv/gitlab/logs:/var/log/gitlab \ --volume /srv/gitlab/data:/var/opt/gitlab \ gitlab/gitlab-ce:latest docker run 命令将直接拉取镜像和运行容器，相当于运行docker pull和docker start两条命令。gitlab-ce容器将制定域名，开放的端口，以及容器挂载本地的文件系统，所有GitLab数据都将存储为子目录 /srv/gitlab/。restart系统重启后，容器将自动运行。 宿主目录 docker目录 用途 /srv/gitlab/data /var/opt/gitlab 用于存储应用数据 /srv/gitlab/logs /var/log/gitlab 用于存储日志 /srv/gitlab/config /etc/gitlab 用于存储GitLab配置文件 配置girlab-ce此容器使用官方的Omnibus GitLab软件包，因此所有配置都在docker唯一的配置文件中完成/etc/gitlab/gitlab.rb，也可以通过修改挂载卷文件来完成。 要访问GitLab的配置文件，您可以在正在运行的容器的上下文中启动shell会话。这将允许您浏览所有目录并使用您喜欢的文本编辑器：1$ docker exec -it gitlab /bin/bash 打开后，请/etc/gitlab/gitlab.rb 确保将指针设置external_url 为有效的URL。 此外您可以通过将环境变量添加GITLAB_OMNIBUS_CONFIG 到docker run命令来预配置GitLab Docker映像。此变量可以包含任何gitlab.rb设置，并在加载容器gitlab.rb文件之前进行评估。这样，您可以轻松配置GitLab的外部URL，从Omnibus GitLab模板进行任何数据库配置或任何其他选项 。注意：包含的设置GITLAB_OMNIBUS_CONFIG 不会写入gitlab.rb配置文件，而是在加载时进行评估。这是一个设置外部URL并在启动容器时启用LFS的示例：12345678910$ docker run -d \ --hostname gitlab.example.com \ --env GITLAB_OMNIBUS_CONFIG="external_url 'http://my.domain.com/'; gitlab_rails['lfs_enabled'] = true;" \ --publish 443:443 --publish 80:80 --publish 22:22 \ --name gitlab \ --restart always \ --volume /srv/gitlab/config:/etc/gitlab \ --volume /srv/gitlab/logs:/var/log/gitlab \ --volume /srv/gitlab/data:/var/opt/gitlab \ gitlab/gitlab-ce:latest 要从GitLab接收电子邮件，您必须配置 SMTP设置，因为GitLab Docker映像没有安装SMTP服务器。默认也为HTTP，还可以启动https访问。 重启gitlab-ce完成所需的所有更改后，需要重新启动容器才能重新配置GitLab：1$ docker restart gitlab 手动配置HTTPS默认情况下，omnibus-gitlab不使用HTTPS。如果要为gitlab.example.com启用HTTPS，请将以下语句添加到/etc/gitlab/gitlab.rb ：1external_url "https://gitlab.example.com" 因为在我们的例子中，主机名是“gitlab.example.com”，所以创建/etc/gitlab/ssl 目录并在那里复制密钥和证书。123$ sudo mkdir -p /etc/gitlab/ssl$ sudo chmod 700 /etc/gitlab/ssl$ sudo cp gitlab.example.com.key gitlab.example.com.crt /etc/gitlab/ssl/ 重新加载配置，当重新配置完成时，您的GitLab实例应该可以访问https://gitlab.example.com。1$ gitlab-ctl reconfigure 如果certificate.key文件有设置密码，Nginx在sudo gitlab-ctl reconfigure 执行时不会要求输入密码。在这种情况下，Omnibus GitLab将无声地失败，没有错误消息。要从密钥中删除密码，请使用以下命令： openssl rsa -in certificate_before.key -out certificate_after.key 。]]></content>
      <categories>
        <category>CI/CD</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql状态信息查询]]></title>
    <url>%2F2018%2F10%2F22%2Fmysql-status.html</url>
    <content type="text"><![CDATA[Mysql在安装运行后，其性能并不一定为实际需要的性能，因此我们常常会在mysql运行一段时间后进行mysql的性能状态查询来了解mysql服务器的性能指标，从而来优化mysql。 查看mysql性能状态命令查看mysql全局状态12mysql&gt; show global status;可以列出MySQL服务器运行各种状态值，另外，查询MySQL服务器配置信息语句。 12mysql&gt; show variables;可以查看Mysql系统变量及其值。 12mysql&gt; show processlist;可以查看Mysql当前连接数的进程以及状态。 慢查询123456789101112131415mysql&gt; show variables like '%slow%'; +------------------+-------+ | Variable_name | Value | +------------------+-------+ | log_slow_queries | ON | | slow_launch_time | 2 | +------------------+-------+ mysql&gt; show global status like '%slow%'; +---------------------+-------+ | Variable_name | Value | +---------------------+-------+ | Slow_launch_threads | 0 | | Slow_queries | 4148 | +---------------------+-------+ 配置中打开了记录慢查询，执行时间超过2秒的即为慢查询，系统显示有4148个慢查询，你可以分析慢查询日志，找出有问题的SQL语句，慢查询时间不宜设置过长，否则意义不大，最好在5秒以内，如果你需要微秒级别的慢查询，可以考虑给MySQL打补丁 ，记得找对应的版本。 打开慢查询日志可能会对系统性能有一点点影响，如果你的MySQL是主-从结构，可以考虑打开其中一台从服务器的慢查询日志，这样既可以监控慢查询，对系统性能影响又小。 连接数经常会遇见”MySQL: ERROR 1040: Too many connections”的情况，一种是访问量确实很高，MySQL服务器抗不住，这个时候就要考虑增加从服务器分散读压力，另外一种情况是MySQL配置文件中max_connections值过小：123456mysql&gt; show variables like 'max_connections'; +-----------------+-------+ | Variable_name | Value | +-----------------+-------+ | max_connections | 256 | +-----------------+-------+ 这台MySQL服务器最大连接数是256，然后查询一下服务器响应的最大连接数：1mysql&gt; show global status like ‘Max_used_connections’; MySQL服务器过去的最大连接数是245，没有达到服务器连接数上限256，应该没有出现1040错误，比较理想的设置是：Max_used_connections / max_connections * 100% ≈ 85% 最大连接数占上限连接数的85%左右，如果发现比例在10%以下，MySQL服务器连接数上限设置的过高了。 Key_buffer_sizekey_buffer_size是对MyISAM表性能影响最大的一个参数，下面一台以MyISAM为主要存储引擎服务器的配置：123456mysql&gt; show variables like ‘key_buffer_size’; +-----------------+------------+ | Variable_name | Value | +-----------------+------------+ | key_buffer_size | 536870912 | +-----------------+------------+ 分配了512MB内存给key_buffer_size，我们再看一下key_buffer_size的使用情况：1234567mysql&gt; show global status like 'key_read%'; +------------------------+-------------+ | Variable_name | Value | mysql +------------------------+-------------+ | Key_read_requests | 27813678764 | | Key_reads | 6798830 | +------------------------+-------------+ 一共有27813678764个索引读取请求，有6798830个请求在内存中没有找到直接从硬盘读取索引，计算索引未命中缓存的概率：key_cache_miss_rate = Key_reads / Key_read_requests * 100%比如上面的数据，key_cache_miss_rate为0.0244%，4000个索引读取请求才有一个直接读硬盘，已经很BT了,key_cache_miss_rate在0.1%以下都很好(每1000个请求有一个直接读硬盘)，如果key_cache_miss_rate在0.01%以下的话，key_buffer_size分配的过多，可以适当减少。MySQL服务器还提供了key_blocks_*参数：1234567mysql&gt; show global status like 'key_blocks_u%'; +------------------------+-------------+ | Variable_name | Value | +------------------------+-------------+ | Key_blocks_unused | 0 | | Key_blocks_used | 413543 | +------------------------+-------------+ Key_blocks_unused表示未使用的缓存簇(blocks)数，Key_blocks_used表示曾经用到的最大的blocks数，比如这台服务器，所有的缓存都用到了，要么增加key_buffer_size，要么就是过渡索引了，把缓存占满了。比较理想的设置：Key_blocks_used / (Key_blocks_unused + Key_blocks_used) * 100% ≈ 80% 临时表12345678mysql&gt; show global status like 'created_tmp%'; +-------------------------+---------+ | Variable_name | Value | +-------------------------+---------+ | Created_tmp_disk_tables | 21197 | | Created_tmp_files | 58 | | Created_tmp_tables | 1771587 | +-------------------------+---------+ 每次创建临时表，Created_tmp_tables增加，如果是在磁盘上创建临时表，Created_tmp_disk_tables也增加,Created_tmp_files表示MySQL服务创建的临时文件文件数，比较理想的配置是：Created_tmp_disk_tables / Created_tmp_tables * 100% &lt;= 25%比如上面的服务器Created_tmp_disk_tables / Created_tmp_tables * 100% = 1.20%, 应该相当好了。我们再看一下MySQL服务器对临时表的配置：1234567mysql&gt; show variables where Variable_name in ('tmp_table_size', 'max_heap_table_size'); +---------------------+-----------+ | Variable_name | Value | +---------------------+-----------+ | max_heap_table_size | 268435456 | | tmp_table_size | 536870912 | +---------------------+-----------+ 只有256MB以下的临时表才能全部放内存，超过的就会用到硬盘临时表。 Open Table1234567mysql&gt; show global status like 'open%tables%'; +---------------+-------+ | Variable_name | Value | +---------------+-------+ | Open_tables | 919 | | Opened_tables | 1951 | +---------------+-------+ Open_tables表示打开表的数量，Opened_tables表示打开过的表数量，如果Opened_tables数量过大，说明配置中table_cache(5.1.3之后这个值叫做table_open_cache)值可能太小，我们查询一下服务器table_cache值：123456mysql&gt; show variables like 'table_cache'; +---------------+-------+ | Variable_name | Value | +---------------+-------+ | table_cache | 2048 | +---------------+-------+ 比较合适的值为：Open_tables / Opened_tables * 100% &gt;= 85%, 及Open_tables / table_cache * 100% &lt;= 95%` 六、进程使用情况123456789mysql&gt; show global status like ‘Thread%’; +-------------------+-------+ | Variable_name | Value | +-------------------+-------+ | Threads_cached | 46 | | Threads_connected | 2 | | Threads_created | 570 | | Threads_running | 1 | +-------------------+-------+ 如果我们在MySQL服务器配置文件中设置了thread_cache_size， 当客户端断开之后，服务器处理此客户的线程将会缓存起来以响应下一个客户而不是销毁(前提是缓存数未达上限)。Threads_created表示创建过的线程数，如果发现Threads_created值过大的话，表明MySQL服务器一直在创建线程，这也是比较耗资源，可以适当增加配置文件中thread_cache_size值，查询服务器thread_cache_size配置：123456mysql&gt; show variables like 'thread_cache_size'; +-------------------+-------+ | Variable_name | Value | +-------------------+-------+ | thread_cache_size | 64 | +-------------------+-------+ 示例中的服务器还是挺健康的。 查询缓存(query cache)12345678910111213mysql&gt; show global status like 'qcache%'; +-------------------------+-----------+ | Variable_name | Value | +-------------------------+-----------+ | Qcache_free_blocks | 22756 | | Qcache_free_memory | 76764704 | | Qcache_hits | 213028692 | | Qcache_inserts | 208894227 | | Qcache_lowmem_prunes | 4010916 | | Qcache_not_cached | 13385031 | | Qcache_queries_in_cache | 43560 | | Qcache_total_blocks | 111212 | +-------------------------+-----------+ MySQL查询缓存变量解释：Qcache_free_blocks： 缓存中相邻内存块的个数。数目大说明可能有碎片。FLUSH QUERY CACHE会对缓存中的碎片进行整理，从而得到一个空闲块。Qcache_free_memory： 缓存中的空闲内存。Qcache_hits： 每次查询在缓存中命中时就增大Qcache_inserts： 每次插入一个查询时就增大。命中次数除以插入次数就是不中比率。Qcache_lowmem_prunes： 缓存出现内存不足并且必须要进行清理以便为更多查询提供空间的次数。这个数字最好长时间来看;如果这个数字在不断增长，就表示可能碎片非常严重，或者内存很少。(上面的 free_blocks和free_memory可以告诉您属于哪种情况)Qcache_not_cached： 不适合进行缓存的查询的数量，通常是由于这些查询不是 SELECT 语句或者用了now()之类的函数。Qcache_queries_in_cache： 当前缓存的查询(和响应)的数量。Qcache_total_blocks： 缓存中块的数量。 我们再查询一下服务器关于query_cache的配置：12345678910mysql&gt; show variables like 'query_cache%'; +------------------------------+-----------+ | Variable_name | Value | +------------------------------+-----------+ | query_cache_limit | 2097152 | | query_cache_min_res_unit | 4096 | | query_cache_size | 203423744 | | query_cache_type | ON | | query_cache_wlock_invalidate | OFF |+——————————+———–+ 各字段的解释：query_cache_limit： 超过此大小的查询将不缓存query_cache_min_res_unit： 缓存块的最小大小query_cache_size： 查询缓存大小query_cache_type： 缓存类型，决定缓存什么样的查询，示例中表示不缓存 select sql_no_cache 查询query_cache_wlock_invalidate： 当有其他客户端正在对MyISAM表进行写操作时，如果查询在query cache中，是否返回cache结果还是等写操作完成再读表获取结果。query_cache_min_res_unit 的配置是一柄”双刃剑”，默认是4KB，设置值大对大数据查询有好处，但如果你的查询都是小数据查询，就容易造成内存碎片和浪费。 查询缓存碎片率 = Qcache_free_blocks / Qcache_total_blocks * 100%如果查询缓存碎片率超过20%，可以用FLUSH QUERY CACHE整理缓存碎片，或者试试减小query_cache_min_res_unit，如果你的查询都是小数据量的话。查询缓存利用率 = (query_cache_size - Qcache_free_memory) / query_cache_size * 100%查询缓存利用率在25%以下的话说明query_cache_size设置的过大，可适当减小;查询缓存利用率在80%以上而且Qcache_lowmem_prunes &gt; 50的话说明query_cache_size可能有点小，要不就是碎片太多。查询缓存命中率 = (Qcache_hits - Qcache_inserts) / Qcache_hits * 100%示例服务器 查询缓存碎片率 = 20.46%，查询缓存利用率 = 62.26%，查询缓存命中率 = 1.94%，命中率很差，可能写操作比较频繁吧，而且可能有些碎片。 排序使用情况123456789mysql&gt; show global status like 'sort%'; +-------------------+------------+ | Variable_name | Value | +-------------------+------------+ | Sort_merge_passes | 29 | | Sort_range | 37432840 | | Sort_rows | 9178691532 | | Sort_scan | 1860569 | +-------------------+------------+ Sort_merge_passes 包括两步。MySQL 首先会尝试在内存中做排序，使用的内存大小由系统变量 Sort_buffer_size 决定，如果它的大小不够把所有的记录都读到内存中，MySQL 就会把每次在内存中排序的结果存到临时文件中，等 MySQL 找到所有记录之后，再把临时文件中的记录做一次排序。这再次排序就会增加 Sort_merge_passes。实际上，MySQL 会用另一个临时文件来存再次排序的结果，所以通常会看到 Sort_merge_passes 增加的数值是建临时文件数的两倍。因为用到了临时文件，所以速度可能会比较慢，增加 Sort_buffer_size 会减少 Sort_merge_passes 和 创建临时文件的次数。但盲目的增加 Sort_buffer_size 并不一定能提高速度。另外，增加read_rnd_buffer_size(3.2.3是record_rnd_buffer_size)的值对排序的操作也有一点的好处，参见：http://www.mysqlperformanceblog.com/2007/07/24/what-exactly-is-read_rnd_buffer_size/ 文件打开数(open_files)123456789101112mysql&gt; show global status like 'open_files'; +---------------+-------+ | Variable_name | Value | +---------------+-------+ | Open_files | 1410 | +---------------+-------+ mysql&gt; show variables like 'open_files_limit'; +------------------+-------+ | Variable_name | Value | +------------------+-------+ | open_files_limit | 4590 | +------------------+-------+ 比较合适的设置：Open_files / open_files_limit * 100% &lt;= 75% 表锁情况1234567mysql&gt; show global status like 'table_locks%'; +-----------------------+-----------+ | Variable_name | Value | +-----------------------+-----------+ | Table_locks_immediate | 490206328 | | Table_locks_waited | 2084912 | +-----------------------+-----------+ Table_locks_immediate 表示立即释放表锁数，Table_locks_waited 表示需要等待的表锁数，如果Table_locks_immediate / Table_locks_waited &gt; 5000，最好采用InnoDB引擎，因为InnoDB是行锁而MyISAM是表锁，对于高并发写入的应用InnoDB效果会好些。示例中的服务器Table_locks_immediate / Table_locks_waited = 235，MyISAM就足够了。 表扫描情况123456789101112131415161718mysql&gt; show global status like 'handler_read%'; +-----------------------+-------------+ | Variable_name | Value | +-----------------------+-------------+ | Handler_read_first | 5803750 | | Handler_read_key | 6049319850 | | Handler_read_next | 94440908210 | | Handler_read_prev | 34822001724 | | Handler_read_rnd | 405482605 | | Handler_read_rnd_next | 18912877839 | +-----------------------+-------------+ mysql&gt; show global status like 'com_select'; +---------------+-----------+ | Variable_name | Value | +---------------+-----------+ | Com_select | 222693559 | +---------------+-----------+ 计算表扫描率：表扫描率 = Handler_read_rnd_next / Com_select如果表扫描率超过4000，说明进行了太多表扫描，很有可能索引没有建好，增加read_buffer_size值会有一些好处，但最好不要超过8MB。 查看Mysql状态QPS/TPS/缓存命中率QPS(每秒Query量)12QPS = Questions(or Queries) / seconds mysql &gt; show global status like 'Question%'; TPS(每秒事务量)123TPS = (Com_commit + Com_rollback) / seconds mysql &gt; show global status like 'Com_commit'; mysql &gt; show global status like 'Com_rollback'; key Buffer 命中率12345mysql&gt;show global status like 'key%'; key_buffer_read_hits = (1-key_reads / key_read_requests) * 100% key_buffer_write_hits = (1-key_writes / key_write_requests) * 100% ``` #### InnoDB Buffer命中率 mysql&gt; show status like ‘innodb_buffer_pool_read%’;innodb_buffer_read_hits = (1 - innodb_buffer_pool_reads / innodb_buffer_pool_read_requests) * 100%12#### Query Cache命中率 ` mysql&gt; show status like ‘Qcache%’;Query_cache_hits = (Qcahce_hits / (Qcache_hits + Qcache_inserts )) * 100%;1#### Table Cache状态量 mysql&gt; show global status like ‘open%’;123比较 open_tables 与 opend_tables 值 #### Thread Cache 命中率 mysql&gt; show global status like ‘Thread%’;mysql&gt; show global status like ‘Connections’;Thread_cache_hits = (1 - Threads_created / connections ) * 100%1#### 锁定状态 mysql&gt; show global status like ‘%lock%’;Table_locks_waited/Table_locks_immediate=0.3% 如果这个比值比较大的话，说明表锁造成的阻塞比较严重Innodb_row_lock_waits innodb行锁，太大可能是间隙锁造成的1#### 复制延时量 mysql &gt; show slave status查看延时时间1#### Tmp Table 状况(临时表状况) mysql &gt; show status like ‘Create_tmp%’;Created_tmp_disk_tables/Created_tmp_tables比值最好不要超过10%，如果Created_tmp_tables值比较大，可能是排序句子过多或者是连接句子不够优化1#### Binlog Cache 使用状况 mysql &gt; show status like ‘Binlog_cache%’;如果Binlog_cache_disk_use值不为0 ，可能需要调大 binlog_cache_size大小1#### Innodb_log_waits 量 mysql &gt; show status like ‘innodb_log_waits’;Innodb_log_waits值不等于0的话，表明 innodb log buffer 因为空间不足而等待123456### Mysql查看死锁和解除锁所谓死锁&lt;DeadLock&gt;：是指两个或两个以上的进程在执行过程中,因争夺资源而造成的一种互相等待的现象,若无外力作用，它们都将无法推进下去.此时称系统处于死锁状态或系统产生了死锁，这些永远在互相等待的进程称为死锁进程。表级锁不会产生死锁.所以解决死锁主要还是针对于最常用的InnoDB。解除正在死锁的状态有两种方法：* **第一种**1.查询是否锁表 mysql&gt; show OPEN TABLES where In_use &gt; 0;12.查询进程（如果您有SUPER权限，您可以看到所有线程。否则，您只能看到您自己的线程） mysql&gt; show processlist;13.杀死进程id（就是上面命令的id列） mysql&gt; kill id12* **第二种**1.查看下在锁的事务 mysql&gt; SELECT * FROM INFORMATION_SCHEMA.INNODB_TRX;12.杀死进程id（就是上面命令的trx_mysql_thread_id列） mysql&gt; kill 线程ID1234567&gt;例子：查出死锁进程：mysql&gt; SHOW PROCESSLIST杀掉进程: mysql&gt; KILL 420821;其它关于查看死锁的命令： 1：查看当前的事务mysql&gt; SELECT FROM INFORMATION_SCHEMA.INNODB_TRX;2：查看当前锁定的事务mysql&gt; SELECT FROM INFORMATION_SCHEMA.INNODB_LOCKS;3：查看当前等锁的事务mysql&gt; SELECT * FROM INFORMATION_SCHEMA.INNODB_LOCK_WAITS;` 后记：文中提到一些数字都是参考值，了解基本原理就可以，除了MySQL提供的各种status值外，操作系统的一些性能指标也很重要，比如常用的top,iostat等，尤其是iostat，现在的系统瓶颈一般都在磁盘IO上，关于iostat的使用.在尝试学习新的语言之前先理解这门语言的设计原理能够让你在探索这门新语言时保持一个清醒而且开发的状态。]]></content>
      <categories>
        <category>SQL</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zabbix服务器监控之《配置Zabbix报警》（六）]]></title>
    <url>%2F2018%2F09%2F13%2Fzabbox-alert.html</url>
    <content type="text"><![CDATA[上一篇 我们已经配置过zabbix的触发器了，也知道当触发器阀值达到时，应该有一个动作，而这个动作可以是执行脚本，也可以是发邮件报警通知用户。那么本篇我们将再进行对zabbix的报警进行配置。 报警媒介类型当zabbix中的某些被监控指标出现异常时，zabbix会通过哪种方式通知运维人员呢？是通过邮件呢，还是通过短信呢，或者是通过其他方式呢？今天我们就来聊聊zabbix的报警方式，无论是通过邮件报警还是通过短信报警，无非都是通过某种”媒介”将报警信息传递给收信人，所以在zabbix中，报警方式被称为”报警媒介”，那么，zabbix都支持哪些报警媒介呢，我们一起来看看。zabbix支持的报警媒介如下： Email：邮件，这是最常用也是最传统的一种报警媒介，邮件报警，zabbix通过配置好的SMTP邮件服务器向用户发送对应的报警信息。 Script：脚本，当zabbix中的某些监控项出现异常时，也可以调用自定义的脚本进行报警，脚本的使用就比较灵活，具体怎样报警全看你的脚本怎么写。 SMS：短信，如果想要使用短信报警，则需要依赖短信网关（貌似需要北美的运行商）。 Jabber：即时通讯服务。 Ez Texting：商业的，收费的短信服务（北美运营商提供服务）。 第三方的onealert 定义报警媒介 下面我们通过配置邮件报警媒介来进行配置说明 点击管理-报警媒介类型-创建媒介类型，输入完信息后，点击添加之后，我们可以在报警媒介类型中看到已经添加的报警类型，点击右边的测试，可以测试当前邮件配置是否正常，如果配置正常你将收到测试的邮件。到此处，我们已经成功的定义了一个”报警媒介”，从此，我们可以通过这个媒介，向用户发送报警信息了。 配置用户接受报警通知媒介但是，如果想要某个zabbix用户能够接收到从”email报警媒介”发送过来的报警，还需要进一步配置，比如，当”Admin”用户想要通过”email”报警媒介接收警报时，则必须能够”适配”这种媒介，如果”Admin”用户没有使用”email媒介”的能力，那么”Admin”用户将无法接收到由”email媒介”发出的报警信息。我们应该怎样让用户能够对应的报警媒介呢，配置步骤如下。 点击管理-用户-Admin-报警媒介到此处，我们已经成功的定义了一个用户能接受对应”报警媒介”的邮件通知了。 配置告警动作 在zabbix中创建一个动作，前文中我们已经创建了用于监控磁盘根目录使用率的监控项,以及对应的触发器，现在，我们需要创建一个动作，与监控项和触发器结合起来一起使用。 打开zabbix控制台，点击配置-动作-事件源-触发器-创建动作点击操作点击恢复操作完成以上配置，点击添加即可添加一个对该触发器的动作。 上图报警的宏如下，更多的宏参考官方宏使用 1234567891011121314151617181920212223# 故障时默认标题：故障&#123;TRIGGER.STATUS&#125;,服务器:&#123;HOSTNAME&#125;发生: &#123;TRIGGER.NAME&#125;故障!消息内容：告警主机:&#123;HOSTNAME&#125;告警时间:&#123;EVENT.DATE&#125; &#123;EVENT.TIME&#125;告警等级:&#123;TRIGGER.SEVERITY&#125;告警信息: &#123;TRIGGER.NAME&#125;告警项目:&#123;TRIGGER.KEY&#125;问题详情:&#123;ITEM.NAME&#125;:&#123;ITEM.VALUE&#125;当前状态:&#123;TRIGGER.STATUS&#125;:&#123;ITEM.VALUE&#125;事件ID:&#123;EVENT.ID&#125;恢复时默认标题：恢复&#123;TRIGGER.STATUS&#125;, 服务器:&#123;HOSTNAME&#125;: &#123;TRIGGER.NAME&#125;已恢复!消息内容：告警主机:&#123;HOSTNAME&#125;告警时间:&#123;EVENT.DATE&#125; &#123;EVENT.TIME&#125;告警等级:&#123;TRIGGER.SEVERITY&#125;告警信息: &#123;TRIGGER.NAME&#125;告警项目:&#123;TRIGGER.KEY&#125;问题详情:&#123;ITEM.NAME&#125;:&#123;ITEM.VALUE&#125;当前状态:&#123;TRIGGER.STATUS&#125;:&#123;ITEM.VALUE&#125;事件ID:&#123;EVENT.ID&#125; 模拟告警在此之前我们已经总结了主机、监控项、触发器、事件、动作等相关知识点，但是到目前为止，还没有真正的收到过任何一个zabbix中的警告，那么这次，我们就在之前的基础上，刻意的让磁盘根目录使用率这个监控项达到指定的阈值，看看能否正常的收到报警信息。在模拟前，我们先看看之前磁盘根目录使用率是多少，使用率0.39% 阀值5%。好了，现在我们进入到被监控主机的根分区，在根分区中创建一个大文件，提高磁盘使用率。1234567891011121314[root@k8s nginx]# dd if=/dev/zero of=/testfile count=20 bs=1G记录了20+0 的读入记录了20+0 的写出21474836480字节(21 GB)已复制，44.8811 秒，478 MB/秒[root@k8s nginx]# df -h文件系统 容量 已用 可用 已用% 挂载点/dev/sda5 296G 22G 275G 8% /devtmpfs 3.9G 0 3.9G 0% /devtmpfs 3.9G 0 3.9G 0% /dev/shmtmpfs 3.9G 18M 3.9G 1% /runtmpfs 3.9G 0 3.9G 0% /sys/fs/cgroup/dev/sda2 200G 33M 200G 1% /home/dev/sda1 297M 107M 191M 36% /boottmpfs 797M 0 797M 0% /run/user/0 此时我们的根分区使用率已经达到了8%，超过了触发器设置的阀值5%，我们看看下图，（这里显示的是7.16%和df -h命令8%差一点点是由于linux只显示整数，小数点增1位）我们配置的是1分钟检测一次，因此我们能很快收到故障邮件通知报警。如下此时我们删掉dd命令生成的testfile文件进行恢复测试。1234567891011[root@k8s /]# rm -rf testfile [root@k8s /]# df -h文件系统 容量 已用 可用 已用% 挂载点/dev/sda5 296G 1.2G 295G 1% /devtmpfs 3.9G 0 3.9G 0% /devtmpfs 3.9G 0 3.9G 0% /dev/shmtmpfs 3.9G 18M 3.9G 1% /runtmpfs 3.9G 0 3.9G 0% /sys/fs/cgroup/dev/sda2 200G 33M 200G 1% /home/dev/sda1 297M 107M 191M 36% /boottmpfs 797M 0 797M 0% /run/user/0 那么很快我们将收到监控项恢复的邮件通知至此，我们已经完成了初步的zabbix使用了。也可以用户监控环境去监控服务了，当然监控项、触发器等还需要根据实际情况去配置。]]></content>
      <categories>
        <category>Zabbix</category>
      </categories>
      <tags>
        <tag>zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zabbix服务器监控之《配置Zabbix触发器》（五）]]></title>
    <url>%2F2018%2F09%2F13%2Fzabbix-trigger.html</url>
    <content type="text"><![CDATA[上一篇 我们已经配置过zabbix的监控项了，本篇我们将再进行对监控项的触发器配置。而在配置zabbix触发器前，我们再来理解一下触发器，事件，动作。 触发器（Triggers）：我们可以把zabbix的触发器理解成一个条件表达式，我们往往通过触发器定义被监控项的阈值，当触发器对应的表达式被满足时，则代表被监控项达到了我们设定的阈值，也就意味着发生了我们不想要遇到的问题，换句话说，当监控项的值处于合理范围时，触发器不会被触发，当监控项的值超出合理范围（即达到阈值），触发器则会被触发，当触发器被触发时，往往代表着出现了问题，触发器未被触发时，其的状态为”OK”,当触发器被触发时，触发器的状态为”Problem”，当被监控项的值达到阈值时，触发器的状态从”OK”变为”Problem”,当监控项的值再次回归到合理范围时，触发器的状态会从”Problem”转换回”OK”。 事件（Events）：当触发器的状态发生改变时，则会产生对应的”事件”，当然，由触发器的状态改变而产生的事件被称为”触发器事件”，zabbix中，事件分为几种类型，除了”触发器事件”，还有一些别的事件，此处为了方便描述，暂且不提及他们，我们可以把”事件”大概理解成一个重要的事情。 动作（Actions）：当某个事件产生时，需要对应的处理措施，这种处理措施被称为动作。 配置触发器 我们还是以上节监控nginx1主机根目录的监控项为例进行创建触发器 点击配置-主机进入对应主机上的触发器后，点击右上角的创建触发器根据下图配置触发器信息，触发器也可以在模版的监控项中创建，也可以在主机中创建。点击添加后，我们可以在对应主机中查看根目录使用率的触发器最后，我们在监测-最新数据中过滤显示根目录使用率监控项，点击图形，可以看到该监控项已经有触发器了。至此，一个关于磁盘根目录使用率超过5%后的触发器就配置完成。然而当这个监控项的值达到我们指定的阈值5%时，就会产生某个”事件”，以便我们采取后续的措施，而这个后续操作可以是命令，也可以是报警通知。 触发器表达式说明如下图，触发器的表达式生成方式 其实，上面的5个部分我们可以通过如下语法表示，如下语法描述了一个触发器的条件表达式的基本结构，{&lt;server&gt;:&lt;key&gt;.&lt;function&gt;(&lt;parameter&gt;)}&lt;operator&gt;&lt;constant&gt; 下面我们对触发器的表达式语法进行说明，如：{web server1:vfs.fs.size[/,pused].last()}&gt;5，那么，我们把该表达式分解成5个部分，从而方便我们去理解。 web server1：表示主机名称。 vfs.fs.size[/,pused]：表示对应主机上某个监控项对应的key。 last()：对应上述语法中的()，last()被我们称之为函数。 \&gt;：对应了上述语法中的，其实就是常用的比较操作符或者运算操作符。 5：表示用于设定监控项对应的阈值。 function除了last()常用的有 avg、count、change、sum、max、min、date等，看这些函数的名字你也能猜出其大概的作用，无非就是获取监控项的值的最大值，最小值，值的总和，或者平均值等，如果你想要了解它们，可以登录zabbix的官网查看触发器表达式函数在线手册 而函数的参数格式变化则比较少，如果参数值前面带有”#”作为前缀,则表示次数，比如avg(#10),则表示最近10次监控项的值的平均值，如果参数值前面没有”#”作为前缀，则表示时间，比如sum(300),表示300秒内监控项的值的总和，max(#20)则表示最近20次监控项的值的最大值，min(600)则表示最近10分钟内监控项的值的最小值，但是需要注意，last(0)的含义与last(#1)的含义相同，都表示最近一次。有的函数还支持使用第二个参数，比如avg(1h,1d) ，表示一天前的一小时内的监控项的值的平均值，假设现在的时间是5点，avg(1h)可以理解为4点到5点之间的监控项的值的平均值，而avg(1h,1d) 中的1d表示时间偏移量，那么avg(1h,1d)可以理解为昨天4点到5点的监控项的值的平均值。 operator，其实就是常用的比较操作符或者运算操作符，由于zabbix版本的不同，操作符可能有所变化，zabbix4.0支持的操作符可以操作符参考在线手册。]]></content>
      <categories>
        <category>Zabbix</category>
      </categories>
      <tags>
        <tag>zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zabbix服务器监控之《配置Zabbix监控项》（四）]]></title>
    <url>%2F2018%2F09%2F12%2Fzabbix-item.html</url>
    <content type="text"><![CDATA[通过上一篇 我们已经进行了对zabbix的agent端安装，以及通过手动、自动发现、自动注册的方式将agent端监控到server端，也知道应用集、监控项、触发器是什么概念。那么本篇我们将实际配置zabbix的监控项。 监控项说明在Zabbix中，我们要监控的某一个指标，被称为“监控项(item)”，比如监控磁盘的使用率，这就属于一个监控项，如果要获取到”监控项”的相关信息，我们则要执行一个命令，但是我们不能直接调用命令，而是通过一个”别名”去调用命令，这个”命令别名”在zabbix中被称为”键”(key)，所以，在zabbix中，如果我们想要获取到一个”监控项”的值，则需要有对应的”键”，通过”键”能够调用相应的命令，获取到对应的监控信息，而监控项的key、键值又可以为带参数和不带参数两种，下面我们将分别对其进行说明配置。 不参数键值监控项键值中只有键名的键称为不带参数的键值，如system.cpu.switches 带参数的键值监控项键值中有键名和参数的键值，如vfs.fs.size[fs,&lt;mode&gt;]，对于vfs.fs.size[fs,&lt;mode&gt;]这个键来说，vfs.fs.size就是键名，[fs,&lt;mode&gt;]就是这个键需要的参数。而[fs,&lt;mode&gt;]这两个参数中，fs是不可省参数，mode是可省参数。 配置监控项监控项可以单独配置在主机中，使该监控项专属于该主机。也可以配置在相应模版中，然后主机对该模版进行链接，从而使该主机也具有该模版的所有监控项。 在通常配置zabbix监控时，我们通常将监控项配置在模版中，这样方便调度和管理。(以下配置也采用监控项配置在模版里) 系统默认自带一些模版，这些模版有监控CPU的、内存的、http的等等，如果没有自己需要的也可以自定义模版，应用集、监控项、触发器等。 配置不带参数的监控项点击配置-模版-Template OS Linux 找到Template OS Linux模版 点击Template OS Linux模版中的监控项-创建监控项，完成下图设置后添加现在我们想要在Template OS Linux模版中创建CPU的上下文切换的监控项，那么我们可以在此界面进行如下配置。现在我们可以看到在Template OS Linux模版中已经有我们刚刚创建的名为Context switches per second监控项此时我们让主机和该模版关联，使主机能使用该模版的监控项， 点击配置-主机-模版，把Template OS Linux模版链接到主机中完成以上操作后，我们对主机进行cpu上下文监控也就配置完成，可以等待2分钟让zabbix进行数据采集后，在监测-最新数据通过过滤器过滤出CPU上下文切换的监控项最新数据也可以点击旁边的图形，查看图形信息 配置带参数的监控项点击Template OS Linux模版中的监控项-创建监控项，由于配置项已在上面说明，我们只阐述不同的地方由于Template OS Linux模版在上面已经加入到nginx1主机了，所以我们这里就不做主机和模版关联了最后等待几分钟在监测-最新数据通过过滤器过滤出磁盘使用率的监控项最新数据 关于这个键到底怎么使用呢，类似和fs和mode这两个参数分别代表了什么呢，我们可以通过官网帮助手册，查看这些”键”的含义与使用方法。]]></content>
      <categories>
        <category>Zabbix</category>
      </categories>
      <tags>
        <tag>zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zabbix服务器监控之《安装Zabbix Agent》（三）]]></title>
    <url>%2F2018%2F09%2F11%2Fzabbix-agent.html</url>
    <content type="text"><![CDATA[上一篇 我们已经进行了对zabbix的server端安装，本篇我们将进行对Zabbix agent端安装配置！ 通过第一篇 我们已经知道server-agent 都是部署到被监控主机上，由agent采集数据，报告给zabbix-server端进行监控的。 安装Zabbix代理端 由于zabbix-server用的是4.2版，那么我们agent也用4.2版。 yum安装zabbix-agent123456#yum安装zabbix源$ rpm -Uvh https://repo.zabbix.com/zabbix/4.2/rhel/7/x86_64/zabbix-release-4.2-1.el7.noarch.rpm# 安装zabbix-agent代理端# zabbix_sender` 为测试是否能够向server端发送数据的工具$ yum install -y zabbix-agent zabbix-sender zabbix代理端配置123456789# 修改/etc/zabbix/zabbix_agentd.conf配置文件# 设置被动模式下zabbix-server的IP地址Server=192.168.20.210# 设置主动模式下zabbix-server的IP地址ServerActive=192.168.20.210# 设置本机zabbix-agent主机名称Hostname=web server1 启动zabbix代理端12345# 启动zabbix-agent$ systemctl start zabbix-agent# 开机自启动zabbix-agent`$ systemctl enable zabbix-agent 默认情况下，zabbix-agent不能使用root用户运行的，如果非要以root用户运行，可以在/etc/zabbix/zabbix_agentd.conf 配置文件中设置AllowRoot=1 此时查看zabbix-agent进程和端口是否正常：1234567891011121314# 查看端口是否正常$ netstat -ptlnProto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 0.0.0.0:10050 0.0.0.0:* LISTEN 11734/zabbix_agentd # 查看进程是否正常$ ps -ef | grep zabbix_agentzabbix 11734 1 0 4月13 ? 00:00:00 /usr/sbin/zabbix_agentd -c /etc/zabbix/zabbix_agentd.confzabbix 11735 11734 0 4月13 ? 00:00:00 /usr/sbin/zabbix_agentd: collector [idle 1 sec]zabbix 11736 11734 0 4月13 ? 00:00:00 /usr/sbin/zabbix_agentd: listener #1 [waiting for connection]zabbix 11737 11734 0 4月13 ? 00:00:00 /usr/sbin/zabbix_agentd: listener #2 [waiting for connection]zabbix 11738 11734 0 4月13 ? 00:00:00 /usr/sbin/zabbix_agentd: listener #3 [waiting for connection]zabbix 11739 11734 0 4月13 ? 00:00:00 /usr/sbin/zabbix_agentd: active checks #1 [idle 1 sec]root 12105 11549 0 00:23 pts/0 00:00:00 grep --color=auto zabbix_agent 至此zabbix-agent 代理端安装完毕。 监控zabbix-agent代理端zabbix-server监控zabbix-agent有两种方式： 手动添加对应的zabbix-agent客户端 自动发现和自动注册 手动添加zabbix-agent适用于少量的被监控主机，而自动发现主要是通过发现网络中的主机，并自动把主机添加到监控中，并关联特定的模板，实现自动监控，适合有大量被监控主机，减少频繁手动添加主机的麻烦操作。 手动添加zabbix-agent客户端浏览器输入http://192.168.20.210/zabbix ，访问zabbix服务，点击配置-主机-创建主机点击模版 ，选择主机属于哪个模版 （模版里面包含需要监控的条目）点击添加 应用集： 表示模版中监控业务的一类型。（如：CPU应用集属于Template OS Linx模版） 监控项： 表示某一应用集中监控的某一项。（如：CPU负载、CPU使用率都属于CPU应用集中的监控项） 触发器： 表示某一监控项达到某自定义的阀值时，该出现的状态。（如：当CPU使用率达到70%设置他的状态为警告，80%为严重，这就属于触发器） 图 形： 表示哪些监控项进行了图形显示。（如：将CPU使用率用图形显示一段时间里的曲线变化） 自动发现：表示哪些监控项是可以通过自动发现进行监控。（如：自动发现主机网络接口流量） web监测: 表示zabbix把web某页面也监控起来，第一时间得知web崩溃信息并做相应处理。（如：监控http某一页面，如果状态码不是200，通过触发器返回严重告警） 至此手动添加zabbix-agent客户端步骤完成，可以在监控-图形中查看对监控项设置了图形的状态了。 自动发现当监控主机不断增多，有的时候需要添加一批机器，特别是刚用zabbix的运维人员需要将公司的所有服务器添加到zabbix，如果使用传统办法去单个添加设备、分组、项目、图像…..结果可想而知。鉴于这个问题我们可以好好利用下Zabbix的一个发现(Discovery)模块，进而来实现自动刚发现主机、自动将主机添加到主机组、自动加载模板、自动创建项目（item）、自动创建图像，下面我们来看看这个模块如何使用。 自动发现由服务端主动发起，Zabbix Server开启发现进程，定时扫描局域网中IP服务器、设备。 自动发现过程需要分为两个步骤： 通过网络扫描制定的服务，如：Zabbix Agent是否可以访问system.uname指标 发现主机之后需要执行添加的动作，这个过程由动作（Action）完成 点击配置-自动发现-创建发现规则填入名称、需发现服务器、设备的IP范围、更新间隔、检查项（ssh和zabbix客户端）、设备唯一性准则，最后勾选已启用、点击添加。进行自动发现规则的创建 在点击配置-动作-事件源-自动发现-创建动作 进行主机自动加入主机组并关联模板点击操作 对该动作关联到模版的操作添加到主机：将发现到的主机添加到主机添加到主机群组：选择要添加到的主机组链接到模版：链接到模板、选择相应的模板 点击添加完成动作规则的创建，至此发现主机、添加主机并将主机添加到主机组、链接模板全部完毕。 此时可以在监测-自动发现中看到已经被自动发现规则监测到的内网服务器！也可以在配置-主机群组-Discovered hosts中看到已经被自动发现的主机。也可以在监测-图形中查看自动发现的主机监控项的图形。 自动注册当主机分布在不同的城市，比如不同的云环境中时，使用主动发现就不好处理了，使用自动注册的方式非常适合在云环境中的部署。 由客户端主动发起，客户端必须安装并启动Agentd，否则无法被自动注册添加至主机列表。 点击配置-动作-事件源-自动注册-创建动作 进行主机自动加入主机组并关联模板点击操作选择具体的操作类型：添加主机、添加到主机群组、与模板关联的操作，最后点击添加。 在配置-主机中查看注册的设备信息]]></content>
      <categories>
        <category>Zabbix</category>
      </categories>
      <tags>
        <tag>zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zabbix服务器监控之《安装Zabbix Server》（二）]]></title>
    <url>%2F2018%2F09%2F11%2Fzabbix-server.html</url>
    <content type="text"><![CDATA[上一篇 我们已经讲了zabbix的常用组件和工作模式，本篇我们将进行对Zabbix Server端安装配置！ 搭建环境系统信息 系统 版本 IP 关系 centos 7.5 192.168.20.210 服务端 centos 7.5 192.168.20.211 代理端 环境配置 设置主机名，重启生效 12345# server端$ echo "zabbix-server" &gt; /etc/hostname # agent端$ echo "zabbix-agent" &gt; /etc/hostname 关闭SELinux和防火墙检查 123$ sed -i "s/SELINUX=enforcing/SELINUX=disabled/g" /etc/selinux/config$ systemctl stop firewalld.service$ systemctl disable firewalld.service 安装Zabbix服务端 我们这里安装的zabbix版本为4.2版本 yum安装zabbix-server123456#yum安装zabbix源$ rpm -Uvh https://repo.zabbix.com/zabbix/4.2/rhel/7/x86_64/zabbix-release-4.2-1.el7.noarch.rpm#安装zabbix服务端#zabbix-get为测试是否能够从agent端拉取数据的工具$ yum -y install zabbix-server-mysql zabbix-web-mysql zabbix-agent zabbix-get 安装mysql数据库 如果有现有的数据库环境，请跳过安装数据库环节，直接从创建zabbix数据库 开始。 12345678910111213# 在线yum安装mysql5.7$ wget -c https://dev.mysql.com/get/mysql80-community-release-el7-1.noarch.rpm$ rpm -ivh mysql80-community-release-el7-1.noarch.rpm$ yum -y install yum-utils$ yum-config-manager --disable mysql80-community$ yum-config-manager --enable mysql57-community$ yum install mysql-community-server -y# 启动mysql$ systemctl start mysqld# 开机启动$ systemctl enable mysqld 修改root密码12345# 查看mysql临时密码$ grep 'temporary password' /var/log/mysqld.log# 使用mysql临时登录，修改root密码mysql&gt; ALTER USER 'root'@'localhost' IDENTIFIED BY 'Ala@2018'; 创建zabbix数据库创建zabbix用户和库12mysql&gt; create database zabbix character set utf8 collate utf8_bin;mysql&gt; grant all privileges on zabbix.* to zabbix@localhost identified by "Ala@2018"; 导入zabbix数据库在shell命令行执行导入zabbix数据1$ zcat /usr/share/doc/zabbix-server-mysql*/create.sql.gz | mysql -uzabbix -p'Ala@2018' zabbix zabbix服务端配置123456789101112131415# 配置zabbix连接的数据库地址、数据库名以及数据库用户DBHost=localhostDBName=zabbixDBUser=zabbix# 修改`/etc/zabbix/zabbix_server.conf` 文件，修改mysql连接密码DBPassword=Ala@2018# 添加上海区$ sed -i.ori '19a php_value date.timezone Asia/Shanghai' /etc/httpd/conf.d/zabbix.conf# 解决图形列表下中文乱码$ yum -y install wqy-microhei-fonts$ mv /usr/share/fonts/dejavu/DejaVuSans.ttf /usr/share/fonts/dejavu/DejaVuSans.ttf.bak$ cp -f /usr/share/fonts/wqy-microhei/wqy-microhei.ttc /usr/share/fonts/dejavu/DejaVuSans.ttf 启动zabbix服务端并配置12345# 启动 zabbix-server和httpd服务$ systemctl start zabbix-server httpd# 开机启动$ systemctl enable zabbix-server httpd 浏览器输入http://192.168.20.210/zabbix ，访问zabbix，如下图接下来点击 Next setup从上图可以看到zabbix相关组件配置，继续点击 Next setup上图中配置好之后，继续点击 Next setup上图中，name尽量取有意义的名字，继续点击 Next setup到这一步可以看到全部配置，确认无误后点击 Next setup登录zabbix登录之后点击 管理-用户-点击Admin ，可以设置超级管理基本属性，例如语言和主题，点击配置-主机 ，可以看到如下图，接下来安装zabbix客户端 安装zabbix agent客户端 这里的客户端作用是监控服务端本机 配置客户端，配置文件/etc/zabbix/zabbix_agentd.conf12345678# 主要配置如下，默认即可Server=127.0.0.1ServerActive=127.0.0.1Hostname=Zabbix server# 启动zabbix客户端systemctl start zabbix-agent# 开机启动systemctl enable zabbix-agent 现在可以看到可用性ZBX 为绿色，表示的是zabbix-agent服务连接正常]]></content>
      <categories>
        <category>Zabbix</category>
      </categories>
      <tags>
        <tag>zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zabbix服务器监控之《初识zabbix概念》（一）]]></title>
    <url>%2F2018%2F09%2F11%2Fzabbix-summarize.html</url>
    <content type="text"><![CDATA[为什么要用Zabbix对于运维人员来说，监控是非常重要的，因为如果想要保证线上业务整体能够稳定运行，那么我们则需要实时关注与其相关的各项指标是否正常，而一个业务系统的背后，往往存在着很多的服务器、网络设备等硬件资源，如果我们想要能够更加方便的、集中的监控他们，我们则需要依靠一些外部的工具，而zabbix就是一个被广泛使用的，可以实现集中监控管理的应用程序。 Zabbix支持那些通讯方式 agent ：通过专用的代理程序进行监控，与常见的master/agent模型类似,如果被监控对象支持对应的agent，推荐首选这种方式。 ssh/telnet ：通过远程控制协议进行通讯，比如ssh或者telnet。 SNMP ：通过SNMP协议与被监控对象进行通讯，SNMP协议的全称为Simple Network Management Protocol ,被译为 “简单网络管理协议”，通常来说，我们无法在路由器、交换机这种硬件上安装agent，但是这些硬件往往都支持SNMP协议，SNMP是一种比较久远的、通行的协议，大部分网络设备都支持这种协议，其实SNMP协议的工作方式也可以理解为master/agent的工作方式，只不过是在这些设备中内置了SNMP的agent而已，所以，大部分网络设备都支持这种协议。 IPMI ：通过IPMI接口进行监控，我们可以通过标准的IPMI硬件接口，监控被监控对象的物理特征，比如电压，温度，风扇状态，电源状态等。 JMX ：通过JMX进行监控，JMX（Java Management Extensions，即Java管理扩展），监控JVM虚拟机时，使用这种方法也是非常不错的选择。 Zabbix监控流程一般情况下，我们将zabbix agent部署到被监控主机上，由agent采集数据，报告给负责监控的中心主机，中心主机也就是master/agent模型中的master，负责监控的中心主机被称为zabbix server，zabbix server将从agent端接收到的信息存储于zabbix的数据库中，我们把zabbix的数据库端称为zabbix database， 如果管理员需要查看各种监控信息，则需要zabbix的GUI，zabbix的GUI是一种Web GUI，我们称之为zabbix web，zabbix web是使用php编写的，所以，如果想要使用zabbix web展示相关监控信息，需要依赖LAMP环境，不管是zabbix server ，或是zabbix web，他们都需要连接到zabbix database获取相关数据，这样说可能不容易理解，对比下图理解上述概念，就容易许多。 当监控规模变得庞大时，我们可能有成千上万台设备需要监控，这时我们是否需要部署多套zabbix系统进行监控呢？如果部署多套zabbix监控系统，那么监控压力将会被分摊，但是，这些监控的对象将会被尽量平均的分配到不同的监控系统中，这个时候，我们就无法通过统一的监控入口，去监控这些对象了，虽然分摊了监控压力，但是也增加了监控工作的复杂度，那么，我们到底该不该建立多套zabbix监控系统从而分摊巨大的监控压力呢？其实，zabbix天生就有处理这种问题的能力，因为zabbix支持分布式监控，我们可以把成千上万台的被监控对象分成不同的区域，每个区域中设置一台代理主机，区域内的每个被监控对象的信息被agent采集，提交给代理主机，在这个区域内，代理主机的作用就好比zabbix server，我们称这些代理主机为zabbix proxy，zabbix proxy再将收集到的信息统一提交给真正的zabbix server处理，这样，zabbix proxy分摊了zabbix server的压力，同时，我们还能够通过统一的监控入口，监控所有的对象，当监控规模庞大到需要使用zabbix proxy时，zabbix的架构如下图，我们可以对比下图，理解上述描述。 总结zabbix核心组件 zabbix agent ：部署在被监控主机上，负责被监控主机的数据，并将数据发送给zabbix server。 zabbix server ：负责接收agent发送的报告信息，并且负责组织配置信息、统计信息、操作数据等。 zabbix database ：用于存储所有zabbix的配置信息、监控数据的数据库。 zabbix proxy ：可选组件，用于分布式监控环境中，zabbix proxy代表server端，完成局部区域内的信息收集，最终统一发往server端。 zabbix工作模式我们知道，agent端会将采集完的数据主动发送给server端，这种模式我们称之为主动模式，即对于agent端来说是主动的。其实，agent端也可以不主动发送数据，而是等待server过来拉取数据，这种模式我们称之为被动模式。但是，不管是主动模式还是被动模式，都是对于agent端来说的，而且，主动模式与被动模式可以同时存在，并不冲突。管理员可以在agent端使用一个名为zabbix_sender 的工具，测试是否能够向server端发送数据。管理员也可以在server端使用一个名为zabbix_get 的工具，测试是否能够从agent端拉取数据。]]></content>
      <categories>
        <category>Zabbix</category>
      </categories>
      <tags>
        <tag>zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Centos7.X系统初始化优化脚本]]></title>
    <url>%2F2018%2F04%2F30%2Fcentos7_optimize.html</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192#!/bin/bash################################################## --Info# Initialization CentOS 7.x script################################################## Check if user is root#if [ $(id -u) != "0" ]; then echo "Error: You must be root to run this script, please use root to initialization OS." exit 1fiecho "+------------------------------------------------------------------------+"echo "| To initialization the system for security and performance |"echo "+------------------------------------------------------------------------+"# set hostnameedit_hostname() &#123; read -p "Please input your hostname: " hs echo "$&#123;hs&#125;" &gt; /etc/hostname&#125;# add user for osuser_add() &#123; read -p "To create a system user, enter username created or input 'n' not created: " new_user if [ $new_user == "n" ]; then echo "Skip create user..." else id -u $new_user if [ $? -ne 0 ];then useradd -s /bin/bash -d /home/$new_user -m $new_user &amp;&amp; echo password | passwd --stdin $new_user &amp;&amp; echo "mongod ALL=(ALL) NOPASSWD: ALL" | sudo tee /etc/sudoers.d/$new_user else echo "$new_user user is exist." fi fi&#125;# update system &amp; install pakeagesystem_update() &#123; echo "*** Starting update system &amp;&amp; install tools pakeage... ***" mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo yum -y update yum clean all &amp;&amp; yum makecache yum -y install rsync wget vim openssh-clients iftop htop iotop sysstat lsof telnet traceroute tree lrzsz net-tools dstat tree ntpdate dos2unix net-tools egrep [ $? -eq 0 ] &amp;&amp; echo "System upgrade &amp;&amp; install pakeages complete."&#125;# Set timezone synchronizationtimezone_config() &#123; echo "Setting timezone..." /usr/bin/timedatectl | grep "Asia/Shanghai" if [ $? -eq 0 ];then echo "System timezone is Asia/Shanghai." else timedatectl set-local-rtc 0 &amp;&amp; timedatectl set-timezone Asia/Shanghai fi # config chrony yum -y install chrony &amp;&amp; systemctl start chronyd.service &amp;&amp; systemctl enable chronyd.service [ $? -eq 0 ] &amp;&amp; echo "Setting timezone &amp;&amp; Sync network time complete."&#125;# disable selinuxselinux_config() &#123; sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config setenforce 0 echo "Dsiable selinux complete."&#125;# ulimit comfigulimit_config() &#123; echo "Starting config ulimit..." cat &gt;&gt; /etc/security/limits.conf &lt;&lt; EOF* soft nproc 65535* hard nproc 65535* soft nofile 65535* hard nofile 65535EOF [ $? -eq 0 ] &amp;&amp; echo "Ulimit config complete!"&#125;# sshd configsshd_config() &#123; echo "Starting config sshd..." sed -i '/^#Port/s/#Port 22/Port 2024/g' /etc/ssh/sshd_config #sed -i "$ a\ListenAddress 0.0.0.0:21212\nListenAddress 0.0.0.0:22 " /etc/ssh/sshd_config sed -i '/^#UseDNS/s/#UseDNS yes/UseDNS no/g' /etc/ssh/sshd_config systemctl restart sshd #sed -i 's/#PermitRootLogin yes/PermitRootLogin no/g' /etc/ssh/sshd_config #sed -i 's/#PermitEmptyPasswords no/PermitEmptyPasswords no/g' /etc/ssh/sshd_config [ $? -eq 0 ] &amp;&amp; echo "SSH config complete."&#125;# firewalld configdisable_firewalld() &#123; echo "clean iptables roles..." iptables -F &amp;&amp; iptables -X &amp;&amp; iptables -t nat -F &amp;&amp; iptables -t nat -X echo "Starting disable firewalld..." rpm -qa | grep firewalld &gt;&gt; /dev/null if [ $? -eq 0 ];then systemctl stop firewalld &amp;&amp; systemctl disable firewalld [ $? -eq 0 ] &amp;&amp; echo "Disable firewalld complete." else echo "Firewalld not install." fi&#125;# sysctl configconfig_sysctl() &#123; echo "Staring config sysctl..." /usr/bin/cp -f /etc/sysctl.conf /etc/sysctl.conf.bak modprobe nf_conntrack_ipv4 modprobe nf_conntrack cat &gt; /etc/sysctl.conf &lt;&lt; EOFfs.file-max=1000000vm.swappiness = 0net.ipv4.ip_forward = 1net.ipv4.conf.default.rp_filter = 1net.ipv4.conf.default.accept_source_route = 0net.ipv4.tcp_max_tw_buckets = 6000net.ipv4.ip_local_port_range = 1024 65000net.ipv4.tcp_timestamps = 1net.ipv4.tcp_tw_recycle = 0net.ipv4.tcp_tw_reuse = 1net.ipv4.tcp_syncookies = 1net.ipv4.tcp_max_syn_backlog = 262144net.core.netdev_max_backlog = 262144net.core.somaxconn = 262144net.ipv4.tcp_max_orphans = 262144net.ipv4.tcp_synack_retries = 1net.ipv4.tcp_syn_retries = 1net.ipv4.tcp_fin_timeout = 1net.ipv4.tcp_keepalive_time = 30net.ipv4.tcp_sack = 1net.ipv4.tcp_window_scaling = 1net.ipv4.tcp_rmem = 4096 87380 4194304net.ipv4.tcp_wmem = 4096 16384 4194304net.core.wmem_default = 8388608net.core.rmem_default = 8388608net.core.rmem_max = 16777216net.core.wmem_max = 16777216net.ipv4.tcp_mem = 94500000 915000000 927000000net.nf_conntrack_max = 6553500net.netfilter.nf_conntrack_max = 6553500net.netfilter.nf_conntrack_tcp_timeout_close_wait = 60net.netfilter.nf_conntrack_tcp_timeout_fin_wait = 120net.netfilter.nf_conntrack_tcp_timeout_time_wait = 120net.netfilter.nf_conntrack_tcp_timeout_established = 3600EOF# set kernel parameters work /usr/sbin/sysctl -p [ $? -eq 0 ] &amp;&amp; echo "Sysctl config complete."&#125;# close THPclose_thp() &#123; echo "Close THP..." if [[ -f /sys/kernel/mm/transparent_hugepage/enabled ]]; then echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled fi if [[ -f /sys/kernel/mm/transparent_hugepage/defrag ]]; then echo never &gt; /sys/kernel/mm/transparent_hugepage/defrag fi echo "Power Boot close THP..." cat &gt;&gt; /etc/rc.local &lt;&lt; EOFif test -f /sys/kernel/mm/transparent_hugepage/enabled; then echo never &gt; /sys/kernel/mm/transparent_hugepage/enabledfiif test -f /sys/kernel/mm/transparent_hugepage/defrag; then echo never &gt; /sys/kernel/mm/transparent_hugepage/defragfiEOF chmod +x /etc/rc.d/rc.local&#125;# disable NUMAdisable_numa() &#123; echo "configure /etc/default/grub numa=off..." sed -i "s/rhgb quiet/rhgb quiet numa=off/g" /etc/default/grub echo "rebuild grub2.cfg configure file..." grub2-mkconfig -o /etc/grub2.cfg&#125;#main functionmain() &#123; edit_hostname user_add system_update timezone_config selinux_config ulimit_config #sshd_config disable_firewalld config_sysctl close_thp disable_numa&#125;# execute main functionsmainecho "+------------------------------------------------------------------------+"echo "| To initialization system all completed !!! |"echo "| You can now restart the system for the configuration to take effect |"echo "+------------------------------------------------------------------------+"]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[阿里云ECS用465端口发邮件]]></title>
    <url>%2F2018%2F03%2F25%2Faliyun-ecs-mail.html</url>
    <content type="text"><![CDATA[原由大家可能都知道在阿里云购买的ECS云主机是不能直接通过25号端口发邮件的，因为阿里云底层对25号端口做了屏蔽。所以例如我们需要做监控、报告等邮件通知行为时，只能修改默认的25号端口。下面我们就开始如何用465端口来发送邮件。 请求数字证书 创建目录，用来存放证书 [root@PLAY ~]# mkdir -p /root/.certs/ [root@PLAY ~]# echo -n | openssl s_client -connect smtp.126.com:465 | sed -ne &apos;/-BEGIN CERTIFICATE-/,/-END CERTIFICATE-/p&apos; &gt; ~/.certs/126.crt depth=2 C = US, O = DigiCert Inc, OU = www.digicert.com, CN = DigiCert Global Root CA verify return:1 depth=1 C = US, O = DigiCert Inc, OU = www.digicert.com, CN = GeoTrust RSA CA 2018 verify return:1 depth=0 C = CN, L = Hangzhou, O = &quot;NetEase (Hangzhou) Network Co., Ltd&quot;, OU = Mail Dept., CN = *.126.com verify return:1 DONE smtp.126.com:465 为发件者的邮件服务器，我这用的是网易126.com 邮箱。生成126.crt 的证书。 添加一个证书到证书数据库中 [root@PLAY ~]# certutil -A -n &quot;GeoTrust SSL CA&quot; -t &quot;C,,&quot; -d ~/.certs -i ~/.certs/126.crt [root@PLAY ~]# certutil -A -n &quot;GeoTrust Global CA&quot; -t &quot;C,,&quot; -d ~/.certs -i ~/.certs/126.crt 列出目录下证书 [root@PLAY ~]# certutil -L -d /root/.certs Certificate Nickname Trust Attributes SSL,S/MIME,JAR/XPI GeoTrust SSL CA C,, 获取126邮箱的授权码前往邮箱的登陆网站登陆自己的邮箱，在邮箱设置里找到POP3/SMTP/IMAP 设置，并且完成客户端授权密码。如下图： 配置/etc/mail.rc文件在/etc/mail.rc文件末尾追加如下参数12345678set bsdcompatset from=test_wly@126.comset smtp="smtps://smtp.126.com:465"set smtp-auth-user=test_wly@126.comset smtp-auth-password=73jdi9dw7j3gc8gf1xvak01fssset smtp-auth=loginset ssl-verify=ignoreset nss-config-dir=/root/.certs 测试邮件发送是否正常1[root@PLAY ~]# echo "test mail" | mail -s "nagios report" test_wly@126.com 如果配置正确，此时test_wly@126.com就能收到刚刚发送的测试邮件了。 看起来已经成功了，但是发送完邮件还有报错：证书不被信任，且命令行就此卡住，需要按键才能出现命令提示符Error in certificate: Peer&#39;s certificate issuer is not recognized.可以按如下操作即可解决问题：123[root@PLAY ~]# cd /root/.certs/[root@PLAY .certs]# certutil -A -n "GeoTrust SSL CA - G3" -t "Pu,Pu,Pu" -d ./ -i 126.crt Notice: Trust flag u is set automatically if the private key is present.]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>aliyun</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker三剑客之docker-swarm集群]]></title>
    <url>%2F2018%2F03%2F23%2Fdocker-swarm.html</url>
    <content type="text"><![CDATA[Docker Swarm 概念Docker Swarm是Docker的群集。 在Docker 1.12及更高版本中，Swarm模式与Docker Engine集成在一起。它将若干单个的Docker Engine主机组成一个Docker集群。 由于Docker Swarm提供标准的Docker API，因此任何已与Docker守护程序通信的工具都可以使用Swarm透明地扩展到多个Docker Engine主机。 Docker 集群从主机的层面来看，Docker Swarm 管理的是 Docker Host 集群。所以先来讨论一个重要的概念 - 集群化（Clustering）。服务器集群由一组网络上相互连接的服务器组成，它们一起协同工作。一个集群和一堆服务器最显著的区别在于： 群能够像单个系统那样工作，同时提供高可用、负载均衡和并行处理。 实现集群化后我们的思维方式就必须改变了：不再考虑一个一个的服务器，而是将集群看做是一个整体。 集群整体容量的调整是通过往集群中添加和删除主机节点实现的。但不管做怎样的操作，集群始终还是一个整体。 swarm swarm 是运行 Docker Engine 的多个主机组成的集群。从 v1.12 开始，集群管理和编排功能已经集成进 Docker Engine。 当 Docker Engine 初始化了一个 swarm 或者加入到一个存在的 swarm 时，它就启动了 swarm mode。没启动 swarm mode 时，Docker 执行的是容器命令。 运行 swarm mode 后，Docker 增加了编排 service 的能力。Docker 允许在同一个 Docker 主机上既运行 swarm service，又运行单独的容器。 node swarm 中的每个 Docker Engine 都是一个 node，有两种类型的node，manager 和 worker。 为了向 swarm 中部署应用，我们需要在 manager node 上执行部署命令，manager和node 会将部署任务拆解并分配给一个或多个worker node 完成部署。 manager node 负责执行编排和集群管理工作，保持并维护 swarm 处于期望的状态。swarm 中如果有多个 manager node，它们会自动协商并选举出一个 leader 执行编排任务。 woker node 接受并执行由 manager node 派发的任务。默认配置下 manager node 同时也是一个 worker node，不过可以将其配置成 manager-only node，让其专职负责编排和集群管理工作。 work node 会定期向 manager node 报告自己的状态和它正在执行的任务的状态，这样 manager 就可以维护整个集群的状态。 service service 定义了 worker node 上要执行的任务。 swarm 的主要编排任务就是保证 service 处于期望的状态下。举一个 service 的例子：在 swarm 中启动一个 http 服务，使用的镜像是 httpd:latest，副本数为 3。manager node 负责创建这个 service，经过分析知道需要启动 3 个 httpd 容器，根据当前各 worker node 的状态将运行容器的任务分配下去，比如 worker1 上运行两个容器，worker2 上运行一个容器。运行了一段时间，worker2 突然宕机了，manager 监控到这个故障，于是立即在 worker1上启动了一个新的 httpd 容器。这样就保证了 service 处于期望的三个副本状态。 task 任务是在 docekr 容器中执行的命令，Manager 节点根据service指定数量的任务副本分配任务给 worker 节点。 常用指令docker swarm：集群管理，子命令有（docker swarm –help 查看更多帮助）： init: 初始化一个swarm集群 join-token: 显示manager和worker加入swarm集群的token join: 新节点接入集群 leave：节点离开当前集群 update：升级swarm集群 docker service：服务创建，子命令有（docker service –help 查看帮助）: create：创建一个新服务 inspect：获取指定服务的json状态数据 logs：获取服务或者任务的日志 ls：显示当前服务列表 ps：显示指定服务任务状态 rm：删除一个或者更多服务 scale：伸缩一个或多个复制服务 update：升级一个服务 docker node：节点管理，子命令有 （docker service –help 查看帮助）： promote：从群集中的管理器中降级一个或多个节点 demote：从群集中的管理器中提升一个或多个节点 inspect： 获取指定节点的json状态数据 update：升级一个节点 ls：显示swarm中节点列表 rm：从swarm集群中删除一个或者多个节点 ps：指定列出一个或多个节点上运行的任务，默认为当前节点 创建Docker swarm集群环境准备 所有节点Docker版本均不低于 v1.12，我们是最新版的V18.09.6，实验环境node的操作系统为 Centos7.4，当然其他 Linux 也是可以的。 所有主机均配置了/etc/hosts对应的主机和ip解析。 我们所有的操作都在k8s-master1主机上，请配置ssh-keygen确保k8s-master1主机能无密码登陆其他主机。 主机名 ip 角色 k8s-master1 192.168.20.210 swarm manager1 k8s-master2 192.168.20.211 swarm manager2 k8s-master3 192.168.20.212 swarm manager3 k8s-node1 192.168.20.213 swarm worker1 k8s-node2 192.168.20.214 swarm worker2 安装docker engine环境准备好后，我们需要为所有主机安装docker engine，你可以用传统方法登陆每台主机手动安装docker，也可以使用docker-machine统一管理和安装docker engine。我们这里使用docker-machine方法为所有主机安装docker engine。步骤我们已经在上一篇文章配置过，安装完后如下：1234567$ docker-machine lsNAME ACTIVE DRIVER STATE URL SWARM DOCKER ERRORSk8s-master1 - generic Running tcp://192.168.20.210:2376 v18.09.6 k8s-master2 - generic Running tcp://192.168.20.211:2376 v18.09.6 k8s-master3 - generic Running tcp://192.168.20.212:2376 v18.09.6 k8s-node1 - generic Running tcp://192.168.20.213:2376 v18.09.6 k8s-node2 - generic Running tcp://192.168.20.214:2376 v18.09.6 创建swarm在 k8s-master1 主机上执行如下命令创建 swarm，使用命令docker swarm init –advertise-addr 192.168.20.210。12345678$ docker swarm init --advertise-addr 192.168.20.210Swarm initialized: current node (tlviocryph6mlw8yqp2m1nmv9) is now a manager.To add a worker to this swarm, run the following command: docker swarm join --token SWMTKN-1-1fdudt9by9zsvvr480thnxzsbbd66d50q5dh0swxraa8szp7s5-awrgjg1n17793vif828xsp63x 192.168.20.210:2377To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions. 从结果输出我们可以看出 manager 已经初始化完成，swarm-manager 成为 manager node，可以看到添加 worker node 和 manager node 的执行指令。 --advertise-addr：指定与其他 node 通信的地址。 添加 node执行 docker node ls 查看当前 swarm 的 node，目前只有一个 manager。123$ docker node lsID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSIONtlviocryph6mlw8yqp2m1nmv9 * k8s-master1 Ready Active Leader 18.09.6 如果当时没有记录下 docker swarm init 提示的添加 worker 的完整命令，可以通过docker swarm join-token worker查看。 复制前面的 docker swarm join命令，分别在 swarm-worker1 和 swarm-worker2 上执行，将它们添加到 swarm 中。123# 在k8s-node1和k8s-node2上都执行$ docker swarm join --token SWMTKN-1-1fdudt9by9zsvvr480thnxzsbbd66d50q5dh0swxraa8szp7s5-awrgjg1n17793vif828xsp63x 192.168.20.210:2377This node joined a swarm as a worker. 查看添加node结果docker node ls可以看到两个 worker node 已经添加进来了。12345$ docker node lsID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSIONtlviocryph6mlw8yqp2m1nmv9 * k8s-master1 Ready Active Leader 18.09.6odkkwwrfdowu2szjxo5yejuex k8s-node1 Ready Active 18.09.6uokltoj11sdw9ffov7c1vb1lp k8s-node2 Ready Active 18.09.6 添加manager通过上面的步骤我们已经创建了一个swarm manager两个 swarm worker，那么为了高可用动态选举Leader，我们这里在创建2个manager。 如果当时没有记录下 docker swarm init 提示的添加 worker 的完整命令，可以通过docker swarm join-token manager查看。 12345# 显示对swarm集群加入manager节点的命令$ docker swarm join-token managerTo add a manager to this swarm, run the following command: docker swarm join --token SWMTKN-1-1fdudt9by9zsvvr480thnxzsbbd66d50q5dh0swxraa8szp7s5-353ufwphwu11rkokojlte8fzt 192.168.20.210:2377 复制这条 docker swarm join命令，在k8s-master1 上执行，将它们添加到 swarm 中。12345# 把k8s-master2加入集群$ docker-machine ssh k8s-master2 docker swarm join --token SWMTKN-1-1fdudt9by9zsvvr480thnxzsbbd66d50q5dh0swxraa8szp7s5-353ufwphwu11rkokojlte8fzt 192.168.20.210:2377# 把k8s-master3加入集群$ docker-machine ssh k8s-master3 docker swarm join --token SWMTKN-1-1fdudt9by9zsvvr480thnxzsbbd66d50q5dh0swxraa8szp7s5-353ufwphwu11rkokojlte8fzt 192.168.20.210:2377 这里使用的命令和加入worker节点不一样，使用的是docker-machine命令，它是通过ssh远程执行docker swarm join命令，和直接在对应主机上执行效果一样。当然还可以使用ansible。 查看添加manager结果docker node ls可以看到两个 manager node 已经添加进来了。1234567$ docker node lsID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSIONtlviocryph6mlw8yqp2m1nmv9 * k8s-master1 Ready Active Leader 18.09.6cgacdrgz7g80bdujq6x66ze7v k8s-master2 Ready Active Reachable 18.09.6n40j3686zd4f5w0vrfjb0p8be k8s-master3 Ready Active Reachable 18.09.6odkkwwrfdowu2szjxo5yejuex k8s-node1 Ready Active 18.09.6uokltoj11sdw9ffov7c1vb1lp k8s-node2 Ready Active 18.09.6 至此，三manager节点和两worker节点的 swarm 集群就已经搭建好了，操作还是相当简单的。 部署Service通过开头对Service的介绍，我们已经知道什么是Service了，现在我们就来创建它。 创建Service我们创建好了 Swarm 集群， 现在部署一个运行nginx镜像的 service，执行如下命令：12345$ docker service create --name httpd nginx5vk0p382fudcwkf82ksf20soverall progress: 1 out of 1 tasks 1/1: running [==================================================&gt;] verify: Service converged 请docker service create –help 查看更多关于配置service的选项。 此时我们已经创建好了一个名为httpd的nginx镜像服务，查看一下：123$ docker service lsID NAME MODE REPLICAS IMAGE PORTS5vk0p382fudc httpd replicated 1/1 nginx:latest REPLICAS 显示当前副本信息，1/1 的意思是 httpd 这个 service 期望的容器副本数量为 1，目前已经启动的副本数量为 1。也就是当前 service 已经部署完成。命令 docker service ps可以查看 service 每个副本的状态。123$ docker service ps httpdID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTSp3p9ld1k3xq9 httpd.1 nginx:latest k8s-master1 Running Running 1 minutes ago 我们可以看到 service 被分配到了 k8s-master1 上面。 设置服务副本数前面部署了只有一个副本的 Service，不过对于 web 服务，我们通常会运行多个实例。这样可以负载均衡，同时也能提供高可用。swarm 要实现这个目标非常简单，增加 service 的副本数就可以了。在 swarm-manager 上执行如下命令：1234567$ docker service scale httpd=3web_server scaled to 3overall progress: 3 out of 3 tasks 1/3: running [==================================================&gt;] 2/3: running [==================================================&gt;] 3/3: running [==================================================&gt;] verify: Service converged 副本数增加到 3，通过 docker service ls和 docker service ps httpd查看副本的详细信息。123456789$ docker service lsID NAME MODE REPLICAS IMAGE PORTS5vk0p382fudc httpd replicated 3/3 nginx:latest#$ docker service ps httpdID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTSp3p9ld1k3xq9 httpd.1 nginx:latest k8s-master1 Running Running 2 minutes ago 2ojzjk66n986 httpd.2 nginx:latest k8s-node1 Running Running 2 minutes ago 41pkpuzqq6rs httpd.3 nginx:latest k8s-node2 Running Running 2 minutes ago 我们可以看到 k8s-master1 上面运行了一个副本，默认配置下 manager node 也是 worker node，所以 swarm-manager 上也运行了副本。如果不希望在 manager 上运行 service，可以执行如下命令：123$ docker node update --availability drain k8s-master1$ docker node update --availability drain k8s-master2$ docker node update --availability drain k8s-master3 在此查看swarm node状态：12345678# 发现AVAILABILITY为Drain状态，表示该节点不接受service的任务分配$ docker node lsID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSIONtlviocryph6mlw8yqp2m1nmv9 * k8s-master1 Ready Drain Leader 18.09.6cgacdrgz7g80bdujq6x66ze7v k8s-master2 Ready Drain Reachable 18.09.6n40j3686zd4f5w0vrfjb0p8be k8s-master3 Ready Drain Reachable 18.09.6odkkwwrfdowu2szjxo5yejuex k8s-node1 Ready Active 18.09.6uokltoj11sdw9ffov7c1vb1lp k8s-node2 Ready Active 18.09.6 经过上面的配置后，我们在查看一下service任务的分配情况：123456$ docker service ps httpdID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTSu724moiz30wk httpd.1 nginx:latest k8s-node2 Running Running 1 minutes ago p3p9ld1k3xq9 \_ httpd.1 nginx:latest k8s-master1 Shutdown Shutdown 1 minutes ago 2ojzjk66n986 httpd.2 nginx:latest k8s-node1 Running Running 5 minutes ago 41pkpuzqq6rs httpd.3 nginx:latest k8s-node2 Running Running 5 minutes ago 我们可以看到 k8s-master1 上面的副本已经转移到k8s-node2上了。 前面已经对副本数量进行了增加，那么减少也是一样。执行：123456$ docker service scale httpd=2web_server scaled to 2overall progress: 2 out of 2 tasks 1/2: running [==================================================&gt;] 2/2: running [==================================================&gt;]verify: Service converged 查看减少副本数service的任务状态：12345$ docker service ps httpdID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTSu724moiz30wk httpd.1 nginx:latest k8s-node2 Running Running 3 minutes ago p3p9ld1k3xq9 \_ httpd.1 nginx:latest k8s-master1 Shutdown Shutdown 3 minutes ago 2ojzjk66n986 httpd.2 nginx:latest k8s-node1 Running Running 7 minutes ago 我们可以看到目前 k8s-node1和k8s-node2 上面各运行了一个副本。NAME为httpd.3的已经删除了。 故障转移故障是在所难免的，容器可能崩溃，Docker Host 可能宕机，不过幸运的是，Swarm 已经内置了 failover 策略。创建 service 的时候，我们没有告诉 swarm 发生故障时该如何处理，只是说明了我们期望的状态（比如运行3个副本），swarm 会尽最大的努力达成这个期望状态，无论发生什么状况。以前面部署的 Service 为例，当前 2 个副本分布在 k8s-node1 和 k8s-node2 上，现在我们测试 swarm 的 failover 特性，关闭 k8s-node1。1234567$ docker node lsID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSIONtlviocryph6mlw8yqp2m1nmv9 * k8s-master1 Ready Drain Reachable 18.09.6cgacdrgz7g80bdujq6x66ze7v k8s-master2 Ready Drain Reachable 18.09.6n40j3686zd4f5w0vrfjb0p8be k8s-master3 Ready Drain Leader 18.09.6odkkwwrfdowu2szjxo5yejuex k8s-node1 Down Active 18.09.6uokltoj11sdw9ffov7c1vb1lp k8s-node2 Ready Active 18.09.6 Swarm 会将 k8s-node1 上的副本调度到其他可用节点。我们可以通过 docker service ps httpd查看。123456$ docker service ps httpdID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTSu724moiz30wk httpd.1 nginx:latest k8s-node2 Running Running 1 hours ago p3p9ld1k3xq9 \_ httpd.1 nginx:latest k8s-master1 Shutdown Shutdown about an hour ago ip79r8o6423n httpd.2 nginx:latest k8s-node2 Running Running 51 seconds ago 2ojzjk66n986 \_ httpd.2 nginx:latest k8s-node1 Shutdown Running about a minute ago 可以看到，副本已经从 k8s-node1 迁移到了k8s-node2，之前运行在故障节点 k8s-node1 上的副本状态被标记为 Shutdown。 访问Service前面我们已经学习了如何部署 service，也验证了 swarm 的 failover 特性。不过截止到现在，有一个重要问题还没有涉及：如何访问 service？通过上面的配置我们运行了一个httpd的服务，服务副本数有2个，都运行在k8s-node2的worker节点上。为了方便演示，我们将其scale副本数扩大到5个。1234567891011121314151617181920212223$ docker service scale httpd=5httpd scaled to 5overall progress: 5 out of 5 tasks 1/5: running [==================================================&gt;] 2/5: running [==================================================&gt;] 3/5: running [==================================================&gt;] 4/5: running [==================================================&gt;] 5/5: running [==================================================&gt;] verify: Service converged #$ docker service lsID NAME MODE REPLICAS IMAGE PORTS5vk0p382fudc httpd replicated 5/5 nginx:latest #$ docker service ps httpdID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTSu724moiz30wk httpd.1 nginx:latest k8s-node2 Running Running 3 hours ago p3p9ld1k3xq9 \_ httpd.1 nginx:latest k8s-master1 Shutdown Shutdown about an hour ago ip79r8o6423n httpd.2 nginx:latest k8s-node2 Running Running 10 minutes ago 2ojzjk66n986 \_ httpd.2 nginx:latest k8s-node1 Shutdown Shutdown 3 minutes ago glfr7knev7rc httpd.3 nginx:latest k8s-node1 Running Running 15 seconds ago qrpl6tcv0dud httpd.4 nginx:latest k8s-node1 Running Running 15 seconds ago 3vnh68jq0a34 httpd.5 nginx:latest k8s-node2 Running Running 15 seconds ago 要访问 http 服务，最起码网络得通吧，服务的 IP 我们得知道吧，但这些信息目前我们都不清楚。不过至少我们知道每个副本都是一个运行的容器，要不先看看容器的网络配置吧。1234$ docker-machine ssh k8s-node1 docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES0f1eab42a2e3 nginx:latest "nginx -g 'daemon of…" 3 minutes ago Up 3 minutes 80/tcp httpd.4.qrpl6tcv0dudfmt3tj1n8pxta5852b6a01d0f nginx:latest "nginx -g 'daemon of…" 3 minutes ago Up 3 minutes 80/tcp httpd.3.glfr7knev7rcj5dpjnztd1pp1 在 k8s-node1 上运行了两个容器，是 httpd 的副本，容器监听了 80 端口，但并没有映射到 Docker Host，所以只能通过容器的 IP 访问。查看一下容器的 IP。12$ docker-machine ssh k8s-node1 docker inspect -f &#123;&#123;.NetworkSettings.IPAddress&#125;&#125; 0f1eab42a2e3172.17.0.3 那么此时我们可以在k8s-node1节点上访问该地址是没问题的。12$ wget -O - -q 172.17.0.3&lt;h1&gt;Welcome to nginx!&lt;/h1&gt; 但这样的访问也仅仅是容器层面的访问，服务并没有暴露给外部网络，只能在 Docker 主机上访问。换句话说，当前配置下，我们无法访问 service httpd。 从外部访问 service要将 service 暴露到外部，方法其实很简单，执行下面的命令：123456789$ docker service update --publish-add 8080:80 httpdhttpdoverall progress: 5 out of 5 tasks 1/5: running [==================================================&gt;] 2/5: running [==================================================&gt;] 3/5: running [==================================================&gt;] 4/5: running [==================================================&gt;] 5/5: running [==================================================&gt;] verify: Service converged 如果是新建 service，可以直接用使用 –publish 参数，比如：docker service create –name httpd –publish 8080:80 –replicas=2 nginx 容器在 80 端口上监听 http 请求，–publish-add 8080:80 将容器的 80 映射到主机的 8080 端口，这样外部网络就能访问到 service 了。当执行完docker service update --publish-add 8080:80 httpd命令后，现有已运行的容器全部会停止，并重启启动指定副本数的容器。此时我们可以netstat查看8080端口是否正常，以及对应的docker host通过8080能否访问容器web。答案是：可以的！1234567891011121314$ docker service ps httpdID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTSnlfur26tb52u httpd.1 nginx:latest k8s-node1 Running Running 4 minutes ago u724moiz30wk \_ httpd.1 nginx:latest k8s-node2 Shutdown Shutdown 4 minutes ago p3p9ld1k3xq9 \_ httpd.1 nginx:latest k8s-master1 Shutdown Shutdown 2 hours ago opdevklkud0z httpd.2 nginx:latest k8s-node2 Running Running 4 minutes ago ip79r8o6423n \_ httpd.2 nginx:latest k8s-node2 Shutdown Shutdown 4 minutes ago 2ojzjk66n986 \_ httpd.2 nginx:latest k8s-node1 Shutdown Shutdown 20 minutes ago 7hnckjv791yp httpd.3 nginx:latest k8s-node2 Running Running 4 minutes ago glfr7knev7rc \_ httpd.3 nginx:latest k8s-node1 Shutdown Shutdown 4 minutes ago k03wuojgarr3 httpd.4 nginx:latest k8s-node1 Running Running 4 minutes ago qrpl6tcv0dud \_ httpd.4 nginx:latest k8s-node1 Shutdown Shutdown 4 minutes ago zjriixku92t4 httpd.5 nginx:latest k8s-node2 Running Running 4 minutes ago 3vnh68jq0a34 \_ httpd.5 nginx:latest k8s-node1 Shutdown Shutdown 4 minutes ago 123456789101112131415# 访问httpd service$ curl 192.168.20.210:8080&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;$ curl 192.168.20.211:8080&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;$ curl 192.168.20.212:8080&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;$ curl 192.168.20.213:8080&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;$ curl 192.168.20.214:8080&lt;h1&gt;Welcome to nginx!&lt;/h1&gt; 其实为什么 curl 集群中任何一个节点的 8080 端口，都能够访问到 web_server？这实际上就是使用 swarm 的好处了，这个功能叫做 routing mesh。我们可以在任意主机上查看docker网络情况！1234567$ docker network lsNETWORK ID NAME DRIVER SCOPEed5feb91f2b5 bridge bridge localaf66f21b53bd docker_gwbridge bridge local8dd3b514208e host host local5bvz86l4evlq ingress overlay swarm918b30962f2a none null local 很明显容器现在有两块网卡，每块网卡连接不同的 Docker 网络。 eth0 连接的是一个 overlay 类型的网络，名字为 ingress，其作用是让运行在不同主机上的容器可以相互通信。 eth1 连接的是一个 bridge 类型的网络，名字为 docker_gwbridge，其作用是让容器能够访问到外网。 ingress 网络是 swarm 创建时 Docker 为自动我们创建的，swarm 中的每个 node 都能使用 ingress。通过 overlay 网络，主机与容器、容器与容器之间可以相互访问；同时，routing mesh 将外部请求路由到不同主机的容器，从而实现了外部网络对 service 的访问。 Shipyard图形化管理工具Shipyard介绍Shipyard构建于Docker Swarm之上，能够以web界面可视化管理Docker资源，包括容器，镜像，私有仓库等且为其提供基于身份验证和角色的访问控制。了解更多请访问Shipyard官网 安装Shipyard提供两种安装方式： 自动安装 手动安装 自动安装1$ curl -sSL https://shipyard-project.com/deploy | bash -s 一旦部署成功，脚本将显示要访问的URL以及帐号密码信息。 手动部署datastoreshipyard使用RethinkDB作为数据存储区。首先，我们将启动一个RethinkDB容器。123456$ docker run \ -ti \ -d \ --restart=always \ --name shipyard-rethinkdb \ rethinkdb Discovery要启用Swarm leader选举，我们必须使用Swarm容器中的外部键值存储。 我们这使用etcd，也可以使用其他键值对存储工具。12345678$ docker run \ -ti \ -d \ -p 4001:4001 \ -p 7001:7001 \ --restart=always \ --name shipyard-discovery \ microbox/etcd -name discovery Proxy认情况下，Docker引擎只监听套接字。我们可以重新配置引擎以使用TLS，或者您可以使用代理容器。这是一个非常轻量级的容器，它只是将请求从TCP转发到Docker侦听的Unix套接字。当然如果你部署docker时已经让docker支持了tcp连接，则不需要这样做。12345678910$ docker run \ -ti \ -d \ -p 2375:2375 \ --hostname=$HOSTNAME \ --restart=always \ --name shipyard-proxy \ -v /var/run/docker.sock:/var/run/docker.sock \ -e PORT=2375 \ shipyard/docker-proxy:latest Swarm Manager运行配置为管理的Swarm容器1234567$ docker run \ -ti \ -d \ --restart=always \ --name shipyard-swarm-manager \ swarm:latest \ manage --host tcp://0.0.0.0:3375 etcd://&lt;IP-OF-HOST&gt;:4001 Controller运行shipyard控制器1234567891011$ docker run \ -ti \ -d \ --restart=always \ --name shipyard-controller \ --link shipyard-rethinkdb:rethinkdb \ --link shipyard-swarm-manager:swarm \ -p 8080:8080 \ shipyard/shipyard:latest \ server \ -d tcp://swarm:3375 当以上容器启动完成后，您应该可以通过http://[ip-of-host]:8080登录shipyard。默认用户名：admin，密码：shipyard 以下附上几张shipyard的截图，应该很好理解其中各个选项的意思。]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ansible playbook详解]]></title>
    <url>%2F2018%2F03%2F23%2Fansible-playbook.html</url>
    <content type="text"><![CDATA[Playbook简介Playbooks与Ad-Hoc相比，是一种完全不同的运用Ansible的方式，而且是非常之强大的；也是系统ansible命令的集合，其利用yaml语法编写，运行过程，ansbile-playbook命令根据自上而下的顺序依次执行任务。playbook 由一个或多个 ‘plays’ 组成.它的内容是一个以 ‘plays’为元素的列表，在 play 之中,一组机器被映射为定义好的角色.在 ansible 中,play 的内容,被称为 tasks,即任务.在基本层次的应用中,一个任务是一个对 ansible模块的调用。当第一个任务依次在所有主机上执行完毕后，开始执行第二个任务。如果某个主机执行时发生错误，则所有操作将会回滚。 Playbook基础组件 hosts：运行执行任务（task）的目标主机 remote_user：在远程主机上执行任务的用户 tasks：任务列表 handlers：任务，与tasks不同的是只有在接受到通知时才会被触发 templates：使用模板语言的文本文件，使用jinja2语法。 variables：变量，变量替换 tag：标签，为某tasks指定标签，运行该标签可以即运行特定的tasks，定义为always的tag总会执行 when: 条件判断，当条件成立则执行tasks，不成立不执行表达式 判断表达式如：not or and != = with_items：循环迭代需要重复执行的任务列表，用{item}}引用列表值 例如一个简单的playbook文件：12345678910111213141516171819202122232425--- - hosts: test # 指定运行任务的主机组 remote_user: root # 指定远程执行任务的用户 vars: # 指定变量 - bsh: b.sh - httprpm: httpd task: # 任务的开始 - name: install httpd # 一个安装httpd的任务 yum: name=&#123;&#123; httprpm &#125;&#125; state=present # ansible的yum模块 tags: install_httpd # 为该任务打一个install_httpd的标签 - name: copy b.sh # 又一个复制b.sh脚本的任务 copy: src=/root/&#123;&#123; bsh &#125;&#125; dest=/root/ owner=ala group=ala mode=0644 notify: # 如果copy的文件内容发生改变就会触发 - reload httpd # 指定通知的哪个handlers when: ansible_distribution == "CentOS" # 通过变量判断系统为CentOS时才执行该copy任务 - name: copy b.sh copy: src=/root/&#123;&#123; bsh &#125;&#125; dest=/opt/ owner=ala group=ala mode=0644 notify: - reload httpd when: ansible_distribution == "Ubuntu" # 通过变量判断系统为Ubuntu时才执行该copy任务 - name: start httpd # 又一个启动httpd服务的任务 service: name=httpd state=started enabled=yes handlers: # 满足触发条件则执行的任务 - name: reload httpd # 满足触发条件的任务名 service: name=httpd state=reloaded # 该任务为重新加载一下httpd服务 其中的ansible_distribution是ansible收集的facts变量。 playbook定义变量 ansible 常见定义变量有以下 6 种 /etc/ansible/hosts文件主机中定义 /etc/ansible/hosts/文件主机组中定义 playbook的yaml文件中通过vars定义 获取系统变量，也称facts变量 分文件定义主机和主机组的变量 playbook 的role中定义 123456789101112131415161718192021222324252627282930313233343536373839# /etc/ansible/hosts文件主机中定义 主机变量192.168.200.136 http_port=808 maxRequestsPerChild=808192.168.200.137 http_port=8080 maxRequestsPerChild=909 # /etc/ansible/hosts文件主机中定义 主机组变量[websers]192.168.200.136192.168.200.137 [websers:vars] ntp_server=ntp.exampl.comproxy=proxy.exampl.com # ntp_server和proxy变量可为websers组中主机使用# playbook的yaml文件中通过vars定义 --- - hosts: test remote_user: root vars: # 定义bsh和httprpm的两个变量 - bsh: b.sh - httprpm: httpd# 获取系统facts变量ansible 192.168.200.136 -m setup # 可以获取主机所有的facts变量，通过&#123;&#123;variable_name&#125;&#125;引用# 分文件定义主机和主机组的变量/etc/ansible/group_vars/websers # 定义主机组名为websers的变量文件/etc/ansible/host_vars/hostpc # 定义主机名为hostpc的变量文件$ cat /etc/ansible/host_vars/hostpc # 变量文件内容格式如下---ntp_server: acme.example.orgdatabase_server: storage.example.org# role中定义 （目录结构下文讲述）$ cat /etc/ansible/roles/nginx/vars/main.yml---nginx_port: 80nginx_domain: www.abc.comnginx_user: nginx playbook role目录结构Roles简介Ansible为了层次化、结构化地组织Playbook，使用了角色（roles）。Roles能够根据层次型结构自动装载变量文件、task及handlers等。简单来讲，roles就是通过分别将变量、文件、任务、模块及处理器放置于单独的目录中，并可以便捷地include它们，roles一般用于基于主机构建服务的场景中，但也可以用于构建守护进程等场景中。 创建Roles创建roles时一般需要以下步骤：首先创建以roles命名的目录。然后在roles目标下分别创建以这个角色名称命令的目录，如websevers等，然后在每个角色命令的目录中分别创建files、handlers、tasks、templates、meta、defaults和vars目录，用不到的目录可以创建为空目录。最后在Playbook文件中调用各角色进行使用。 roles内各目录含义解释 files：用来存放由copy模块或script模块调用的文件。 templates：用来存放jinjia2模板，template模块会自动在此目录中寻找jinjia2模板文件。 tasks：此目录应当包含一个main.yml文件，用于定义此角色的任务列表，此文件可以使用include包含其它的位于此目录的task文件。 handlers：此目录应当包含一个main.yml文件，用于定义此角色中触发条件时执行的动作。 vars：此目录应当包含一个main.yml文件，用于定义此角色用到的变量。 defaults：此目录应当包含一个main.yml文件，用于为当前角色设定默认变量。 meta：此目录应当包含一个main.yml文件，用于定义此角色的特殊设定及其依赖关系。 如下定义了一个nginx服务的playbook目录结构树。1234567891011121314151617[root@k8s-master1 roles]# tree /etc/ansible/roles/nginx/etc/ansible/roles/nginx├── defaults│ └── main.yml├── files│ └── nginx.tar.gz├── handlers│ └── main.yml├── meta│ └── main.yml├── tasks│ └── main.yml├── templates│ └── nginx.conf.j2│ └── default.conf.j2└── vars └── main.yml 使用roles安装nginx案例实例环境 主机名 IP 系统 角色 ansible 192.168.20.210 centos7.5 ansible控制器 web-nginx 192.168.20.213 centos7.5 web服务器 hosts主机组清单123$ cat /etc/ansilbe/hosts # ansible hosts主机组配置 [web-node] 192.168.20.213 生成ssh密钥1234567891011121314151617181920212223242526272829# 在ansible控制机上生成ssh密钥对 $ ssh-keygen -t rsaGenerating public/private rsa key pair.Enter file in which to save the key (/root/.ssh/id_rsa): Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /root/.ssh/id_rsa.Your public key has been saved in /root/.ssh/id_rsa.pub.The key fingerprint is:SHA256:m4+23Eh+dJ8r/9zSjpUbHJECh1iFQU8Z0QMlygrRm98 root@k8s-node1The key's randomart image is:+---[RSA 2048]----+| .. +==OB. || .o.o*..o.|| . oo o o.|| .o. . .|| S.. . . || o...E. o|| +. . . *.|| +.=. . ++=|| .*oo o+*=|+----[SHA256]-----+# 复制ansible控制机上的公钥到nginx web服务器$ ssh-copy-id -i ~/.ssh/id_rsa.pub 192.168.20.213这里是root用户验证，输入完root密码后，就可以在ansible主机上无密码ssh登陆nginx web服务器了。 创建nginx的roles结构目录1$ mkdir -pv /etc/ansible/roles/nginx/&#123;defaults,files,handlers,meta,tasks,templates,vars&#125; 准备安装包和依赖包12345cd /etc/ansible/roles/nginx/files$ wget http://nginx.org/download/nginx-1.16.0.tar.gz$ wget https://www.openssl.org/source/openssl-1.0.2r.tar.gz$ wget ftp://ftp.csx.cam.ac.uk/pub/software/programming/pcre/pcre-8.42.tar.gz$ wget http://www.zlib.net/zlib-1.2.11.tar.gz 准备安装脚本 该脚本已在手动安装测试过，建议各位在通过ansible playbook安装服务器前，先手动安装一遍确保无误。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081$ cat /etc/ansible/roles/nginx/files/install_nginx.sh#!/bin/sh#info: source code install nginx 1.16.0#author: pyker &lt;pyker@qq.com&gt;export PATH=`echo $PATH`PWD="/opt"NGINX=nginx-1.16.0OPENSSL=openssl-1.0.2rPCRE=pcre-8.42ZLIB=zlib-1.2.11RUN_USER=nginxid -u $&#123;RUN_USER&#125; &gt;/dev/null 2&gt;&amp;1[ $? -ne 0 ] &amp;&amp; useradd -M -s /sbin/nologin $&#123;RUN_USER&#125;mkdir -p /var/log/nginxyum install -y epel-release yum install -y jemalloc jemalloc-develyum install -y gcc \ gcc-c++ \ gcc++ \ perl \ perl-devel \ perl-ExtUtils-Embed \ libxslt \ libxslt-devel \ libxml2 \ libxml2-devel \ gd \ gd-devel \ GeoIP \ GeoIP-develcd $PWDfor tar in `ls *.tar.gz`; do tar zxf $tardonecd $PWD/$NGINX &amp;&amp; ./configure --prefix=/usr/local/nginx \ --sbin-path=/usr/sbin/nginx \ --user=nginx \ --group=nginx \ --pid-path=/var/run/nginx.pid \ --lock-path=/var/run/nginx.lock \ --error-log-path=/var/log/nginx/error.log \ --http-log-path=/var/log/nginx/access.log \ --with-select_module \ --with-poll_module \ --with-threads \ --with-file-aio \ --with-http_ssl_module \ --with-http_v2_module \ --with-http_realip_module \ --with-http_addition_module \ --with-http_xslt_module=dynamic \ --with-http_image_filter_module=dynamic \ --with-http_geoip_module=dynamic \ --with-http_sub_module \ --with-http_dav_module \ --with-http_flv_module \ --with-http_mp4_module \ --with-http_gunzip_module \ --with-http_gzip_static_module \ --with-http_auth_request_module \ --with-http_random_index_module \ --with-http_secure_link_module \ --with-http_degradation_module \ --with-http_slice_module \ --with-http_stub_status_module \ --with-mail=dynamic \ --with-mail_ssl_module \ --with-stream \ --with-stream_ssl_module \ --with-stream_realip_module \ --with-stream_geoip_module=dynamic \ --with-stream_ssl_preread_module \ --with-compat \ --with-ld-opt="-ljemalloc" \ --with-pcre=../$&#123;PCRE&#125; \ --with-pcre-jit \ --with-zlib=../$&#123;ZLIB&#125; \ --with-openssl=../$&#123;OPENSSL&#125;\ --with-openssl-opt=no-nextprotoneg \ --with-debug &amp;&amp; make -j 4 &amp;&amp; make install 配置变量 在安装nginx的时候或者其他服务的时候常常用到变量，下面配置变量是通过roles方式配置。 12345$ cat /etc/ansible/roles/nginx/vas/main.ymlcat nginx/vars/main.yml nginx_user: nginxnginx_port: 80nginx_dir: /usr/local/nginx 准备nginx.conf模版 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859$ cat /etc/ansible/roles/nginx/templates/nginx.conf.j2user &#123;&#123; nginx_user &#125;&#125;;worker_processes &#123;&#123; ansible_processor_vcpus &#125;&#125;;error_log /var/log/nginx/error_nginx.log crit;pid /var/run/nginx.pid;worker_rlimit_nofile 51200;events &#123; use epoll; worker_connections 51200; multi_accept on;&#125;http &#123; map $http_upgrade $connection_upgrade &#123; default upgrade; '' close; &#125; include mime.types; default_type application/octet-stream; server_names_hash_bucket_size 128; client_header_buffer_size 4k; large_client_header_buffers 4 4k; client_max_body_size 100m; client_body_buffer_size 10m; sendfile on; tcp_nopush on; keepalive_timeout 120; server_tokens off; tcp_nodelay on; #Gzip Compression gzip on; gzip_buffers 16 8k; gzip_comp_level 6; gzip_http_version 1.1; gzip_min_length 1k; gzip_proxied any; gzip_vary on; gzip_types text/xml application/xml application/atom+xml application/rss+xml application/xhtml+xml image/svg+xml text/javascript application/javascript application/x-javascript text/x-json application/json application/x-web-app-manifest+json text/css text/plain text/x-component font/opentype application/x-font-ttf application/vnd.ms-fontobject image/x-icon; gzip_disable "MSIE [1-6]\.(?!.*SV1)"; #If you have a lot of static files to serve through Nginx then caching of the files' metadata (not the actual files' contents) can save some latency. open_file_cache max=65535 inactive=20s; open_file_cache_valid 30s; open_file_cache_min_uses 1; open_file_cache_errors on;########################## vhost ############################# include upstream/*.conf; include vhost/*.conf;&#125; nginx.conf.j2文件中nginx_user和ansible_processor_vcpus分别为用户自定义变量和-m setup获取的facts。 定义默认server模版 1234567891011121314151617 ```bash$ cat /etc/ansible/roles/nginx/templates/default.conf.j2server &#123; listen &#123;&#123; nginx_port &#125;&#125;; server_name &#123;&#123; ansible_all_ipv4_addresses &#125;&#125;; index index.html index.htm index.jsp index.do; access_log off; location / &#123; root /data/wwwroot; &#125; location /nginx_status &#123; stub_status on; access_log off; &#125;&#125; default.conf.j2文件中nginx_port和ansible_all_ipv4_addresses分别为用户自定义变量和-m setup获取的facts。 定义一个测试index.html文件12$ cat /etc/ansible/roles/nginx/templates/index.html this is test pages! 定义nginx启动脚本12345678910111213141516$ cat /etc/ansible/roles/nginx/files/nginx.service[Unit]Description=The NGINX HTTP and reverse proxy serverAfter=syslog.target network.target remote-fs.target nss-lookup.target[Service]Type=forkingPIDFile=/var/run/nginx.pidExecStartPre=/usr/sbin/nginx -tExecStart=/usr/sbin/nginxExecReload=/usr/sbin/nginx -s reloadExecStop=/usr/bin/kill -s QUIT $MAINPIDPrivateTmp=true[Install]WantedBy=multi-user.target 定义tasks任务1234567891011121314151617181920# 在tasks目录中定义了一个copy.yaml文件用户复制files目录下的文件到nginx服务器# 定义完成后，需要在tasks/main.yaml文件中用-include包含该copy.yaml$ cat /etc/ansible/roles/nginx/tasks/copy.yaml- name: copy nginx-1.16.0.tar.gz to client copy: src=/etc/ansible/roles/nginx/files/nginx-1.16.0.tar.gz dest=/opt/nginx-1.16.0.tar.gz- name: copy install_nginx.sh to client copy: src=/etc/ansible/roles/nginx/files/install_nginx.sh dest=/opt/install_nginx.sh- name: mkdir nginx data directory file: path=/data/wwwroot state=directory recurse=yes- name: copy index.html copy: src=/etc/ansible/roles/nginx/files/index.html dest=/data/wwwroot/index.html- name: copy nginx systemctl file copy: src=/etc/ansible/roles/nginx/files/nginx.service dest=/usr/lib/systemd/system/nginx.service- name: copy dependency package copy: src=/etc/ansible/roles/nginx/files/&#123;&#123; item &#125;&#125; dest=/opt/&#123;&#123; item &#125;&#125; with_items: - openssl-1.0.2r.tar.gz - pcre-8.42.tar.gz - zlib-1.2.11.tar.gz 123456789101112131415161718# 定义main.yaml文件$ cat /etc/ansible/roles/nginx/tasks/main.yml ---- include: copy.yml # 使用include将上面copy.yaml文件包含进来- name: install nginx shell: /usr/bin/sh /opt/install_nginx.sh- name: replace nginx.conf file template: src=/etc/ansible/roles/nginx/templates/nginx.conf.j2 dest=&#123;&#123; nginx_dir &#125;&#125;/conf/nginx.conf- name: mkdir nginx vhost directory file: path=&#123;&#123;nginx_dir&#125;&#125;/conf/vhost state=directory- name: relpace default.conf file template: src=/etc/ansible/roles/nginx/templates/default.conf.j2 dest=&#123;&#123;nginx_dir&#125;&#125;/conf/vhost/default.conf tags: ngxdef # 定义一个tag，当default.conf发生改变时可以指定运行该任务 notify: - reload nginx # 该名称需要和handler中定义的一样- name: start nginx command: /usr/sbin/nginx # 由于配置了nginx启动服务，也可以使用 shell: systemctl start nginx 定义触发通知handlers1234$ cat /etc/ansible/roles/nginx/handlers/main.yml ---- name: reload nginx shell: /usr/sbin/nginx -t; /usr/sbin/nginx -s reload # 也可以使用 shell: systemctl reload nginx 定义role入口文件123456$ cat /etc/ansible/roles/nginx.yml ---- hosts: web-node # 定义要执行该playbook的主机组 remote_user: root # 定义远程执行命令的用户 roles: # - nginx # 主机组中的主机使用哪个playbook剧本 查看当前nginx roles目录树结构123456789101112131415161718192021222324$ tree /etc/ansible/roles//etc/ansible/roles/├── nginx│ ├── defaults│ ├── files│ │ ├── index.html│ │ ├── nginx.service│ │ ├── install_nginx.sh│ │ ├── nginx-1.16.0.tar.gz│ │ ├── openssl-1.0.2r.tar.gz│ │ ├── pcre-8.42.tar.gz│ │ └── zlib-1.2.11.tar.gz│ ├── handlers│ │ └── main.yml│ ├── meta│ ├── tasks│ │ ├── copy.yml│ │ └── main.yml│ ├── templates│ │ ├── default.conf.j2│ │ └── nginx.conf.j2│ └── vars│ └── main.yml└── nginx.yml playbook语法检测通过上面的配置，我们已经完成了使用playbook方式安装nginx所需要的步骤，现在我们应该在执行该playbook前检查一下上述语法有没有错误。123$ ansible-playbook --syntax-check /etc/ansible/roles/nginx.yml playbook: /etc/ansible/roles/nginx.yml # 如果语法没问题，将直接显示文件名称，如果有错误将告知你那里错了 测试安装ansible-playbook -C 命令可以测试运行playbook剧本，而非真正在远端服务器上执行，这样可以方便查看playbook在执行过程中将会做哪些事情。1234567891011121314151617181920212223242526272829303132333435363738394041424344$ ansible-playbook -C nginx.yml PLAY [web-node] ************************************************************************************************************************************************TASK [nginx : copy nginx-1.16.0.tar.gz to client] **************************************************************************************************************changed: [192.168.20.213]TASK [nginx : copy install_nginx.sh to client] *****************************************************************************************************************changed: [192.168.20.213]TASK [nginx : mkdir nginx data directory] **********************************************************************************************************************changed: [192.168.20.213]TASK [nginx : copy index.html] *********************************************************************************************************************************changed: [192.168.20.213]TASK [nginx : copy nginx systemctl file] ***********************************************************************************************************************changed: [192.168.20.213]TASK [nginx : copy dependency package] *************************************************************************************************************************changed: [192.168.20.213] =&gt; (item=openssl-1.0.2r.tar.gz)changed: [192.168.20.213] =&gt; (item=pcre-8.42.tar.gz)changed: [192.168.20.213] =&gt; (item=zlib-1.2.11.tar.gz)TASK [nginx : install nginx] ***********************************************************************************************************************************skipping: [192.168.20.213]TASK [nginx : replace nginx.conf file] *************************************************************************************************************************changed: [192.168.20.213]TASK [nginx : mkdir nginx vhost directory] *********************************************************************************************************************changed: [192.168.20.213]TASK [nginx : relpace default.conf file] ***********************************************************************************************************************changed: [192.168.20.213]TASK [nginx : start nginx] *************************************************************************************************************************************skipping: [192.168.20.213]RUNNING HANDLER [nginx : reload nginx] *************************************************************************************************************************skipping: [192.168.20.213]PLAY RECAP *****************************************************************************************************************************************************192.168.20.213 : ok=8 changed=8 unreachable=0 failed=0 运行nginx playbook1$ ansible-playbook nginx.yml 执行ansible-playbook nginx.yml命令后，ansible将会按照刚刚测试安装的步骤在远端进行安装nginx服务并且启动。 验证安装结果等待一会nginx playbook将会在远端服务器上安装完毕，现在我们来验证一下结果：1234567891011121314151617181920212223# 在web服务器上查看nginx端口$ netstat -ptln | grep 80tcp 0 0 0.0.0.0:80 0.0.0.0:* LISTEN 14461/nginx: master# 查看nginx进程$ ps -ef | grep nginxroot 14461 1 0 00:39 ? 00:00:00 nginx: master process /usr/sbin/nginxnginx 14474 14461 0 00:39 ? 00:00:00 nginx: worker processnginx 14475 14461 0 00:39 ? 00:00:00 nginx: worker processnginx 14476 14461 0 00:39 ? 00:00:00 nginx: worker processnginx 14477 14461 0 00:39 ? 00:00:00 nginx: worker processroot 39501 1664 0 11:38 pts/0 00:00:00 grep --color=auto nginx# 让我们在内网任意一台电脑上来访问一下该nginx页面$ curl http://192.168.20.213this is a test pages! # 结果是我们前面定义的index.html内容# 访问一下nginx_status的uri$ curl http://192.168.20.213/nginx_status #前面default.conf.j2文件中定义的nginx locationActive connections: 1 server accepts handled requests 8 8 10 Reading: 0 Writing: 1 Waiting: 0 至此，一个使用ansible-playbook安装nginx服务已完成。]]></content>
      <categories>
        <category>CI/CD</category>
      </categories>
      <tags>
        <tag>ansible</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker三剑客之docker-compose用法]]></title>
    <url>%2F2018%2F03%2F22%2Fdocker-compose.html</url>
    <content type="text"><![CDATA[docker-compose简介docker-compose项目是Docker官方的开源项目，负责实现对Docker容器集群的快速编排。它是一个定义和运行多容器的docker应用工具。使用docker-compose，你能通过YMAL文件配置你自己的服务，然后通过一个命令，你能使用配置文件创建和运行所有的服务。docker-compose的定位是 “ 定义和运行多个Docker容器应用的工具 ”，docker-compose开源代码地址：https://github.com/docker/compose 。 我们知道在Docker中构建自定义的镜像是通过使用Dockerfile模板文件来实现的，从而可以让用户很方便定义一个单独的应用容器。而docker-compose使用的模板文件就是一个YAML格式文件，它允许用户通过一个docker-compose.yml来定义一组相关联的应用容器为一个项目(project)。 Compose 中有两个重要的概念： 服务(service): 一个应用的容器，实际上可以包括若干运行相同镜像的容器实例 项目(project): 由一组关联的应用容器组成的一个完整业务单元，在docker-compose.yml中定义 以上可以理解为：服务（service）就是在定义应用需要的一些服务，代表配置文件中的每一项服务。每个服务都有自己的名字、使用的镜像、挂载的数据卷、所属的网络、依赖哪些其他服务等等，即以容器为粒度，用户需要Compose所完成的任务。 项目（project）代表用户需要完成的一个项目，即是Compose的一个配置文件可以解析为一个项目，即Compose通过分析指定配置文件，得出配置文件所需完成的所有容器管理与部署操作。 Compose的默认管理对象时项目，通过子命令对项目中的一组容器进行便捷地生命周期管理。 docker-compose安装二进制安装：12$ curl -L https://github.com/docker/compose/releases/download/1.24.0/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-compose$ chmod +x /usr/local/bin/docker-compose 更多系统环境安装请参考docker-compose仓库。 docker-compose命令在我们使用Compose前，可以通过执行docker-compose --help来查看Compose基本命令用法。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556$ docker-compose --helpDefine and run multi-container applications with Docker.使用Docker定义和运行多容器应用程序Usage: docker-compose [-f &lt;arg&gt;...] [options] [COMMAND] [ARGS...] docker-compose -h|--helpOptions: -f, --file FILE 指定模板文件，默认是docker-compose.yml模板文件,可以多次指定 -p, --project-name NAME 指定项目名称，默认使用所在目录名称作为项目名称 --verbose 输入更多的调试信息 --log-level LEVEL 设置日志等级，有 (DEBUG, INFO, WARNING, ERROR, CRITICAL) --no-ansi 不打印ANSI控制字符 -v, --version 打印docker-compose版本 -H, --host HOST 要连接到的守护进程套接字 --tls Use TLS; implied by --tlsverify --tlscacert CA_PATH Trust certs signed only by this CA --tlscert CLIENT_CERT_PATH Path to TLS certificate file --tlskey TLS_KEY_PATH Path to TLS key file --tlsverify Use TLS and verify the remote --skip-hostname-check Don't check the daemon's hostname against the name specified in the client certificate --project-directory PATH Specify an alternate working directory (default: the path of the Compose file) --compatibility If set, Compose will attempt to convert keys in v3 files to their non-Swarm equivalentCommands: build Build or rebuild services (构建或重建项目中的服务容器) bundle Generate a Docker bundle from the Compose file (从Compose文件生成分布式应用程序包) config Validate and view the Compose file (验证并查看docker-compose.yml文件) create Create services (为服务创建容器) down Stop and remove containers, networks, images, and volumes (停止容器并删除由其创建的容器，网络，卷和镜像) events Receive real time events from containers (为项目中的每个容器流式传输容器事件) exec Execute a command in a running container (这相当于docker exec。在服务中运行任意命令) help Get help on a command (获得一个命令的帮助) images List images (显示项目中当前所有镜像) kill Kill containers (通过发送SIGKILL信号来强制停止服务容器) logs View output from containers (查看服务容器的输出) pause Pause services (暂停一个容器) port Print the public port for a port binding (打印某个容器端口所映射的公共端口) ps List containers (列出项目中目前所有的容器) pull Pull service images (拉取服务依赖镜像) push Push service images (推送服务镜像) restart Restart services (重启项目中的服务) rm Remove stopped containers (删除所有停止状态的服务容器) run Run a one-off command (在指定服务上执行一个命令，会生成一个退出的容器，docker ps -a查看) scale Set number of containers for a service (设置指定服务执行的容器个数，只在swarm集群有效) start Start services (启动已存在的服务容器) stop Stop services (停止正在运行的服务容器) top Display the running processes (显示容器正在运行的进程) unpause Unpause services (恢复处于暂停状态的容器) up Create and start containers (自动完成包括构建镜像、创建服务、启动服务并关联服务相关容器的一系列操作) version Show the Docker-Compose version information (显示compose及依赖的环境当前版本) 更多命令的子命令可以使用docker-compose command --help格式查看。也可以查看官方说明 这里主要说一下docker-compose run指令：1$ docker-compose run ubuntu ping www.baidu.com 将会新启动一个ubuntu容器，并执行ping www.baidu.com 命令。默认情况下，如果该服务存在关联，则所有关联的服务将会自动被启动，除非这些服务已经在运行中或者指定--no-deps参数。该命令类似于启动容器后运行指定的命令，相关卷、链接等都会按照配置自动创建。有两个不同点： 给定命令将会覆盖原有的自动运行命令 不会自动创建端口，以避免冲突 12345678910111213141516171819202122232425262728293031323334$ docker-compose run --helpRun a one-off command on a service.For example: $ docker-compose run web python manage.py shellBy default, linked services will be started, unless they are alreadyrunning. If you do not want to start linked services, use`docker-compose run --no-deps SERVICE COMMAND [ARGS...]`.Usage: run [options] [-v VOLUME...] [-p PORT...] [-e KEY=VAL...] [-l KEY=VALUE...] SERVICE [COMMAND] [ARGS...]Options: -d, --detach Detached mode: Run container in the background, print new container name. (在后台运行服务容器) --name NAME Assign a name to the container (为容器指定一个名字) --entrypoint CMD Override the entrypoint of the image. (覆盖默认的容器启动指令) -e KEY=VAL Set an environment variable (can be used multiple times) (设置环境变量值，可多次使用选项来设置多个环境变量) -l, --label KEY=VAL Add or override a label (can be used multiple times) (添加或覆盖标签可以多次使用) -u, --user="" Run as specified username or uid (指定运行容器的用户名或者uid) --no-deps Don't start linked services. (不自动启动管理的服务容器) --rm Remove container after run. Ignored in detached mode. (运行命令后自动删除容器，d模式下将忽略) -p, --publish=[] Publish a container's port(s) to the host (映射容器端口到本地主机) --service-ports Run command with the service's ports enabled and mapped to the host. (配置服务端口并映射到本地主机) --use-aliases Use the service's network aliases in the network(s) the container connects to. (在容器连接到的网络中使用服务的网络别名) -v, --volume=[] Bind mount a volume (default []) (绑定一个数据卷，默认为空) -T Disable pseudo-tty allocation. By default `docker-compose run` allocates a TTY. (不分配伪tty，意味着依赖tty的指令将无法运行) -w, --workdir="" Working directory inside the container (为容器指定默认工作目录) docker-compose模版文件模板文件是使用Compose的核心，涉及的指令关键字也比较多，大部分指令与docker run相关参数的含义都是类似的。默认的模板文件迷城为docker-compose.yml，格式为YAML格式。 我们这里主要说明的是3.x版本，当然你也可以参考官方配置说明查看更多版本模块文件 首先我们要知道compose模版文件主要分为4个区域，分别是： version compose版本，指定你当前使用的 compose 文件版本 services 服务，在它下面可以定义应用需要的一些服务，每个服务都有自己的名字、使用的镜像、挂载的数据卷、所属的网络、依赖哪些其他服务等等。 volumes 数据卷，在它下面可以定义的数据卷（名字等等），然后挂载到不同的服务下去使用。 networks 应用的网络，在它下面可以定义应用的名字、使用的网络类型等等。 注意：每个服务都必须通过image指令指定镜像或build指令（需要Dockerfile）等来自动构建生成镜像。如果使用build指令，在Dockefile中设置的选项（例如：CMD、EXPOSE、VOLUME、ENV等）将会自动被获取，无需在docker-compose.yml中再次设置。 image指定image的ID，这个image ID可以是本地也可以是远程的，如果本地不存在，compose会尝试pull下来。如：1image: 1.17.0-alpine build服务除了可以基于指定的镜像，还可以基于一份 Dockerfile，在使用 up 启动之时执行构建任务，这个构建标签就是 build，它可以指定 Dockerfile 所在文件夹的路径。Compose 将会利用它自动构建这个镜像，然后使用这个镜像启动服务容器。例如：12345# 绝对路径build: /path/to/build/dir# 相对路径build: ./dir 在swarm模式下使用（版本3）Compose文件部署stack时，将忽略此选项。 docker stack命令仅接受预先构建好的的镜像。 contextcontext 选项可以是 Dockerfile 的文件路径，也可以是到链接到 git 仓库的 url, 当提供的值是相对路径时，它被解析为相对于撰写文件的路径。12build: context: ./dir dockerfile使用此 dockerfile 文件来构建，必须指定构建路径123build: context: ./dir dockerfile: Dockerfile-tomcat args添加构建参数，这些参数是仅在构建过程中可访问的环境变量,它和Dockerfile中ARG效果类似首先， 在Dockerfile中指定参数：12345ARG buildnoARG gitcommithashRUN echo "Build number: $buildno"RUN echo "Based on commit: $gitcommithash" 然后指定 build 下的参数,可以传递映射或列表12345build: context: . args: buildno: 1 gitcommithash: cdc3b19 在Dockerfile中，如果在FROM指令之前指定了ARG，那么在FROM下面的构建指令中就不能使用ARG。如果您需要一个参数在两个地方都可用，也可以在FROM指令下指定它。有关使用细节，请参见ARGS和FROM如何交互。 指定构建参数时可以省略该值，在这种情况下，构建时的值默认构成运行环境中的值123args: - buildno - gitcommithash Tip： YAML 布尔值（true，false，yes，no，on，off）必须使用引号括起来，以为了能够正常被解析为字符串 cache_from编写缓存解析镜像列表12345build: context: . cache_from: - alpine:latest - corp/web_app:3.14 labels使用 Docker标签 将元数据添加到生成的镜像中，可以使用数组或字典。建议使用反向 DNS 标记来防止签名与其他软件所使用的签名冲突, 例如：123456build: context: . labels: com.example.description: "Accounting webapp" com.example.department: "Finance" com.example.label-with-empty-value: "" shm_size设置容器 /dev/shm 分区的大小，值为表示字节的整数值或表示字符的字符串123build: context: . shm_size: '2gb' target根据对应的 Dockerfile 构建指定 Stage, 关于Dockerfile创建多个stage参考官方说明123build: context: . target: prod cap_add, cap_drop添加或删除容器功能，可查看 man 7 capabilities123456cap_add: - ALLcap_drop: - NET_ADMIN - SYS_ADMIN 使用（版本3）Compose文件在swarm群集模式下部署stack时，该选项被忽略。因为 docker stack 命令只接受预先构建的镜像。 cgroup_parent为容器指定可选的父cgroup。1cgroup_parent: m-executor-abcd 使用（版本3）Compose文件在swarm群集模式下部署stack时，该选项被忽略。 command覆盖容器启动后默认执行的命令1command: bundle exec thin -p 3000 该命令也可以是一个列表，方法类似于 dockerfile:1command: ["bundle", "exec", "thin", "-p", "3000"] configs使用服务 configs 配置为每个服务赋予相应的访问权限，支持两种不同的语法。 配置必须存在或在 configs 此堆栈文件的顶层中定义，否则堆栈部署失效 SHORT 语法SHORT 语法只能指定配置名称，这允许容器访问配置并将其安装在 /&lt;config_name&gt; 容器内，源名称和目标装入点都设为配置名称。1234567891011121314version: "3.7"services: redis: image: redis:latest deploy: replicas: 1 configs: - my_config - my_other_configconfigs: my_config: file: ./my_config.txt my_other_config: external: true 以上实例使用 SHORT 语法将 redis 服务访问授予 my_config 和 my_other_config ,并被 my_other_config 定义为外部资源，这意味着它已经在 Docker 中定义。可以通过 docker config create 命令或通过另一个堆栈部署。如果外部部署配置都不存在，则堆栈部署会失败并出现 config not found 错误。 LONG 语法LONG 语法提供了创建服务配置的更加详细的信息 source: Docker 中存在的配置的名称 target: 要在服务的任务中装载的文件的路径或名称。如果未指定则默认为 / uid 和 gid: 在服务的任务容器中拥有安装的配置文件的数字 UID 或 GID。如果未指定，则默认为在Linux上。Windows不支持。 mode: 在服务的任务容器中安装的文件的权限，以八进制表示法。例如，0444 代表文件可读的。默认是 0444。如果配置文件无法写入，是因为它们安装在临时文件系统中，所以如果设置了可写位，它将被忽略。可执行位可以设置。如果您不熟悉 UNIX 文件权限模式，Unix Permissions Calculator 下面示例在容器中将 my_config 名称设置为 redis_config，将模式设置为 0440（group-readable）并将用户和组设置为 103。该 ｀redis 服务无法访问 my_other_config 配置。1234567891011121314151617version: "3.7"services: redis: image: redis:latest deploy: replicas: 1 configs: - source: my_config target: /redis_config uid: '103' gid: '103' mode: 0440configs: my_config: file: ./my_config.txt my_other_config: external: true 可以同时授予多个配置的服务相应的访问权限，也可以混合使用 LONG 和 SHORT 语法。定义配置并不意味着授予服务访问权限。 container_name为自定义的容器指定一个名称，而不是使用默认的名称1container_name: my-web-container 因为 docker 容器名称必须是唯一的，所以如果指定了一个自定义的名称，不能扩展一个服务超过 1 个容器 depends_on此选项解决了启动顺序的问题, 在使用 Compose 时，最大的好处就是少打启动命令，但是一般项目容器启动的顺序是有要求的，如果直接从上到下启动容器，必然会因为容器依赖问题而启动失败。例如在没启动数据库容器的时候启动了应用容器，这时候应用容器会因为找不到数据库而退出，为了避免这种情况我们需要加入一个标签，就是 depends_on，这个标签解决了容器的依赖、启动先后的问题。指定服务之间的依赖关系，有两种效果: docker-compose up 以依赖顺序启动服务，下面例子中 redis 和 db 服务在 web 启动前启动 docker-compose up SERVICE 自动包含 SERVICE 的依赖性，下面例子中，例如下面容器会先启动 redis 和 db 两个服务，最后才启动 web 服务：1234567891011version: "3.7"services: web: build: . depends_on: - db - redis redis: image: redis db: image: postgres 注意的是，默认情况下使用 docker-compose up web 这样的方式启动 web 服务时，也会启动 redis 和 db 两个服务，因为在配置文件中定义了依赖关系 deploy指定与部署和运行服务相关的配置。 这仅在使用docker stack deploy部署到swarm时生效，单机编排或使用docker-compose up和docker-compose run命令将忽略该配置。 1234567891011version: "3.7"services: redis: image: redis:alpine deploy: replicas: 6 update_config: parallelism: 2 delay: 10s restart_policy: condition: on-failure 这里有几个子选项 endpoint_mode指定连接到群组外部客户端服务发现方法 endpoint_mode:vip ：Docker 为该服务分配了一个虚拟 IP(VIP),作为客户端的 “前端“ 部位用于访问网络上的服务。 endpoint_mode: dnsrr : DNS轮询（DNSRR）服务发现不使用单个虚拟 IP。Docker为服务设置 DNS 条目，使得服务名称的 DNS 查询返回一个 IP 地址列表，并且客户端直接连接到其中的一个。如果想使用自己的负载平衡器，或者混合 Windows 和 Linux 应用程序，则 DNS 轮询调度（round-robin）功能就非常实用。123456789101112131415161718192021222324252627282930version: "3.7"services: wordpress: image: wordpress ports: - "8080:80" networks: - overlay deploy: mode: replicated replicas: 2 endpoint_mode: vip mysql: image: mysql volumes: - db-data:/var/lib/mysql/data networks: - overlay deploy: mode: replicated replicas: 2 endpoint_mode: dnsrrvolumes: db-data:networks: overlay: labels指定服务的标签，这些标签仅在服务上设置。1234567version: "3.7"services: web: image: web deploy: labels: com.example.description: "This label will appear on the web service" 通过将 deploy 外面的 labels 标签来设置容器上的 labels123456version: "3.7"services: web: image: web labels: com.example.description: "This label will appear on all containers for the web service" mode global:每个集节点只有一个容器 replicated:指定容器数量（默认）123456version: "3.7"services: worker: image: dockersamples/examplevotingapp_worker deploy: mode: global placement指定 constraints 和 preferences,有关语法的完整描述以及约束和首选项的可用类型，请参阅docker service create documentation1234567891011version: "3.7"services: db: image: postgres deploy: placement: constraints: - node.role == manager - engine.labels.operatingsystem == ubuntu 14.04 preferences: - spread: node.labels.zone replicas如果服务是 replicated（默认)，需要指定运行的容器数量12345678910version: "3.7"services: worker: image: dockersamples/examplevotingapp_worker networks: - frontend - backend deploy: mode: replicated replicas: 6 resources配置资源限制123456789101112version: "3.7"services: redis: image: redis:alpine deploy: resources: limits: cpus: '0.50' memory: 50M reservations: cpus: '0.25' memory: 20M 此例子中，redis 服务限制使用不超过 50M 的内存和 0.50（50％）可用处理时间（CPU），并且 保留 20M 了内存和 0.25 CPU时间 restart_policy配置容器的重新启动，代替 restart condition:值可以为 none 、on-failure 以及 any(默认) delay: 尝试重启的等待时间，默认为 0 max_attempts: 在放弃之前尝试重新启动容器次数（默认：从不放弃）。如果重新启动在配置中没有成功 window，则此尝试不计入配置max_attempts 值。例如，如果 max_attempts 值为 2，并且第一次尝试重新启动失败，则可能会尝试重新启动两次以上。 windows: 在决定重新启动是否成功之前的等时间，指定为持续时间（默认值：立即决定）。12345678910version: "3.7"services: redis: image: redis:alpine deploy: restart_policy: condition: on-failure delay: 5s max_attempts: 3 window: 120s rollback_config配置在更新失败的情况下应如何回滚服务。 parallelism: 一次回滚的容器数。如果设置为0，则所有容器同时回滚 delay: 每个容器组的回滚之间等待的时间（默认为0） failure_action: 如果回滚失败该怎么办。继续或暂停（默认暂停） monitor: 每次更新任务后监视失败的持续时间（ns | us | ms | s | m | h）（默认为0） max_failure_ratio: 回滚期间容忍的失败率（默认为0） order: 回滚期间的操作顺序。stop-first（旧任务在启动新任务之前停止）或start-first（新任务首先启动，运行任务暂时重叠）（默认stop-first）。 update_config配置更新服务，用于无缝更新应用（rolling update)parallelism, delay, failure_action, monitor, max_failure_ratio, order指令和 rollback_config 意思一样，只是是更新而已。123456789101112version: '3.7'services: vote: image: dockersamples/examplevotingapp_vote:before depends_on: - redis deploy: replicas: 2 update_config: parallelism: 2 delay: 10s order: stop-first 不支持 Docker stack desploy 的几个子选项build、cgroup_parent、container_name、devices、tmpfs、external_links、inks、network_mode、restart、security_opt、stop_signal、sysctls、userns_mode devices设置映射列表，与 Docker 客户端的 –device 参数类似 :12devices: - "/dev/ttyUSB0:/dev/ttyUSB0" dns自定义 DNS 服务器，与 –dns 具有一样的用途，可以是单个值或列表1234dns: 8.8.8.8dns: - 8.8.8.8 - 9.9.9.9 dns_search自定义 DNS 搜索域，可以是单个值或列表1234dns_search: example.comdns_search: - dc1.example.com - dc2.example.com entrypoint在 Dockerfile 中有一个指令叫做 ENTRYPOINT 指令，用于指定接入点。在 docker-compose.yml 中可以定义接入点，覆盖 Dockerfile 中的定义：1entrypoint: /code/entrypoint.sh entrypoint 也可以是一个列表，方法类似于 dockerfile1234567entrypoint: - php - -d - zend_extension=/usr/local/lib/php/extensions/no-debug-non-zts-20100525/xdebug.so - -d - memory_limit=-1 - vendor/bin/phpunit env_file从文件中添加环境变量。可以是单个值或是列表,如果已经用 docker-compose -f FILE 指定了 Compose 文件，那么 env_file 路径值为相对于该文件所在的目录, 但 environment 环境中的设置的变量会会覆盖这些值，无论这些值未定义还是为 None.1env_file: .env 或者根据 docker-compose.yml 设置多个：1234env_file: - ./common.env - ./apps/web.env - /opt/secrets.env 环境配置文件 env_file 中的声明每行都是以 VAR=VAL 格式，其中以 # 开头的被解析为注释而被忽略 environment添加环境变量，可以使用数组或字典。与上面的 env_file 选项完全不同，反而和 arg 有几分类似，这个标签的作用是设置镜像变量，它可以保存变量到镜像里面，也就是说启动的容器也会包含这些变量设置，这是与 arg 最大的不同。一般 arg 标签的变量仅用在构建过程中。而 environment 和 Dockerfile 中的 ENV 指令一样会把变量一直保存在镜像、容器中，类似 docker run -e 的效果.1234environment: RACK_ENV: development SHOW: 'true' SESSION_SECRET: expose暴露端口，但不映射到宿主机，只被连接的服务访问。这个标签与 Dockerfile 中的 EXPOSE 指令一样，用于指定暴露的端口，但是只是作为一种参考，实际上 docker-compose.yml 的端口映射还得 ports 这样的标签.123expose: - "3000" - "8000" external_links链接到 docker-compose.yml 外部的容器，甚至 并非 Compose 项目文件管理的容器。参数格式跟 links 类似.1234external_links: - redis_1 - project_db_1:mysql - project_db_1:postgresql extra_hosts添加主机名的标签，就是往 /etc/hosts 文件中添加一些记录，与 Docker 客户端 中的 –add-host 类似：123extra_hosts: - "somehost:162.242.195.82" - "otherhost:50.31.209.229" 具有 IP 地址和主机名的条目在 /etc/hosts 内部容器中创建。启动之后查看容器内部 hosts ，例如：12162.242.195.82 somehost50.31.209.229 otherhost healthcheck用于检查测试服务使用的容器是否正常.123456healthcheck: test: ["CMD", "curl", "-f", "http://localhost"] interval: 1m30s timeout: 10s retries: 3 start_period: 40s interval，timeout 以及 start_period 都定为持续时间, test 必须是字符串或列表，如果它是一个列表，第一项必须是 NONE，CMD 或 CMD-SHELL ；如果它是一个字符串，则相当于指定CMD-SHELL 后跟该字符串。例如：12# Hit the local web apptest: ["CMD", "curl", "-f", "http://localhost"] 如上所述，但包装在/bin/sh中。而以下两种形式都是等同的。123test: ["CMD-SHELL", "curl -f http://localhost || exit 1"]test: curl -f https://localhost || exit 1 如果需要禁用镜像的所有检查项目，可以使用 disable:true,相当于 test:[“NONE”]12healthcheck: disable: true init在容器内运行init，转发信号并重新获得进程。将此选项设置为true可为服务启用此功能。12345version: "3.7"services: web: image: alpine:latest init: true labels使用 Docker 标签将元数据添加到容器，可以使用数组或字典。与 Dockerfile 中的 LABELS 类似：123456789labels: com.example.description: "Accounting webapp" com.example.department: "Finance" com.example.label-with-empty-value: ""labels: - "com.example.description=Accounting webapp" - "com.example.department=Finance" - "com.example.label-with-empty-value" links链接到其它服务的中的容器，可以指定服务名称也可以指定链接别名（SERVICE：ALIAS)，与 Docker 客户端的 –link 有一样效果，会连接到其它服务中的容器.12345web: links: - db - db:database - redis 使用的别名将会自动在服务容器中的 /etc/hosts 里创建。相应的环境变量也将被创建。例如：123172.12.2.186 db172.12.2.186 database172.12.2.187 redis logging配置日志服务1234logging: driver: syslog options: syslog-address: "tcp://192.168.0.42:123" 该 driver值是指定服务器的日志记录驱动程序，默认值为 json-file,与 –log-diver 选项一样123driver: "json-file"driver: "syslog"driver: "none" 只有驱动程序 json-file 和 journald 驱动程序可以直接从 docker-compose up 和 docker-compose logs 获取日志。使用任何其他方式不会显示任何日志。 对于可选值，可以使用 options 指定日志记录中的日志记录选项123driver: "syslog"options: syslog-address: "tcp://192.168.0.42:123" 默认驱动程序 json-file 具有限制存储日志量的选项，所以，使用键值对来获得最大存储大小以及最小存储数量123options: max-size: "200k" max-file: "10" 上面实例将存储日志文件，直到它们达到max-size:200kB，存储的单个日志文件的数量由该 max-file 值指定。随着日志增长超出最大限制，旧日志文件将被删除以存储新日志docker-compose.yml 限制日志存储的示例:12345678services: some-service: image: some-service logging: driver: "json-file" options: max-size: "200k" max-file: "10" network_mode可以指定使用服务或者容器的网络模式，用法类似于 Docke 客户端的 –net 选项，格式为：service:[service name]12345network_mode: "bridge"network_mode: "host"network_mode: "none"network_mode: "service:[service name]"network_mode: "container:[container name/id]" 在群集模式下使用（版本3）Compose文件部署堆栈时，将忽略此选项。network_mode：“host”不能与links混合使用。 networks加入指定网络12345services: some-service: networks: - some-network - other-network aliases同一网络上的其他容器可以使用服务器名称或别名来连接到其他服务的容器12345678910services: some-service: networks: some-network: aliases: - alias1 - alias3 other-network: aliases: - alias2 下面实例中，提供 web 、worker以及db 服务，伴随着两个网络 new 和 legacy 。相同的服务可以在不同的网络有不同的别名:1234567891011121314151617181920212223242526version: "3.7"services: web: image: "nginx:alpine" networks: - new worker: image: "my-worker-image:latest" networks: - legacy db: image: mysql networks: new: aliases: - database legacy: aliases: - mysqlnetworks: new: legacy: ipv4_address、ipv6_address为服务的容器指定一个静态 IP 地址1234567891011121314151617version: "3.7"services: app: image: nginx:alpine networks: app_net: ipv4_address: 172.16.238.10 ipv6_address: 2001:3984:3989::10networks: app_net: ipam: driver: default config: - subnet: "172.16.238.0/24" - subnet: "2001:3984:3989::/64" PID将 PID 模式设置为主机 PID 模式，可以打开容器与主机操作系统之间的共享 PID 地址空间。使用此标志启动的容器可以访问和操作宿主机的其他容器，反之亦然。1pid: "host" ports映射容器端口到本地主机 SHORT 语法可以使用 HOST:CONTAINER 的方式指定端口，也可以指定容器端口（选择临时主机端口），宿主机会随机映射端口。123456789ports: - "3000" - "3000-3005" - "8000:8000" - "9090-9091:8080-8081" - "49100:22" - "127.0.0.1:8001:8001" - "127.0.0.1:5000-5010:5000-5010" - "6060:6060/udp" 注意：当使用 HOST:CONTAINER 格式来映射端口时，如果使用的容器端口小于 60 可能会得到错误得结果，因为YAML 将会解析 xx:yy 这种数字格式为 60 进制，所以建议采用字符串格式。 LONG 语法LONG 语法支持 SHORT 语法不支持的附加字段 target：容器内的端口 published：公开的端口 protocol： 端口协议（tcp 或 udp） mode：通过host 用在每个节点还是哪个发布的主机端口或使用 ingress 用于集群模式端口进行平衡负载，12345ports: - target: 80 published: 8080 protocol: tcp mode: host volumes挂载一个目录或者一个已存在的数据卷容器，可以直接使用 HOST:CONTAINER 这样的格式，或者使用 HOST:CONTAINER:ro 这样的格式，后者对于容器来说，数据卷是只读的，这样可以有效保护宿主机的文件系统。1234567891011121314151617181920212223version: "3.7"services: web: image: nginx:alpine volumes: - type: volume source: mydata target: /data volume: nocopy: true - type: bind source: ./static target: /opt/app/static db: image: postgres:latest volumes: - "/var/run/postgres/postgres.sock:/var/run/postgres/postgres.sock" - "dbdata:/var/lib/postgresql/data"volumes: mydata: dbdata: SHORT 语法可以选择在主机（HOST:CONTAINER）或访问模式（HOST:CONTAINER:ro）上指定路径。可以在主机上挂载相对路径，该路径相对于正在使用的 Compose 配置文件的目录进行扩展。相对路径应始终以 . 或 .. 开头123456789101112131415volumes: # Just specify a path and let the Engine create a volume - /var/lib/mysql # Specify an absolute path mapping - /opt/data:/var/lib/mysql # Path on the host, relative to the Compose file - ./cache:/tmp/cache # User-relative path - ~/configs:/etc/configs/:ro # Named volume - datavolume:/var/lib/mysql LONG 语法LONG 语法有些附加字段 type：安装类型，可以为 volume、bind 或 tmpfs source：安装源，主机上用于绑定安装的路径或定义在顶级 volumes密钥中卷的名称 ,不适用于 tmpfs 类型安装。 target：卷安装在容器中的路径 read_only：标志将卷设置为只读 bind：配置额外的绑定选项 propagation：用于绑定的传播模式 volume：配置额外的音量选项 nocopy：创建卷时禁止从容器复制数据的标志 tmpfs：配置额外的 tmpfs 选项 size：tmpfs 的大小，以字节为单位123456789101112131415161718192021version: "3.7"services: web: image: nginx:alpine ports: - "80:80" volumes: - type: volume source: mydata target: /data volume: nocopy: true - type: bind source: ./static target: /opt/app/staticnetworks: webnet:volumes: mydata: volumes 的配置参考driver指定该卷使用哪个卷驱动程序。 默认为Docker Engine配置使用的任何驱动程序，在大多数情况下是local驱动程序。 如果驱动程序不可用，则当docker-compose尝试创建卷时，Engine会返回错误。123456volumes: example: driver_opts: type: "nfs" o: "addr=10.40.0.199,nolock,soft,rw" device: ":/docker/example" external如果设置为true，则指定此卷是在撰写之外创建的。 docker-compose up不会尝试创建它，如果它不存在则会引发错误。在下面的示例中，Compose不是尝试创建名为[projectname] _data的卷，而是查找名称为该数据卷的现有卷，并将其挂载到db服务的容器中。1234567891011version: "3.7"services: db: image: postgres volumes: - data:/var/lib/postgresql/datavolumes: data: external: true labels使用Docker标签向容器添加元数据。您可以使用数组或字典。1234labels: com.example.description: "Database volume" com.example.department: "IT/Ops" com.example.label-with-empty-value: "" name为该卷设置自定义名称。name字段可用于引用包含特殊字符的卷。该名称按原样使用，不受堆栈名称的限制。1234version: "3.7"volumes: data: name: my-app-data 它也可以与 external 属性一起使用:12345version: "3.7"volumes: data: external: true name: my-app-data networks 的配置参考顶层的网络密钥允许您指定要创建的网络。 driver指定网络使用哪个驱动程序。默认驱动程序取决于您正在使用的Docker引擎是如何配置的，但在大多数情况下，它是在单个主机上桥接的，并覆盖在集群上。如果驱动程序不可用，Docker引擎会返回错误。1driver: overlay bridgeDocker默认使用单个主机上的网桥。有关如何使用桥接网络的示例，请参阅有关Bridge网络的Docker Labs教程。 overlay创建一个跨主机群的多个节点的命名网络。 host or none使用主机的网络堆栈，或不使用网络。相当于docker run --net = host或docker run --net = none。仅在使用docker stack命令时使用。如果使用docker-compose命令，请改用network_mode。12345678910version: "3.7"services: web: networks: hostnet: &#123;&#125;networks: hostnet: external: true name: host driver_opts将选项列表指定为键值对，以传递给此网络的驱动程序。这些选项取决于驱动程序.123driver_opts: foo: "bar" baz: 1 attachable仅在将driver程序设置为overlay时使用,如果设置为true，则除了服务之外，还可以将独立容器附加到此网络。如果一个独立容器连接到overlay网络,那么它可以与服务和独立容器通信，这些服务和容器也从其他Docker守护进程连接到overlay网络。1234networks: mynet1: driver: overlay attachable: true ipam指定自定义IPAM配置。这是一个具有多个属性的对象，每个属性都是可选的: driver: 自定义IPAM驱动程序，默认是default。 config: 包含零个或多个配置块的列表，每个配置块包含以下任意一个键: subnet : CIDR格式的子网，表示网络段 一个完整的例子：1234ipam: driver: default config: - subnet: 172.28.0.0/16 注意：其他IPAM配置（例如网关）目前仅适用于版本2。 internal使用Docker标签向容器添加元数据。您可以使用数组或字典。 external如果设置为true，则该网络已在compose外部创建。docker-compose up不尝试创建它，如果它不存在，就会引发错误。 name为此网络设置自定义名称。名称字段可用于引用包含特殊字符的网络。该名称按原样使用，不会使用堆栈名称作为范围。1234version: "3.7"networks: network1: name: my-app-net 创建一个compose实例本实例是以单机编排进行说明，因此有些配置并不适宜，比如deploy配置，该配置适用于swarm集群。关于哪些配置能在集群中用哪些不能，上文有说明。或者前往官网查看。 试验环境我们将使用docker-compose.yml搭建2个服务。分别为nginx和tomcat来进行说明。而其中nginx使用image指令pull镜像，tomcat使用build进行本地构建。（swarm集群会忽略build指令） 创建docker-compose项目1$ mkdir /webserver 创建docker-compose.yml123456789101112131415161718192021222324252627282930313233343536373839404142434445$ vi docker-compose.ymlversion: "3.7"services: nginx: # 服务名称 image: nginx:1.17.0-alpine # 从dockerhub上拉取nginx镜像 volumes: # 定义容器挂载卷 - nginx_conf:/etc/nginx:rw - nginx_db:/usr/share/nginx/html:ro ports: # 定义容器端口映射到本地的端口 - "80:80" - "443:443" depends_on: # 启动顺序依赖 - tomcat networks: # 加入的网络 - frontend deploy: # 该语句块配置只能在swarm集群中使用，这里写出来只是用于参考，实际上这里我注释了。 mode: replicated replicas: 3 restart_policy: condition: no-failure resources: limits: cpus: '2' memory: 1024M reservations: cpus: '1' memory: 500M tomcat: build: context: ./tomcat_df dockerfile: Dockerfile-tomcat# args:# TOMCAT_PORT: 808 labels: com.ipyker.descrioption: "Accounting webapp" networks: - frontendnetworks: frontend:volumes: nginx_db: nginx_conf: 可以看到tomcat使用build指令构建Dockerfile，Dockerfile-tomcat在当前撰写文件目录的tomcat_df目录中。1234$ cat tomcat_df/Dockerfile-tomcatFROM tomcatMAINTAINER pykerzhang &lt;pyker@qq.com&gt;VOLUME ["/tomcat_data"] 我们这里只是为了演示，所以使用了tomcat镜像，如果你需要自己配置镜像，可以自己编写Dockerfile编写自己需要的tomcat镜像。 启动服务123$ docker-compose up -dStarting webserver_nginx_1 ... doneStarting webserver_tomcat_1 ... done 查看镜像123456$ docker-compose imagesWARNING: Some services (nginx, tomcat) use the 'deploy' key, which will be ignored. Compose does not support 'deploy' configuration - use `docker stack deploy` to deploy to a swarm. Container Repository Tag Image Id Size ------------------------------------------------------------------------------webserver_nginx_1 nginx 1.17.0-alpine bfba26ca350c 19.5 MBwebserver_tomcat_1 webserver_tomcat latest 516992dbffe3 498 MB 上述WARNING是因为在单机编排中使用了swarm集群模式的deploy指令，被忽略了所产生的。 查看运行的容器12345$ docker-compose ps Name Command State Ports --------------------------------------------------------------------------------------------webserver_nginx_1 nginx -g daemon off; Up 0.0.0.0:443-&gt;443/tcp, 0.0.0.0:80-&gt;80/tcpwebserver_tomcat_1 catalina.sh run Up 8080/tcp 查看挂载卷和网络123456789101112$ docker network lsNETWORK ID NAME DRIVER SCOPE5b5654c5583f bridge bridge localc362e57d9526 host host local9ee0bba2c2bd none null local2237dc005d9f webserver_frontend bridge local$ docker volume lsDRIVER VOLUME NAMElocal 0bc45d250accfa394bbfcf669bd3564e3cf7eb6ffcac7224c65f78e07ae76eb5local webserver_nginx_conflocal webserver_nginx_db 此时可以发现我们在docker-compose.yml文件中定义的networks和volumes均自动创建了。类似手动执行docker network create webserver_frontend 和docker volume create webserver_nginx_conf， docker volume create webserver_nginx_db。那么卷将会在/var/lib/docker/volumes下产生，并且会将容器挂载的目录映射到对应的本地目录。如：12$ ls /var/lib/docker/volumes/webserver_nginx_conf/_data/conf.d fastcgi.conf fastcgi_params koi-utf koi-win mime.types modules nginx.conf scgi_params uwsgi_params win-utf 那么此时我们就可以通过修改配置nginx虚拟主机配置文件来反向代理tomcat1234567server &#123; listen 80; server_name localhost; location /tomcat/ &#123; proxy_pass http://172.18.0.2:8080/; ｝｝ 重启nginx服务1$ docker-compose restart nginx 访问 tomcat服务现在可以在局域网任意设备上访问该宿主主机IP的80端口进行访问容器服务了。123456$ curl -I http://192.168.20.210/webtomcat/HTTP/1.1 200Server: nginx/1.17.0Date: Wed, 12 Jun 2019 10:03:24 GMTContent-Type: text/html;charset=UTF-8Connection: keep-alive]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker三剑客之docker-machine用法]]></title>
    <url>%2F2018%2F03%2F22%2Fdocker-machine.html</url>
    <content type="text"><![CDATA[Docker Machine 概述docker-machine 是docker官方提供的docker管理工具。简单来说就是给你快速创建一个docker容器环境的, 假如你要给100台阿里云ECS安装上docker，传统方式就是你一台一台ssh上去安装，但是有了docker-machine就不一样了，你可以快速给100台ecs安装上docker，怎么快速法呢，你看完这文章就知道了。还有就是你要在本地快速创建docker集群环境时，总不能一台一台创建虚拟机吧，所以docker-machine可以解决这个问题。总之docker-machine就是帮助你快速去创建安装docker环境的工具，这样说应该没什么问题吧！通过docker-machine可以轻松的做到： 在Windows平台和MAC平台安装和运行docker 搭建和管理多个docker 主机 搭建swarm集群 Docker Machine安装可以手动前往docker machine的github地址下载需要的版本,然后手动重命名且修改权限，或者也可以直接运行以下命令进行安装docker machine。123$ curl -L https://github.com/docker/machine/releases/download/v0.16.1/docker-machine-`uname -s`-`uname -m` &gt;/tmp/docker-machine &amp;&amp; chmod +x /tmp/docker-machine &amp;&amp; sudo cp /tmp/docker-machine /usr/local/bin/docker-machine 检查是否安装成功:123#执行docker-machine version命令查看是否安装成功以及对应的版本$ docker-machine version docker-machine version 0.16.1, build cce350d7 安装 bash completion scripts为了得到更好的体验，我们可以安装 bash completion script，这样在 bash 能够通过 tab 键补全 docker-mahine 的子命令和参数。 请先确认版本并运行以下命令将脚本保存到/etc/bash_completion.d目录中 12345base=https://raw.githubusercontent.com/docker/machine/v0.16.1for i in docker-machine-prompt.bash docker-machine-wrapper.bash docker-machine.bashdo sudo wget "$base/contrib/completion/bash/$&#123;i&#125;" -P /etc/bash_completion.ddone 然后，您需要在bash终端中运行1$ source /etc/bash_completion.d/docker-machine-prompt.bash 要启用docker-machine shell提示符，请将$(__docker_machine_ps1)添加到~/.bashrc中的PS1设置。123cat &gt;&gt; ~/.bashrc &lt;&lt; EOFPS1='[\u@\h \W$(__docker_machine_ps1)]\$ 'EOF Docker Machine命令介绍123456789101112131415161718192021222324252627282930313233343536373839404142434445464748docker-machine --helpUsage: docker-machine [OPTIONS] COMMAND [arg...]Create and manage machines running Docker.Version: 0.16.1, build cce350d7Author: Docker Machine Contributors - &lt;https://github.com/docker/machine&gt;Options: --debug, -D Enable debug mode --storage-path, -s "/root/.docker/machine" Configures storage path [$MACHINE_STORAGE_PATH] --tls-ca-cert CA to verify remotes against [$MACHINE_TLS_CA_CERT] --tls-ca-key Private key to generate certificates [$MACHINE_TLS_CA_KEY] --tls-client-cert Client cert to use for TLS [$MACHINE_TLS_CLIENT_CERT] --tls-client-key Private key used in client TLS auth [$MACHINE_TLS_CLIENT_KEY] --github-api-token Token to use for requests to the Github API [$MACHINE_GITHUB_API_TOKEN] --native-ssh Use the native (Go-based) SSH implementation. [$MACHINE_NATIVE_SSH] --bugsnag-api-token BugSnag API token for crash reporting [$MACHINE_BUGSNAG_API_TOKEN] --help, -h show help --version, -v print the version Commands: active 显示当前操作的是哪台machine config 显示已连接machine的配置信息 create 创建一个machine主机 env 显示为Docker客户机设置环境的命令 inspect 检查machine的详细信息 ip 获取machine的ip地址 kill 停止一个machine ls 列出所有machine provision 重建现有的machines regenerate-certs 为计machine重新生成TLS证书 restart 重启machine rm 剔除一个machine ssh 使用SSH服务登陆machine去执行命令 scp machine之间复制文件 mount 使用SSHFS在machine中挂载和卸载目录卷 start 启动一个machine status 获取machine的状态 stop 停止一个machine upgrade 将machine升级到Docker的最新版本 url 获取machine tcp地址的url version 显示Docker machine版本 help 显示一个命令的命令列表或帮助 Run 'docker-machine COMMAND --help' for more information on a command. 创建docker-machine对于 Docker Machine 来说，术语 Machine 就是运行 docker daemon 的主机。“创建 Machine” 指的就是在 host 上安装和部署 docker。先执行 docker-machine ls 查看一下当前的 machine：12$ docker-machine lsNAME ACTIVE DRIVER STATE URL SWARM DOCKER ERRORS 当前还没有 machine，接下来我们创建第一个 machine： k8s-node1—192.168.20.213。创建 machine 要求能够无密码登录远程主机，所以需要确保docker-machine主机能无密码登陆machine主机。 创建docker-machine主机12345678910111213141516$ docker-machine create --driver generic --generic-ip-address=192.168.20.213 --generic-ssh-key ~/.ssh/id_rsa --generic-ssh-user=root k8s-node1Running pre-create checks...Creating machine...(k8s-node2) Importing SSH key...Waiting for machine to be running, this may take a few minutes...Detecting operating system of created instance...Waiting for SSH to be available...Detecting the provisioner...Provisioning with centos...Copying certs to the local machine directory...Copying certs to the remote machine...Setting Docker configuration on the remote daemon...Checking connection to Docker...Docker is up and running!To see how to connect your Docker Client to the Docker Engine running on this virtual machine, run: docker-machine env k8s-node1 docker-machine详细命令参见：https://docs.docker.com/machine/overview/ 上述命令分析： create #创建docker主机 --driver generic #驱动类型 generic 支持linux通用服务器，还支持很多种云主机 --generic-ip-address=192.168.20.213 #指定主机 --generic-ssh-key~/.ssh/id_rsa #指定私钥 --generic-ssh-user=root #指定用户 k8s-node1 #主机名称 因为我们是往普通的 Linux 中部署 docker，所以使用 generic driver。,更多driver参考：https://docs.docker.com/machine/drivers/ 再次执行 docker-machine ls：123$ docker-machine lsNAME ACTIVE DRIVER STATE URL SWARM DOCKER ERRORSk8s-node1 - generic Running tcp://192.168.20.213:2376 v18.09.6 可以使用同样的方法创建多台docker machine主机。如我这里在创建一台k8s-node2。1234$ docker-machine lsNAME ACTIVE DRIVER STATE URL SWARM DOCKER ERRORSk8s-node1 * generic Running tcp://192.168.20.213:2376 v18.09.6 k8s-node2 - generic Running tcp://192.168.20.214:2376 v18.09.6 管理machine用 docker-machine 创建 machine 的过程很简洁，非常适合多主机环境。除此之外，Docker Machine 也提供了一些子命令方便对 machine 进行管理。其中最常用的就是无需登录到 machine 就能执行 docker 相关操作。 查看环境变量Docker Machine 则让这个过程更简单。docker-machine env k8s-node1显示访问k8s-node1 需要的所有环境变量：1234567$ docker-machine env k8s-node1export DOCKER_TLS_VERIFY="1"export DOCKER_HOST="tcp://192.168.20.213:2376"export DOCKER_CERT_PATH="/root/.docker/machine/machines/k8s-node1"export DOCKER_MACHINE_NAME="k8s-node1"# Run this command to configure your shell: # eval $(docker-machine env k8s-node1) 根据提示，执行 eval $(docker-machine env k8s-node1) 会把当前shell环境设置为k8s-node1 machine的环境，之后管理docker的命令操作相当于在k8s-node1上。 启动容器在此状态下执行的所有 docker 命令其效果都相当于在 k8s-node1 上执行，可以通过docker-machine active查看当前shell对应哪个machine。例如启动一个 busybox 容器：123456$ docker run --name b1 -itd busybox7079c9d8283994ba77a93bddedd76856c74617e73eb374485a12247d61998e86$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES7079c9d82839 busybox "sh" 2 seconds ago Up 5 second b1 docker-machine ssh除了上面的使用方法外，我们还有更加简便的方式，就是使用docker-machine ssh，需要我们提前配置好 /etc/hosts，比如上面的命令还可以直接通过如下指令创建。123456# 以ssh方式登陆k8s-node1 machine主机运行docker container run命令$ docker-machine ssh k8s-node1 "docker container run --name b1 -itd busybox"# 以ssh方式登陆k8s-node1 machine主机运行docker container ls命令$ docker-machine ssh k8s-node1 "docker container ls" CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES7079c9d82839 busybox "sh" 2 minutes ago Up 2 minutes b1 mount挂载卷docker-machineMount使用sshfs将目录从machine挂载到本地主机,方便我们管理。 前提需要安装fuse-sshfs，yum install -y fuse-sshfs 例如，在docker-machine主机上如下操作：123456$ mkdir /opt/foo$ docker-machine ssh k8s-node1 mkdir foo$ docker-machine mount k8s-node1:/root/foo /opt/foo$ touch foo/bar$ docker-machine ssh k8s-node1 ls foobar 以上命令为在k8s-node1上当前目录创建一个foo挂载目录，然后使用docker-machine mount将该目录挂载到本地/opt/foo目录下，我们可以在本地执行mount命令查看挂载情况：123#通过mount可以看到挂载的是machine目录到本地$ mount | grep fooroot@k8s-node1:/root/foo on /opt/foo type fuse.sshfs (rw,nosuid,nodev,relatime,user_id=0,group_id=0) 那么现在，您可以使用machine机器上的目录来挂载到容器。而在当前docker-machine主机上就可以维护容器里的数据了。12345678#进入k8s-node1的docker shell环境，并且启动一个t1的容器并挂载/root/foo目录到容器/tmp/foo下$ eval $(docker-machine env k8s-node1)$ docker run --name t1 --rm -v /root/foo:/tmp/foo busybox ls /tmp/foobar$ touch foo/baz$ docker run --name t1 --rm -v /root/foo:/tmp/foo busybox ls /tmp/foobarbaz 这些文件实际上是通过sftp(通过ssh连接)传输的，所以这个程序(“sftp”)需要在机器上显示, 通常是这样的。 卸载挂载卷要再次卸载该目录，可以使用相同的选项，但使用-u标志。您还可以直接调用fuserunmount(或fusermount -u)命令。1$ docker-machine mount -u k8s-node1:/root/foo /opt/foo]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ansible 常用模块指令]]></title>
    <url>%2F2018%2F03%2F22%2Fansible-module.html</url>
    <content type="text"><![CDATA[本文主要讲解ansible的常用命令和简单安装步骤，具体配置文件详解以及playbook暂未涉及！后续更新。。。。 安装Ansible1$ yum install epel-release ansible -y 配置文件配置 常用ansible.cfg配置文件 123456789101112131415161718192021222324252627$ cat /etc/ansible/ansible.cfg [defaults] inventory = /etc/ansible/hosts #主机清单 library = /usr/share/my_modules/ # 使用的模块 forks = 5 #处理并发的进程数，建议设置控制机的数量 sudo_user = root #默认执行远程命令的用户 remote_port = 22 #SSH连接被控制机的端口 gathering = smart #'smart'收集facts的信息，如已收集，则采用缓存。可选参数'implicit'、'explicit'，分别表示每次都收集和默认不收集facts信息 fact_caching_timeout = 86400 #'gathering为smart'可用，设置收集超时时间 fact_caching = jsonfile #'gathering为smart'可用，设置以什么存储facts信息。还可在本地安装redis、memcache来作为facts的存储。 fact_caching_connection = /etc/ansible/ansible_facts_cache #'gathering为smart'可用，设置缓存路径。 roles_path = /etc/ansible/roles #设置ansible其他roles的路径 host_key_checking = false #忽略检查主机密钥 log_path = /var/log/ansible.log #ansible日志路径 module_name = command #ansible默认的模块 deprecation_warnings = False #禁用ansible“不建议使用”的警告 [ssh_connection] ssh_args = -C -o ControlMaster=auto -o ControlPersist=1d #开启ssh长连接，保存时间为1天 control_path_dir = /etc/ansible/ssh-socket #ssh长连接存放的路径 control_path = %(directory)s/%%h-%%p-%%r #ssh长连接的格式名称 pipelining = True #减少执行远程模块SSH操作次数，开启这个设置,将显著提高性能，但被控制机需要将/etc/sudoers下"Defaults requiretty"注释掉 [accelerate] accelerate_port = 5099 #使用python程序在被控制机上运行一个守护进程，ansible通过这个守护进程监听的端口进行通信。所有机器都要安装python-keyczar包 accelerate_timeout = 30 #设置用来控制从客户机获取数据的超时时间，如果在这段时间内没有数据传输,套接字连接会被关闭。 accelerate_connect_timeout = 5.0 #设置空着套接字调用的超时时间.这个应该设置相对比较短.这个和`accelerate_port`连接在回滚到ssh或者paramiko连接方式之前会尝试三次开始远程加速daemon守护进程.默认设置为1.0秒: hosts 文件（机器清单，进行分组管理） 123456[tomcat]192.168.16.192192.168.16.193[nginx]app1 ansible_ssh_host=192.168.16.190 ansible_ssh_pass="2039"app2 ansible_ssh_host=192.168.16.191 sudo_user=”ald” ansible_ssh_pass="2039" sudo_user和ansible_ssh_pass为帐号密码方式验证，建议用密钥。 Ansible指令参数 ansible 常用命令参数-m指定模块-a 指定模块的命令。默认是command模块，可以省略-B 指定ansible后台运行超时时间-C 测试运行效果，而不是正在运行-f 指定使用的并行进程的数量-i 指定inventory/hosts文件，默认/etc/ansiable/hosts文件–limit=xxx.xxx.xxx.xxx 限制对某个ip或者网段或者组执行–list-hosts 显示将要执行命令的主机 ansible-doc 常用命令参数-M –module-path=/xxx/xxx 查询模块 默认是/usr/share/ansible/-l –list 显示已存在的所有模块-s command 显示playbook制定模块的用法，类似 man 命令 ansible-galaxy 下载第三方模块指令，类似yum、pip、easy_install这样的命令 ansible-galaxy install &lt;module_name&gt; ansible-playbook 常用命令参数–syntax-check [yaml文件] 语法检测-t TAGS 只允许指定的tags标签任务，多个以 , 分开–skip-tags=SKIP_TAGS 跳过指定的标签–start-at-task=START_AT 从哪个任务后执行 常用指令模块1、 copy——拷贝模块 (用于将本地或远程机器上的文件拷贝到远程主机上)12$ ansible all –m copy -a “src=/xxx/xxx dest=/yyy/yyy owner=root group=root mode=644 force=yes/no backup=yes/no”解释：将src 文件/目录复制到远程dest上，所有者/组为root 权限为644，force为是否强制替换，backup为替换前是否需要备份远程远文件 2、 raw——命令模块 （和command、shell类似）12$ ansible all –m raw -a "ifconfig"解释：在所有主机上执行ifocnfig命令。 3、 yum——安装模块 （安装程序）12$ ansible all –m yum –a “name=httpd state=present”解释：安装httpd程序，state可以是present、latest、installed表示安装程序，absent、removed表示卸载程序 4、 file——文件模块 （文件属性修改）123456789$ ansible all –m file –a “src=/xxx/xxx/1 dest=/yyy/1 state=link owner=alad group=alad mode=777”$ ansible develop -m file -a "path=/xxx/dir recurse=yes owner=root group=alad mode=644"解释：1、将src的文件软连接到dest目录下，并修改所有者/组和权限 2、将path路径的目录递归形式设置所有者和权限*、state还可以是directory：如果目录不存在，创建目录 file：即使文件不存在，也不会被创建 hard：创建硬链接 touch：如果文件不存在，则会创建一个新的文件，如果文件或目录已存在，则更新其最后修改时间 absent：删除目录、文件或者取消链接文件 5、 cron——计划任务模块 （计划任务crontab）1234$ ansible develop -m cron -a "name='show time' minute=*/1 hour=* day=* month=* weekday=* job='/bin/date'"$ ansible develop -m cron -a "name='show time' state=absent"解释：1、创建一个每分钟显示时间的计划任务 2、删除名为show time这个计划任务 6、 group——组模块（用户组）12$ ansible all-m group -a "name=develop"解释：在所有主机上创建一个develop的组 ，state=absent表示删除该组 7、 user——用户模块（用户）12345$ ansible develop -m user -a "name=harlan groups=root password=-1vFO89dP6qyK"$ ansible develop -m user -a "name=harlan state=absent remove=yes"解释：1、在所有主机上创建harlan用户，并将其添加到root组，密码是经过hash加密后的，明文密码会被哈希，所以先填入hash后的密码即可可用此命令hash密码 openssl passwd -salt -1 "123456" 2、删除harlan用户。Remove表示是否删除用户的同时删除家目录 8、 service——服务模块（服务状态）1234$ ansible develop -m service -a "name=nginx state=running"$ ansible develop -m service -a "name=nginx state=restarted enabled=yes"解释：1、无论服务处在什么状态，最后都是将服务状态设置为启动，当服务正在运行的时候，显示为changed为false，state显示为状态，表示为正在运行；当服务停止的时候，显示为changed为true，表示这个时候将服务进行了启动，状态为启动 2、表示重启nginx并且将nginx设置为开机自启动，state还有staeted、reloaded、stoped值 9、 script——脚本模块（运行脚本）12$ ansible develop -m script -a "/root/a.sh"解释：在develop主机上运行当前服务器上的a.sh脚本 10、get_url——下载url上指定文件（类似wget）12$ ansible develop -m get_url -a "url=http://file.alavening.com/alading_file/head_img/1526900421976.jpg dest=/home/ owner=alad group=alad mode=644"解释：将url上的图片下载到dest目的目录上,并且设置相应的所有者/组和权限。 11、synchronize——同步目录（默认推送，mode=pull为拉取）1234$ ansible develop -m synchronize -a "src=/home/test/ dest=/home/test compress=yes delete=yes"$ ansible develop -m synchronize -a "src=/home/test/ dest=/home/test compress=yes mode=pull"解释：1、将src下的文件同步到dest上，delete=yes表示以src 目录为准镜像同步。 2、拉取远程src上的目录文件到本地dest上 12、template——文档内变量的替换的模块12$ ansible develop –m template –a ‘src=/mytemplates/foo.j2 dest=/etc/file.conf mode="u=rw,g=r,o=r"’解释：将src上foo.j2的变量模版复制到dest上。Template适合用playbook编写 ，通过变量然后拷贝到远程主机。 可以参考：https://www.cnblogs.com/jsonhc/p/7895399.html 13、fetch——从远程主机下载文件（不能拉取目录）12$ ansible develop -m fetch -a "src=/home/test/xxx dest=/home/ flat=yes"解释：将远程xxx文件拉取到本地home目录下，目录结构会是dest路径+远程主机名+src，假如远程主机名为develop，拉取的xxx文件在本地的/home/develop/home/test目录。如果需要指定拉取到某目录下 加个flat=yes的参数即可。 14、unarchive——解压文件1234$ ansible develop -m unarchive -a "src=/root/apache-tomcat-7.0.85.tar.gz dest=/home/test owner=alad group=alad mode=755"$ ansible develop -m unarchive -a "src=/home/alad/ansible/elk/logstash-6.2.4.tar.gz dest=/home/test remote_src=yes"$ ansible develop -m unarchive -a "src=http://mirrors.linuxeye.com/oneinstack-full.tar.gz dest=/home/test remote_src=yes"解释：将本地的tomcat压缩包解压到远程主机dest目录下，并修改其权限和所有者/组，remote_src=yes 表示解压远程主机已有的压缩包，src为url表示下载此包到远程主机dest目录进行解压缩后，并删除压缩包源文件 15、command和shell——linux命令模块12shell和command的区别：shell模块可以特殊字符，而command是不支持简单说：command运行的命令中无法使用变量，管道。如果需要使用管道、变量，请使用raw模块,或者shell模块。 16、setup——获取主机信息1234$ ansible develop -m setup$ ansible develop -m setup -a 'filter=ansible_*_mb'解释：1、显示系统所有信息 2、通常配合filter进行过滤来获取主机信息，（例子是显示内存信息） 17、assemble——配置文件组装发送到远程主机12$ ansible test -m assemble -a "src=/root/test dest=/root/ansible/fileone mode=777 remote_src=False delimiter='========'"解释：将src test目录下所有文件（不含test子目录内容）的内容发送到dest fileone文件中，remote_src默认为Ture表示src为远程主机上的路径，False为ansible控制端的路径，delimiter为文件之间内容分隔符。 Playbook语法和结构Playbook需要7个文件夹，如ansible安装nginx，则需要在/etc/ansible/roles目录下建立以下文件夹。mkdir -pv nginx/{default,tasks,vars,meta,handlers,templates,files}对于Ansible，几乎每个YAML文件都以一个列表开始。列表中的每个项目都是键/值对列表，通常称为“散列”或“字典”。所以，我们需要知道如何在YAML中编写列表和字典。YAML还有一个小小的怪癖。所有YAML文件（无论它们是否与Ansible相关联）都可以选择开始—和结束—.。这是YAML格式的一部分，并指示文档的开始和结束。列表中的所有成员都是以相同的缩进级别开头的行（短划线和空格）：”- “ 例如：12345678910111213141516171819---- hosts: test remote_user: root vars: - bsh: b.sh - httprpm: httpd task: - name: install httpd yum: name=&#123;&#123; httprpm &#125;&#125; state=present” tags: install_httpd - name: copy b.sh copy: src=/root/&#123;&#123; bsh &#125;&#125; dest=/root/ owner=ala group=ala mode=0644 notify: - reload httpd - name: start httpd service: name=httpd state=started enabled=yes handlers: - name: reload httpd service: name=httpd state=reloaded 第一次的话都会运行，后边如果copy的文件内容发生改变就会触发 notify ，然后会直接执行 handlers 的内容（ 这里notify后边的事件就都不会执行了 ）。 template模块jinja2语法 template:使用了Jinjia2格式作为文件模版，进行文档内变量的替换的模块。相当于copy，将jinja2的文件模板理解并执行，转化为各个主机间的对应值。如：template: src=httpd.conf.j2 dest=/etc/httpd/conf/httpd.conf http.conf.j2必须是完整的文件内容，因为这是覆盖操作，而非只选择性远程主机替换变量，dest要指定文件名，如果是目录就相当于copy了http.conf.j2到远程目录下，不是我们要的结果。 when语句：在tasks中使用，Jinja2的语法格式 123- name: start nginx service shell: systemctl start nginx.service when: ansible_distribution == "CentOS" and ansible_distribution_major_version == "7" 当系统为 centos 7的时候执行sysctemctl命令，否则不执行 循环：迭代，需要重复执行的任务 变量名为item，而with_item为要迭代的元素。如果某个任务出错，后面不执行 12345- name: install packages yum: name=&#123;&#123; item &#125;&#125; state=latest with_items: - httpd - php 这是基于字符串列表给出元素示例 12345- name: create users user: name=&#123;&#123; item.name &#125;&#125; group=&#123;&#123; item.group &#125;&#125; state=present with_items: - &#123;name: 'userx1', group: 'groupx1'&#125; - &#123;name: 'userx2', group: 'groupx2'&#125; 这是基于字典列表给元素示例：item.name . 后边的表示键。]]></content>
      <categories>
        <category>CI/CD</category>
      </categories>
      <tags>
        <tag>ansible</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker守护进程配置文件daemon.json]]></title>
    <url>%2F2018%2F03%2F22%2Fdocker-daemon.html</url>
    <content type="text"><![CDATA[daemon.json文件说明docker安装后默认没有/etc/docker/daemon.json 文件，需要进行手动创建。--config-file命令参数可用于指定非默认位置。但请注意：配置文件中设置的选项不得与启动参数设置的选项冲突。 如果文件和启动参数之间的选项重复，则docker守护程序无法启动，无论其值如何。 例如，如果在daemon.json配置文件中设置了守护程序标签并且还通过–label命令参数设置守护程序标签，则守护程序无法启动。 守护程序启动时将忽略文件中不存在的选项。 这是Linux上允许的配置选项的完整示例：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990&#123; "authorization-plugins": [],//访问授权插件 "data-root": "",//docker数据持久化存储的根目录,默认为/var/lib/docker "dns": [],//DNS服务器 "dns-opts": [],//DNS配置选项，如端口等 "dns-search": [],//DNS搜索域名 "exec-opts": [],//执行选项 "exec-root": "",//执行状态的文件的根目录 "experimental": false,//是否开启试验性特性 "features": &#123;&#125;,//启用或禁用特定功能。如：&#123;"buildkit": true&#125;使buildkit成为默认的docker镜像构建器。 "storage-driver": "",//存储驱动器类型 "storage-opts": [],//存储选项 "labels": [],//键值对式标记docker元数据 "live-restore": true,//dockerd挂掉是否保活容器（避免了docker服务异常而造成容器退出） "log-driver": "json-file",//容器日志的驱动器 "log-opts": &#123; "max-size": "10m", "max-file":"5", "labels": "somelabel", "env": "os,customer" &#125;,//容器日志的选项 "mtu": 0,//设置容器网络MTU（最大传输单元） "pidfile": "",//daemon PID文件的位置 "cluster-store": "",//集群存储系统的URL "cluster-store-opts": &#123;&#125;,//配置集群存储 "cluster-advertise": "",//对外的地址名称 "max-concurrent-downloads": 3,//设置每个pull进程的最大并发 "max-concurrent-uploads": 5,//设置每个push进程的最大并发 "default-shm-size": "64M",//设置默认共享内存的大小 "shutdown-timeout": 15,//设置关闭的超时时限 "debug": true,//开启调试模式 "hosts": [],//dockerd守护进程的监听地址 "log-level": "",//日志级别 "tls": true,//开启传输层安全协议TLS "tlsverify": true,//开启输层安全协议并验证远程地址 "tlscacert": "",//CA签名文件路径 "tlscert": "",//TLS证书文件路径 "tlskey": "",//TLS密钥文件路径 "swarm-default-advertise-addr": "",//swarm对外地址 "api-cors-header": "",//设置CORS（跨域资源共享-Cross-origin resource sharing）头 "selinux-enabled": false,//开启selinux(用户、进程、应用、文件的强制访问控制) "userns-remap": "",//给用户命名空间设置 用户/组 "group": "",//docker所在组 "cgroup-parent": "",//设置所有容器的cgroup的父类 "default-ulimits": &#123; "nofile": &#123; "Name": "nofile", "Hard": 64000, "Soft": 64000 &#125; &#125;,//设置所有容器的ulimit "init": false,//容器执行初始化，来转发信号或控制(reap)进程 "init-path": "/usr/libexec/docker-init",//docker-init文件的路径 "ipv6": false,//支持IPV6网络 "iptables": false,//开启防火墙规则 "ip-forward": false,//开启net.ipv4.ip_forward "ip-masq": false,//开启ip掩蔽(IP封包通过路由器或防火墙时重写源IP地址或目的IP地址的技术) "userland-proxy": false,//用户空间代理 "userland-proxy-path": "/usr/libexec/docker-proxy",//用户空间代理路径 "ip": "0.0.0.0",//默认IP "bridge": "",//将容器依附(attach)到桥接网络上的桥标识 "bip": "",//指定桥接IP "fixed-cidr": "",//(ipv4)子网划分，即限制ip地址分配范围，用以控制容器所属网段实现容器间(同一主机或不同主机间)的网络访问 "fixed-cidr-v6": "",//（ipv6）子网划分 "default-gateway": "",//默认网关 "default-gateway-v6": "",//默认ipv6网关 "icc": false,//容器间通信 "raw-logs": false,//原始日志(无颜色、全时间戳) "allow-nondistributable-artifacts": [],//不对外分发的产品提交的registry仓库 "registry-mirrors": [],//registry仓库镜像加速地址 "seccomp-profile": "",//seccomp配置文件 "insecure-registries": [],//配置非https的registry地址 "no-new-privileges": false,//禁止新优先级 "default-runtime": "runc",//OCI联盟(The Open Container Initiative)默认运行时环境 "oom-score-adjust": -500,//内存溢出被杀死的优先级(-1000~1000) "node-generic-resources": ["NVIDIA-GPU=UUID1", "NVIDIA-GPU=UUID2"],//对外公布的资源节点 "runtimes": &#123; "cc-runtime": &#123; "path": "/usr/bin/cc-runtime" &#125;, "custom": &#123; "path": "/usr/local/bin/my-runc-replacement", "runtimeArgs": [ "--debug" ] &#125; &#125;,//运行时 "default-address-pools":[&#123;"base":"172.80.0.0/16","size":24&#125;,//默认的dhcp分配地址 &#123;"base":"172.90.0.0/16","size":24&#125;]&#125; 您无法在daemon.json文件中使用已经在dockerd守护程序启动时设置的选项命令参数。 例如：在使用systemd启动Docker守护程序的系统上，-H已设置，因此您无法使用daemon.json中的hosts键添加侦听地址。 有关如何使用systemd drop-in文件完成此任务，请参阅https://docs.docker.com/engine/admin/systemd/#custom-docker-daemon-options。 dockerd守护进程选项123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081$ dockerd --helpUsage: dockerd [OPTIONS]A self-sufficient runtime for containers.Options: --add-runtime runtime Register an additional OCI compatible runtime (default []) --allow-nondistributable-artifacts list Allow push of nondistributable artifacts to registry --api-cors-header string Set CORS headers in the Engine API --authorization-plugin list Authorization plugins to load --bip string Specify network bridge IP -b, --bridge string Attach containers to a network bridge --cgroup-parent string Set parent cgroup for all containers --cluster-advertise string Address or interface name to advertise --cluster-store string URL of the distributed storage backend --cluster-store-opt map Set cluster store options (default map[]) --config-file string Daemon configuration file (default "/etc/docker/daemon.json") --containerd string containerd grpc address --cpu-rt-period int Limit the CPU real-time period in microseconds --cpu-rt-runtime int Limit the CPU real-time runtime in microseconds --data-root string Root directory of persistent Docker state (default "/var/lib/docker") -D, --debug Enable debug mode --default-address-pool pool-options Default address pools for node specific local networks --default-gateway ip Container default gateway IPv4 address --default-gateway-v6 ip Container default gateway IPv6 address --default-ipc-mode string Default mode for containers ipc ("shareable" | "private") (default "shareable") --default-runtime string Default OCI runtime for containers (default "runc") --default-shm-size bytes Default shm size for containers (default 64MiB) --default-ulimit ulimit Default ulimits for containers (default []) --dns list DNS server to use --dns-opt list DNS options to use --dns-search list DNS search domains to use --exec-opt list Runtime execution options --exec-root string Root directory for execution state files (default "/var/run/docker") --experimental Enable experimental features --fixed-cidr string IPv4 subnet for fixed IPs --fixed-cidr-v6 string IPv6 subnet for fixed IPs -G, --group string Group for the unix socket (default "docker") --help Print usage -H, --host list Daemon socket(s) to connect to --icc Enable inter-container communication (default true) --init Run an init in the container to forward signals and reap processes --init-path string Path to the docker-init binary --insecure-registry list Enable insecure registry communication --ip ip Default IP when binding container ports (default 0.0.0.0) --ip-forward Enable net.ipv4.ip_forward (default true) --ip-masq Enable IP masquerading (default true) --iptables Enable addition of iptables rules (default true) --ipv6 Enable IPv6 networking --label list Set key=value labels to the daemon --live-restore Enable live restore of docker when containers are still running --log-driver string Default driver for container logs (default "json-file") -l, --log-level string Set the logging level ("debug"|"info"|"warn"|"error"|"fatal") (default "info") --log-opt map Default log driver options for containers (default map[]) --max-concurrent-downloads int Set the max concurrent downloads for each pull (default 3) --max-concurrent-uploads int Set the max concurrent uploads for each push (default 5) --metrics-addr string Set default address and port to serve the metrics api on --mtu int Set the containers network MTU --network-control-plane-mtu int Network Control plane MTU (default 1500) --no-new-privileges Set no-new-privileges by default for new containers --node-generic-resource list Advertise user-defined resource --oom-score-adjust int Set the oom_score_adj for the daemon (default -500) -p, --pidfile string Path to use for daemon PID file (default "/var/run/docker.pid") --raw-logs Full timestamps without ANSI coloring --registry-mirror list Preferred Docker registry mirror --seccomp-profile string Path to seccomp profile --selinux-enabled Enable selinux support --shutdown-timeout int Set the default shutdown timeout (default 15) -s, --storage-driver string Storage driver to use --storage-opt list Storage driver options --swarm-default-advertise-addr string Set default address or interface for swarm advertised address --tls Use TLS; implied by --tlsverify --tlscacert string Trust certs signed only by this CA (default "/root/.docker/ca.pem") --tlscert string Path to TLS certificate file (default "/root/.docker/cert.pem") --tlskey string Path to TLS key file (default "/root/.docker/key.pem") --tlsverify Use TLS and verify the remote --userland-proxy Use userland proxy for loopback traffic (default true) --userland-proxy-path string Path to the userland proxy binary --userns-remap string User/Group setting for user namespaces -v, --version Print version information and quit]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[dockerfile指令介绍]]></title>
    <url>%2F2018%2F03%2F21%2Fdockerfile-command.html</url>
    <content type="text"><![CDATA[什么是DockerfileDocker的镜像是通过对于在Dockerfile中的一系列指令的顺序解析实现自动的image的构建，通过使用docker build命令，根据Dockerfile的描述来构建镜像。Dockerfile指令只支持Docker自己定义的一套指令，不支持自定义指令，且大小写不敏感，但是建议全部使用大写，构建命令如：123# 分别为Dockerfile当前路径和指定Dockerfile路径$ docker build -t image名称:tag标签 ./$ docker build -t image名称:tag标签 -f &lt;Dockerfile路径&gt; 注意：不要使用根(“/“)目录作为Dockfile PATH，因为它会导致构建将硬盘驱动器的全部内容传输到Docker守护程序。 BuildKit从18.09版本开始，Docker支持一种后端程序用于运行你的构建，它可以被moby/buildkit project提供。这个 BuildKit backend 提供很多与旧版本的好处。如： 捕获并跳过执行未使用的构建阶段 并行构建独立地阶段 在构建环境中，加强传输在构建阶段更改的文件 捕获并跳过传输没有用的文件 使用外部Dockerfile实现很多新的特性 避免REST API的副用作影响（中间image和container） 通过自动优化来提升构建缓存 要使用BuildKit后端，需要在调用docker build之前在CLI上设置环境变量DOCKER_BUILDKIT = 1。要了解基于BuildKit的构建可用的实验性Dockerfile语法，请参阅BuildKit存储库中的文档。 基本结构Dockerfile由一行行命令语句组成，并支持以#开头的注释行。123456789101112131415161718# This dockerfile uses the ubuntu image# VERSION 2 - EDITION 1# Author: docker_user# Command format: Instruction [arguments / command ] ..# Base image to use, this nust be set as the first lineFROM ubuntu# Maintainer: docker_user &lt;docker_user at email.com&gt; (@docker_user)MAINTAINER docker_user docker_user@email.com# Commands to update the imageRUN echo "deb http://archive.ubuntu.com/ubuntu/ raring main universe" &gt;&gt; /etc/apt/sources.listRUN apt-get update &amp;&amp; apt-get install -y nginxRUN echo "\ndaemon off;" &gt;&gt; /etc/nginx/nginx.conf# Commands when creating a new containerCMD /usr/sbin/nginx 其中，开始必须指明所基于的镜像名称，接下来一般是说明维护者信息。后面则是镜像操作指令，例如RUN指令，RUN指令将对镜像执行跟随的命令。每运行一条RUN指令，镜像就添加新的一层，并提交。最后是CMD指令，用来指定运行容器时的操作命令。 在docker官网上有这样两个例子： 在debian:jessie基础镜像上安装nginx环境，从而创建一个新的nginx镜像： 123456789101112131415161718FROM debian:jessieMAINTAINER NGINX Docker Maintainers "docker-maint@nginx.com"ENV NGINX_VERSION 1.10.1-1~jessieRUN apt-key adv --keyserver hkp://pgp.mit.edu:80 --recv-keys 573BFD6B3D8FBC641079A6ABABF5BD827BD9BF62 &amp;&amp; \echo "deb http://nginx.org/package/debian/ jessie nginx" &gt;&gt; /etc/apt/source.list &amp;&amp; apt-get update &amp;&amp; \apt-get install --no-install-recommends --no-install-suggests -y ca-certificates nginx=$(NGINX_VERSION) \nginx-module-xslt nginx-module-geoip nginx-module-image-filter nginx-module-perl nginx-module-njs gettext-base &amp;&amp; \rm -rf /var/lib/apt/lists/*# forward request and error logs to docker log collectorRUN ln -sf /dev/stdout /var/log/nginx/access.log &amp;&amp; ln -sf /dev/stderr /var/log/nginx/err.logEXPOSE 80 443CMD ["nginx","-g","daemon off;"] 基于buildpack-deps:jessie-scm基础镜像，安装golang相关环境，制作一个GO语言的运行环境。 123456789101112131415161718FROM buildpack-deps:jessie-scm# gcc fo cgoRUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends g++ gcc libc6-dev make &amp;&amp; rm -rf /var/lib/apt/lists*ENV GOLANG_VERSION 1.6.3ENV GOLANG_DOWNLOAD_RUL https://golang.org/dl/go$GOLANG_VERSION.linux-amd64.tar.gzENV GOLANG_DOWNLOAD_SHA256 cdd5e08530c0579255d6153b08fdb3b8e47caabbe717bc7bcd7561275a87aebRUN curl -fssL "$GOLANG_DOWNLOAD_RUL" -o golang.tar.gz &amp;&amp; \echo "$GOLANG_DOWNLOAD_SHA256 golang.tar.gz" | sha256sum -c - &amp;&amp; tar -C /usr/local -xzf golang.tar.gz &amp;&amp; rm golang.tar.gzENV GOPATH $GOPATH/bin:/usr/local/go/bin:$PATHRUN mkdir -p "$GOPATH/bin" &amp;&amp; chmod -R 777 "$GOPATH"WORKDIR $GOPATHCOPY go-wrapper /usr/local/bin 指令介绍指令的一般格式为INSTRUNCTION arguments，指令包括FROM、MAINTAINER、RUN等。具体指令及说明如下： FROM 指定所创建的镜像的基础镜像，如果本地不存在，则默认会去Docker Hub下载指定镜像。格式为：FROM &lt;image&gt; [AS &lt;name&gt;]，或FROM&lt;image&gt;:&lt;tag&gt; [AS &lt;name&gt;]，或FROM&lt;image&gt;@&lt;digest&gt; [AS &lt;name&gt;]。任何Dockerfile中的第一条指令必须为FROM指令。并且，如果在同一个Dockerfile文件中创建多个镜像，可以使用多个FROM指令(每个镜像一次)。 MAINTAINER 指定维护者信息，格式为MAINTAINER。(已弃用，但仍兼容，使用LABEL代替) 例如： 1MAINTAINER pyker &lt;pyker@qq.com&gt; 该信息将会写入生成镜像的Author属性域中。可以通过docker inspect image名称查看。 RUN 运行指定命令，如果有多个命令关联，建议用&amp;&amp;符号连接。格式为：RUN &lt;command&gt;或RUN [&quot;executable&quot;,&quot;param1&quot;,&quot;param2&quot;]。 注意：后一个指令会被解析为json数组，所以必须使用双引号。 前者默认将在shell终端中运行命令，即/bin/sh -c；后者则使用exec执行，不会启动shell环境。指定使用其他终端类型可以通过第二种方式实现，例如：RUN [&quot;/bin/bash&quot;,&quot;-c&quot;,&quot;echo hello&quot;] , 每条RUN指令将在FROM指定的镜像的基础上执行指定命令，并提交为新的镜像。当命令较长时可以使用\换行。例如： 123RUN apt-get update \ &amp;&amp; apt-get install -y libsnappy-dev zliblg-dev libbz2-dev \ &amp;&amp; rm -rf /var/cache/apt CMD CMD指令用来指定启动容器时默认执行的命令。它支持三种格式： 1231.CMD ["executable","param1","param2"] 使用exec执行，是推荐使用的方式；2.CMD param1 param2 在/bin/sh中执行，提供给需要交互的应用；3.CMD ["param1","param2"] 提供给ENTRYPOINT的默认参数。 每个Dockerfile只能有一条CMD命令。如果指定了多条命令，只有最后一条会被执行。如果用户在docker run启动容器时指定了运行的命令(作为run的参数)，则会覆盖掉CMD指定的命令。 LABEL LABEL指令用来生成用于生成镜像的元数据的标签信息。格式为：LABEL &lt;key&gt;=&lt;value&gt; &lt;key&gt;=&lt;value&gt; ...。例如： 12LABEL version="1.0"LABEL description="This text illustrates \ that label-values can span multiple lines." EXPOSE 声明镜像内服务所监听的端口。格式为：EXPOSE &lt;port&gt; [&lt;port&gt;...]，例如： 1EXPOSE 22 80/tcp 443/tcp 3306 注意：该命令只是起到声明的作用，并不会自动完成端口映射。在容器启动时需要使用-P(大写P)，Docker主机会自动分配一个宿主机未被使用的临时端口转发到指定的端口；使用-p(小写p)，则可以具体指定哪个宿主机的本地端口映射过来。 ENV 指定环境变量，在镜像生成过程中会被后续RUN指令使用，使用该镜像启动的容器中也会存在。格式为：ENV &lt;key&gt; &lt;value&gt;或ENV &lt;key&gt;=&lt;value&gt; ...。例如： 123456789ENV GOLANG_VERSION 1.6.3ENV GOLANG_DOWNLOAD_RUL https://golang.org/dl/go$GOLANG_VERSION.linux-amd64.tar.gzENV GOLANG_DOWNLOAD_SHA256 cdd5e08530c0579255d6153b08fdb3b8e47caabbe717bc7bcd7561275a87aebRUN curl -fssL "$GOLANG_DOWNLOAD_RUL" -o golang.tar.gz &amp;&amp; echo "$GOLANG_DOWNLOAD_SHA256 golang.tar.gz" | sha256sum -c - &amp;&amp; tar -C /usr/local -xzf golang.tar.gz &amp;&amp; rm golang.tar.gzENV GOPATH $GOPATH/bin:/usr/local/go/bin:$PATHRUN mkdir -p "$GOPATH/bin" &amp;&amp; chmod -R 777 "$GOPATH" 指令指定的环境变量在运行时可以被覆盖掉，如docker run --env &lt;key&gt;=&lt;value&gt; built_image。 ADD 该指令将复制指定的&lt;src&gt;路径下的内容到容器中的&lt;dest&gt;路径下。格式为：ADD &lt;src&gt;... &lt;dest&gt; 或 ADD [&quot;&lt;src&gt;&quot;,... &quot;&lt;dest&gt;&quot;]其中&lt;src&gt;可以使Dockerfile所在目录的一个相对路径(文件或目录)，也可以是一个URL，还可以是一个tar文件(如果是本地tar文件，会自动解压到&lt;dest&gt;路径下)。&lt;dest&gt;可以使镜像内的绝对路径，或者相当于工作目录(WORKDIR)的相对路径。路径支持正则表达式，例如： 1ADD *.c /code/ COPY 复制本地主机的&lt;src&gt;(为Dockerfile所在目录的一个相对路径、文件或目录)下的内容到镜像中的&lt;dest&gt;下。目标路径不存在时，会自动创建。路径同样支持正则。格式为：COPY &lt;src&gt; &lt;dest&gt;或 [&quot;&lt;src&gt;&quot;,... &quot;&lt;dest&gt;&quot;]当使用本地目录为源目录时，推荐使用COPY。 ENTRYPOINT 指定镜像的默认入口命令，该入口命令会在启动容器时作为根命令执行，所有传入值作为该命令的参数（包括在docker run执行的命令都将成为参数）。支持两种格式： 121.ENTRYPOINT ["executable","param1","param2"] (exec调用执行)；2.ENTRYPOINT command param1 param2 (shell中执行)。 此时，CMD指令指定值将作为ENTRYPOINT命令的参数。每个Dockerfile中只能有一个ENTRYPOINT，当指定多个时，只有最后一个有效。在docker run时可以被–entrypoint参数覆盖掉。 VOLUME 创建一个数据卷挂载点。式为：VOLUME [&quot;/data&quot;,...], 可以从本地主机或者其他容器挂载数据卷，一般用来存放数据库和需要保存的数据等。 USER 指定运行容器时的用户名或UID，后续的RUN等指令也会使用特定的用户身份。格式为：USER daemon, 当服务不需要管理员权限时，可以通过该指令指定运行用户，并且可以在之前创建所需要的用户。例如： 1RUN groupadd -r nginx &amp;&amp; useradd -r -g nginx nginx WORKDIR 为后续的RUN、CMD和ENTRYPOINT指令配置工作目录。格式为：WORKDIR /path/to/workdir。可以使用多个WORKDIR指令，后续命令如果参数是相对的，则会基于之前命令指定的路径。例如： 1234WORKDIR /aWORKDIR bWORKDIR cRUN pwd 则最终路径为/a/b/c ARG 指定一些镜像内使用的参数(例如版本号信息等)，这些参数在执行docker build命令时才以--build-arg &lt;varname&gt;=&lt;value&gt;格式传入。格式为：ARG &lt;name&gt;=&lt;value&gt;...。可以用docker build --build-arg&lt;name&gt;=&lt;value&gt;来覆盖Dockefile指定的参数值。 ONBUILD 在配置当所创建的镜像作为其他镜像的基础镜像的时候，所执行创建操作指令。格式为：ONBUILD [INSTRUCTION]。例如Dockerfile使用如下的内容创建了镜像image-A： 1234[...]ONBUILD ADD . /app/srcONBUILD RUN /usr/local/bin/python-build --dir /app/src[...] 如果基于image-A镜像创建新的镜像时，新的Dockerfile中使用FROM image-A指定基础镜像，会自动执行ONBUILD指令的内容，等价于创建新的镜像时在后面添加了两条指令： 12345FROM image-A# Automatically run the followingONBUILD ADD . /app/srcONBUILD RUN /usr/local/bin/python-build --dir /app/src 使用ONBUILD指令的镜像，推荐在标签中注明，例如：ruby:1.9-onbuild。 ONBUILD指令在基础镜像中不会执行。 STOPSIGNAL 指定所创建镜像启动的容器接收退出的信号值。例如： 1STOPSIGNAL singnal HEALTHCHECK 配置所启动容器如何进行健康检查(如何判断是否健康)，自Docker 1.12开始支持。格式有两种： 121.HEALTHCHECK [OPTIONS] CMD command ：根据所执行命令返回值是否为0判断；2.HEALTHCHECK NONE :禁止基础镜像中的健康检查。 [OPTION]支持： 12341. --inerval=DURATION (默认：30s)：多久检查一次；2. --timeout=DURATION (默认：30s)：每次检查等待结果的超时时间；3. --retries=N (默认：3)：如果失败了，重试几次才最终确定失败。 4. --start-period=DURATION （默认：0s）启动的容器提供了初始化的时间段，在此时间段内如果检查失败， 则不会记录失败次数。 例如： 1HEALTHCHECK --interval=1m --timeout=3s CMD curl -f http://localhost/ || exit 1 SHELL SHELL指令允许覆盖用于命令SHELL形式的默认SHELL。格式为： SHELL [&quot;executable&quot;,&quot;parameters&quot;]默认值为 [&quot;bin/sh&quot;,&quot;-c&quot;]。Linux上的默认shell是[&quot;bin/sh&quot;,&quot;-c&quot;],Windows上是[“cmd”, “/S”, “/C”]。SHELL指令必须以JSON格式写入Dockerfile 注意：对于Windows系统，建议在Dockerfile开头添加# escape=`来指定转移信息。例如： 1234567# escape=`FROM microsoft/nanoserverSHELL ["powershell","-command"]RUN New-Item -ItemType Directory C:\ExampleADD Execute-MyCmdlet.ps1 c:\example\RUN c:\example\Execute-MyCmdlet -sample 'hello world' 用法创建镜像编写完Dockerfile之后，可以通过docker build命令来创建镜像。基本的docker build [选项] 内容路径，该命令将读取指定路径下(包括子目录)的Dockerfile，并将该路径下的所有内容发送给Docker服务端，由服务端来创建镜像。因此除非生成镜像需要，否则一般建议放置Dockerfile的目录为空目录。 1. 如果使用非当前路径下的Dockerfile，可以通过-f选项来指定其路径。 2. 要指定生成镜像的标签信息，可以使用-t选项。 例如：指定Dockerfile所在路径为 /tmp/docker_builder/，并且希望生成镜像标签为build_repo:first_image，可以使用下面的命令：1docker build -t build_repo:first_image /tmp/docker_builder 使用 .dockerignore文件.dockerignore文件可以想github的.gitingore文件一样，可以通过 .dockeringore文件(每一行添加一条匹配模式)来让Docker忽略匹配模式路径下的目录和文件。例如：12345# comment*/tmp**/*/tmp*tmp?~* Dockerfile编写小结从需求出发，定制适合自己需求、高效方便的镜像，可以参考他人优秀的Dockerfile文件，在构建中慢慢优化Dockerfile文件：1234567891.精简镜像用途： 尽量让每个镜像的用途都比较集中、单一，避免构造大而复杂、多功能的镜像；2.选用合适的基础镜像： 过大的基础镜像会造成构建出臃肿的镜像，一般推荐比较小巧的镜像作为基础镜像；3.提供详细的注释和维护者信息： Dockerfile也是一种代码，需要考虑方便后续扩展和他人使用；4.正确使用版本号： 使用明确的具体数字信息的版本号信息，而非latest，可以避免无法确认具体版本号，统一环境；5.减少镜像层数： 减少镜像层数建议尽量合并RUN指令，可以将多条RUN指令的内容通过&amp;&amp;连接；6.及时删除临时和缓存文件： 这样可以避免构造的镜像过于臃肿，并且这些缓存文件并没有实际用途；7.提高生产速度： 合理使用缓存、减少目录下的使用文件，使用.dockeringore文件等；8.调整合理的指令顺序： 在开启缓存的情况下，内容不变的指令尽量放在前面，这样可以提高指令的复用性；9.减少外部源的干扰： 如果确实要从外部引入数据，需要制定持久的地址，并带有版本信息，让他人可以重复使用而不出错。 Dockerfile 实例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869#Function: source install nginx1.16 on the centos7 container#How use：docker run --name mynginx1 -d -p hostprot:containerport -v depend:/data/depend -v wwwroot:/data/wwwroot -v wwwlogs:/data/wwwlogs image_name:tagFROM centos:7.6 as centos7LABEL MAINTAINER="pyker &lt;pyker@qq.com&gt;"ENV NGINX=nginx-1.16.0 \ OPENSSL=openssl-1.0.2r \ PCRE=pcre-8.42 \ ZLIB=zlib-1.2.11 \ RUN_USER=nginx \ LOG_DIR=/data/wwwlogs \ WEB_DIR=/data/wwwroot \ DEPEND_DIR=/data/dependVOLUME ["/data/depend","/data/wwwroot","/data/wwwlogs"]ADD nginx-1.16.0.tar.gz openssl-1.0.2r.tar.gz pcre-8.42.tar.gz zlib-1.2.11.tar.gz $&#123;DEPEND_DIR&#125;/RUN id -u $&#123;RUN_USER&#125; &gt;/dev/null 2&gt;&amp;1 || useradd -M -s /sbin/nologin $&#123;RUN_USER&#125;RUN yum install -y epel-release &amp;&amp; \ yum install -y gcc gcc-c++ gcc++ perl perl-devel perl-ExtUtils-Embed libxslt libxslt-devel libxml2 libxml2-devel gd gd-devel GeoIP GeoIP-devel make &amp;&amp; \ yum clean allRUN cd /data/depend/$&#123;NGINX&#125; &amp;&amp; \ ./configure --prefix=/usr/local/nginx \ --sbin-path=/usr/sbin/nginx \ --user=$&#123;RUN_USER&#125; \ --group=$&#123;RUN_USER&#125; \ --pid-path=/var/run/nginx.pid \ --lock-path=/var/run/nginx.lock \ --error-log-path=$&#123;LOG_DIR&#125;/error.log \ --http-log-path=$&#123;LOG_DIR&#125;/access.log \ --with-select_module \ --with-poll_module \ --with-threads \ --with-file-aio \ --with-http_ssl_module \ --with-http_v2_module \ --with-http_realip_module \ --with-http_addition_module \ --with-http_xslt_module=dynamic \ --with-http_image_filter_module=dynamic \ --with-http_geoip_module=dynamic \ --with-http_sub_module \ --with-http_dav_module \ --with-http_flv_module \ --with-http_mp4_module \ --with-http_gunzip_module \ --with-http_gzip_static_module \ --with-http_auth_request_module \ --with-http_random_index_module \ --with-http_secure_link_module \ --with-http_degradation_module \ --with-http_slice_module \ --with-http_stub_status_module \ --with-mail=dynamic \ --with-mail_ssl_module \ --with-stream \ --with-stream_ssl_module \ --with-stream_realip_module \ --with-stream_geoip_module=dynamic \ --with-stream_ssl_preread_module \ --with-compat \ --with-pcre=../$&#123;PCRE&#125; \ --with-pcre-jit \ --with-zlib=../$&#123;ZLIB&#125; \ --with-openssl=../$&#123;OPENSSL&#125;\ --with-openssl-opt=no-nextprotoneg \ --with-debug &amp;&amp; make -j 4 &amp;&amp; make installEXPOSE 80CMD ["nginx","-g","daemon off;"] 1234# Dockerfile构建镜像$ docker build -t nginx:v1 .# 使用nginx:v1镜像运行容器$ docker run --name mynginx1 -p 80:80 -d -v depend:/data/depend -v wwwroot:/data/wwwroot -v wwwlogs:/data/wwwlogs nginx:v1 上面例子中，我们通过官方centos7的镜像源代码安装nginx1.16,并且编译诺干nginx模块，其中做了三个卷，分别存放编译安装依赖的库文件、日志文件目录以及项目静态文件。当然nginx配置文件没有挂载出来，如果你需要也可以挂载出来，但是nginx配置文件修改都得reload服务，甚至重启容器。如果站在容器有生命周期的角度来看的话，配置文件是可以不用挂载出来的，如果你修改了配置文件可以重新run一个新的容器，通过ENTRYPOINT用脚本的方式把配置文件模版写入到容器nginx对应的位置，每当需要修改已有配置的时候可以使用docker run --env来指定配置文件中需要修改的变量从而生成新的nginx容器，以后服务的配置文件修改通过修改编排工具模版实现容器重新生成和销毁。我们不直接接触容器本身。]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS7 安装Docker-CE]]></title>
    <url>%2F2018%2F03%2F21%2Finstall-docker-ce.html</url>
    <content type="text"><![CDATA[CentOS7 安装Docker-CE从2017年3月开始 docker 在原来的基础上分为两个分支版本: Docker-CE 和 Docker-EE。Docker-CE 即社区免费版，Docker-EE 即企业版，强调安全，但需付费使用。本文介绍 Docker-ce的安装使用。 移除旧的版本12345678910$ sudo yum remove docker \ docker-client \ docker-client-latest \ docker-common \ docker-latest \ docker-latest-logrotate \ docker-logrotate \ docker-selinux \ docker-engine-selinux \ docker-engine 安装一些必要的系统工具1$ sudo yum install -y yum-utils device-mapper-persistent-data lvm2 添加软件源信息1$ sudo yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 更新 yum 缓存1$ sudo yum makecache fast 查看可用版本的 Docker-CE1$ yum list docker-ce --showduplicates | sort -r 注意：如果需要只显示table版本，可以关闭测试版本的list 12$ sudo yum-config-manager --enable docker-ce-edge$ sudo yum-config-manager --enable docker-ce-test 更新yum包索引1$ yum makecache fast 安装指定版本的docker-CE1$ sudo yum install -y docker-ce-17.03.2.ce-1.el7.centos 报错：如果在安装指定版本的docker时显示需要安装指定版本的docker-ce-selinux依赖包，请安装： 1$ yum install -y https://download.docker.com/linux/centos/7/x86_64/stable/Packages/docker-ce-selinux-17.03.2.ce-1.el7.centos.noarch.rpm]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用的 curl 和 wget 命令]]></title>
    <url>%2F2018%2F03%2F21%2Fwget-curl.html</url>
    <content type="text"><![CDATA[常用的curl命令1234567891011121314151617181920212223242526272829303132333435-o 将请求地址的输出保持到文件-i 显示请求头和响应内容-I 只显示请求头-L 响应显示重定向跳转后的内容-w 格式输出，如%&#123;time_total&#125;, %&#123;http_code&#125;等-s 静默输出，不显示进度-# 显示下载进度-X 指定GET、PUT、DELETE、POST等请求指令-d 传入数据，默认配合-X POST指令-u 输入服务的用户名和密码，冒号隔开-U 使用http反向代理的地址请求网页-v 显示请求头、响应头以及响应内容信息-c 访问页面后保存页面的cookies-b 从文件中读取cookies访问页面-T 上传文件到目标地址，比如FTP-r 指定范围分块下载，如 -r 0-100 -o ，-r 101-200 -o，-r 200 - -o-k 允许在没有证书的情况下连接SSL站点，不对服务器的证书进行检查-E/--cert 使用客户端证书来发起请求 常用的wget命令1234567891011121314151617181920212223242526272829303132333435-O 指定下载的文件名--limit-rate 限制下载速度，如--limit-rate=300K-c 下载断点续传-b 后台下载，使用tail -f wget-log 查看下载进度-q 安静模式-v 显示详细的信息-nv 关闭详尽输出，但不进入安静模式-nc 不要重复下载已经存在的文件--spider 测试远程文件是否存在-t/--tries 设置下载重试的次数，默认0无限次--http-user=USER 设置http用户名为USER--http-password=PASS 设置tttp密码为PASS--proxy-user=USER 使用USER作为代理用户名--proxy-password=PASS 使用PASS作为代理密码--post-data=STRING 使用POST方式把STRING作为数据发送--post-file=FILE 使用POST方式发送FILE里的内容-O - -q 标准输出请求内容-O- 不保存标准输出内容，直接用管道符运行标准输出]]></content>
      <categories>
        <category>shell</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Shell 特殊变量和字符串截取]]></title>
    <url>%2F2018%2F03%2F11%2Fshell-string.html</url>
    <content type="text"><![CDATA[Shell 特殊变量 ID 表达式 含义 举例 结果 1 $@ 依次列出脚本参数的值 sh test.sh a b c d “a” “b” “c” “d” 2 $# 传递到脚本的参数个数 sh test.sh a b c d 4 3 $* 以一个单字符串显示所有向脚本传递的参数 sh test.sh a b c d “a b c d” 4 $! 后台运行的最后一个进程的ID号 sh t.sh $! 15797 5 $- 显示Shell使用的当前选项，与set命令功能相同 sh t.sh &amp;&amp; echo $- himBH 6 $? 显示命令的退出状态。0表示没有错误，其他为有错误 sh t.sh &amp;&amp; echo $? 0 7 $$ 脚本运行的当前进程ID号 sh t.sh $$ 12738 Shell字符串截取以下举例varible的值等于my_name_is_pyker。 varible=my_name_is_pyker ID 表达式 含义 举例 结果 1 ${varible##*string} 从左向右截取最后一个string后的字符串 echo ${varible##*_} pyker 2 ${varible#*string} 从左向右截取第一个string后的字符串 echo ${varible#*_} name_is_pyker 3 ${varible%%string*} 从右向左截取最后一个string后的字符串 echo ${varible%%_*} my 4 ${varible%string*} 从右向左截取第一个string后的字符串 echo ${varible%_*} my_name_is 5 ${varibleg:position} 在$varible中, 从位置$position开始提取子串 echo ${varible:5} me_is_pyker 6 ${varible:position:length} 在$varible中, 从位置$position开始提取长度为$length的子串 echo ${varible:5:4} me_i 7 ${varible#substring} 从变量$varible的开头, 删除最短匹配$substring的子串 echo ${varible#my_} name_is_pyker 8 ${varible##substring} 从变量$varible的开头, 删除最长匹配$substring的子串 echo ${varible##my_} name_is_pyker 9 ${varible%substring} 从变量$varible的结尾, 删除最短匹配$substring的子串 echo ${varible%pyker} my_name_is_ 10 ${varible%%substring} 从变量$varible的结尾, 删除最长匹配$substring的子串 echo ${varible%%pyker} my_name_is_ 11 ${varible/substring/replacement} 使用$replacement, 来代替第一个匹配的$substring echo ${varible/_/-} my-name_is_pyker 12 ${varible//substring/replacement} 使用$replacement, 代替所有匹配的$substring echo ${varible//_/-} my-name-is-pyker 13 ${varible/#substring/replacement} 如果$varible的前缀匹配$substring, 则$replacement替换$substring echo ${varible/#my_/the-} the-name_is_pyker 14 ${varible/%substring/replacement} 如果$varible的后缀匹配$substring, 则$replacement替换$substring echo ${varible/%pyker/leo} my_name_is_leo Shell 特殊变量赋值以下举例var的值等于demo，var=demo。 var1没有定义。 ID 表达式 含义 举例 结果 1 ${var:-DEFAULT} 如果var没有被声明, 或者其值为空, 那么就以$DEFAULT作为其值 echo ${var:-value}echo ${var1:-value} demovalue 2 ${var:+OTHER} 如果var被设置了, 那么其值就是$OTHER, 否则就为null字符串 echo ${var:+value}echo ${var1:+value} value “ “ 3 ${var:?ERR_MSG} 如果var没被声明, 那么就打印$ERR_MSG echo ${var:?ERR_MSG}echo ${var1:?ERR_MSG} demo-bash: var1: ERR_MSG 4 ${!var*} 匹配之前所有以var开头进行声明的变量 echo ${!var*} var varible]]></content>
      <categories>
        <category>shell</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx内置变量]]></title>
    <url>%2F2018%2F01%2F23%2Fnginx-variable.html</url>
    <content type="text"><![CDATA[Nginx 内置变量解释为了方便配置和使用nginx，nginx核心模块ngx_http_core_module自带有许多内置的人性化变量，这极大的方便了系统管理员对nginx维护和管理。下面我们详解注解内置变量的含义（当然，如果你懂点HTTP知识的话，就更好理解了）：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263$args #请求中的参数值$query_string #同 $args$arg_NAME #GET请求中NAME的值$is_args #如果请求中有参数，值为"?"，否则为空字符串$uri #请求中的当前URI(不带请求参数，参数位于$args)，可以不同于浏览器传递的$request_uri的值，它可以通过内部重定向，或者使用index指令进行修改，$uri不包含主机名，如"/foo/bar.html"。$document_uri #同 $uri$document_root #当前请求的文档根目录或别名$host #优先级：HTTP请求行的主机名&gt;"HOST"请求头字段&gt;符合请求的服务器名$hostname #主机名$https #如果开启了SSL安全模式，值为"on"，否则为空字符串。$binary_remote_addr #客户端地址的二进制形式，固定长度为4个字节$body_bytes_sent #传输给客户端的字节数，响应头不计算在内；这个变量和Apache的mod_log_config模块中的"%B"参数保持兼容$bytes_sent #传输给客户端的字节数$connection #TCP连接的序列号$connection_requests #TCP连接当前的请求数量$content_length #"Content-Length" 请求头字段$content_type #"Content-Type" 请求头字段$cookie_name #cookie名称$limit_rate #用于设置响应的速度限制$msec #当前的Unix时间戳$nginx_version #nginx版本$pid #工作进程的PID$pipe #如果请求来自管道通信，值为"p"，否则为"."$proxy_protocol_addr #获取代理访问服务器的客户端地址，如果是直接访问，该值为空字符串$realpath_root #当前请求的文档根目录或别名的真实路径，会将所有符号连接转换为真实路径$remote_addr #客户端地址$remote_port #客户端端口$remote_user #用于HTTP基础认证服务的用户名$request #代表客户端的请求地址$request_body #客户端的请求主体：此变量可在location中使用，将请求主体通过proxy_pass，fastcgi_pass，uwsgi_pass和scgi_pass传递给下一级的代理服务器$request_body_file #将客户端请求主体保存在临时文件中。文件处理结束后，此文件需删除。如果需要之一开启此功能，需要设置client_body_in_file_only。如果将次文件传递给后端的代理服务器，需要禁用request body，即设置proxy_pass_request_body off，fastcgi_pass_request_body off，uwsgi_pass_request_body off，or scgi_pass_request_body off$request_completion #如果请求成功，值为"OK"，如果请求未完成或者请求不是一个范围请求的最后一部分，则为空$request_filename #当前连接请求的文件路径，由root或alias指令与URI请求生成$request_length #请求的长度 (包括请求的地址，http请求头和请求主体)$request_method #HTTP请求方法，通常为"GET"或"POST"$request_time #处理客户端请求使用的时间; 从读取客户端的第一个字节开始计时$request_uri #这个变量等于包含一些客户端请求参数的原始URI，它无法修改，请查看$uri更改或重写URI，不包含主机名，例如："/cnphp/test.php?arg=freemouse"$scheme #请求使用的Web协议，"http" 或 "https"$server_addr #服务器端地址，需要注意的是：为了避免访问linux系统内核，应将ip地址提前设置在配置文件中$server_name #服务器名$server_port #服务器端口$server_protocol #服务器的HTTP版本，通常为 "HTTP/1.0" 或 "HTTP/1.1"$status #HTTP响应代码$time_iso8601 #服务器时间的ISO 8610格式$time_local #服务器时间（LOG Format 格式）$cookie_NAME #客户端请求Header头中的cookie变量，前缀"$cookie_"加上cookie名称的变量，该变量的值即为cookie名称的值$http_NAME #匹配任意请求头字段；变量名中的后半部分NAME可以替换成任意请求头字段，如在配置文件中需要获取http请求头："Accept-Language"，$http_accept_language即可$http_cookie$http_post$http_referer #url跳转来源 （https://www.baidu.com/）$http_user_agent #用户终端浏览器等信息 （"Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0; SV1; GTB7.0; .NET4.0C;）$http_x_forwarded_for #获取到最原始用户IP，或者代理IP地址。$sent_http_NAME #可以设置任意http响应头字段；变量名中的后半部分NAME可以替换成任意响应头字段，如需要设置响应头Content-length，$sent_http_content_length即可$upstream_response_time #请求过程中，upstream响应时间（0.002）$upstream_addr #后台upstream的地址，即真正提供服务的主机地址 （10.10.10.100:80）$upstream_status #upstream状态 （200）$sent_http_cache_control$sent_http_connection$sent_http_content_type$sent_http_keep_alive$sent_http_last_modified$sent_http_location$sent_http_transfer_encoding 这些变量可以在配置文件中使用，方便你做各种nginx页面代理，转换，重写，重定向等操作；而且还可以对nginx日志做自定义的日志配置，方便你对nginx日志的收集和分析。如常用的nginx日志格式：123log_format main '$remote_addr $remote_user [$time_local] "$request" ' '$status $body_bytes_sent "$http_referer" ' '$http_user_agent $http_x_forwarded_for $request_time $upstream_response_time $upstream_addr $upstream_status'; 日志截取如下（可以从日志中看到代理到后端哪台机器上的哪个端口上，负载访问的状态值等都能看到）：123456[root@nginx1 logs]# tail -f /data/wwwlogs/access.log...110.156.114.121 - [11/Aug/2017:09:57:19 +0800] "GET /rest/mywork/latest/status/notification/count?_=1502416641768 HTTP/1.1" 200 67 "http://wiki.wang-inc.com/pages/viewpage.action?pageId=11174759" Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.78 Safari/537.36 - 0.006 0.006 12.129.120.121:8090 200]]></content>
      <categories>
        <category>web</category>
      </categories>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[详解nginx之location误区]]></title>
    <url>%2F2018%2F01%2F23%2FNginx-location.html</url>
    <content type="text"><![CDATA[location 的匹配顺序是“先匹配正则，再匹配普通”。矫正： location 的匹配顺序其实是“先匹配普通，再匹配正则”。我这么说，大家一定会反驳我，因为按“先匹配普通，再匹配正则”解释不了大家平时习惯的按“先匹配正则，再匹配普通”的实践经验。这里我只能暂时解释下，造成这种误解的原因是：正则匹配会覆盖普通匹配（实际的规则，比这复杂，后面会详细解释）。 location 的执行逻辑跟 location 的编辑顺序无关。矫正：这句话不全对，“普通 location ”的匹配规则是“最大前缀”，因此“普通 location ”的确与 location 编辑顺序无关；但是“正则 location ”的匹配规则是“顺序匹配，且只要匹配到第一个就停止后面的匹配”；“普通location ”与“正则 location ”之间的匹配顺序是？先匹配普通 location ，再“考虑”匹配正则 location 。注意这里的“考虑”是“可能”的意思，也就是说匹配完“普通 location ”后，有的时候需要继续匹配“正则 location ”，有的时候则不需要继续匹配“正则 location ”。两种情况下，不需要继续匹配正则 location ：（ 1 ）当普通 location 前面指定了“ ^~ ”，特别告诉 Nginx 本条普通 location 一旦匹配上，则不需要继续正则匹配；（ 2 ）当普通location 恰好严格匹配上，不是最大前缀匹配，则不再继续匹配正则。 总结一句话： “正则 location 匹配让步普通 location 的严格精确匹配结果；但覆盖普通 location 的最大前缀匹配结果” 匹配优先级为： (location =) &gt; (location 完整路径) &gt; (location ^~ 路径) &gt; (location ~,~* 正则顺序) &gt; (location 部分起始路径) &gt; (/) 官方文档解释：http://wiki.nginx.org/NginxHttpCoreModule#location1234locationsyntax: `location [=|~|~*|^~|@] /uri/ &#123; … &#125;`default: nocontext: server This directive allows different configurations depending on the URI. （译者注：1 、different configurations depending on the URI 说的就是语法格式：location [=|~|~*|^~|@] /uri/ { … } ，依据不同的前缀“= ”，“^~ ”，“~ ”，“~* ”和不带任何前缀的（因为[A] 表示可选，可以不要的），表达不同的含义, 简单的说尽管location 的/uri/ 配置一样，但前缀不一样，表达的是不同的指令含义。2 、查询字符串不在URI范围内。例如：/films.htm?fid=123 的URI 是/films.htm 。）It can be configured using both literal strings and regular expressions. To use regular expressions, you must use a prefix:“~”for case sensitive matching“~*”for case insensitive matching译文：上文讲到location /uri/ 可通过使用不同的前缀，表达不同的含义。对这些不同前缀，分下类，就2 大类：正则location ，英文说法是location using regular expressions 和普通location ，英文说法是location using literal strings 。那么其中“~ ”和“~ ”前缀表示正则location ，“~ ”区分大小写，“~ ”不区分大小写；其他前缀（包括：“=”，“^~ ”和“@ ”）和无任何前缀的都属于普通location 。 To determine which location directive matches a particular query, the literal strings are checked first. 译文：对于一个特定的 HTTP 请求（ a particular query ）， nginx 应该匹配哪个 location 块的指令呢（注意：我们在 nginx.conf 配置文件里面一般会定义多个 location 的）？匹配 规则是：先匹配普通location （再匹配正则表达式）。注意：官方文档这句话就明确说了，先普通location ，而不是有些同学的误区“先匹配正则location ”。 Literal strings match the beginning portion of the query – the most specific match will be used. 前面说了“普通location ”与“正则location ”之间的匹配规则是：先匹配普通location ，再匹配正则location 。那么，“普通location ”内部（普通location 与普通location ）是如何匹配的呢？简单的说：最大前缀匹配。原文：1、match the beginning portion of the query （说的是匹配URI 的前缀部分beginning portion ）； 2 、the most specific match will be used （因为location 不是“严格匹配”，而是“前缀匹配”，就会产生一个HTTP 请求，可以“前缀匹配”到多个普通location ，例如：location /prefix/mid/ {} 和location /prefix/ {} ，对于HTTP 请求/prefix/mid/t.html ，前缀匹配的话两个location 都满足，选哪个？原则是：the most specific match ，于是选的是location /prefix/mid/ {} ）。 Afterwards, regular expressions are checked in the order defined in the configuration file. The first regular expression to match the query will stop the search. 这段话说了两层意思，第一层是：“Afterwards, regular expressions are checked ”, 意思是普通location 先匹配，而且选择了最大前缀匹配后，就不能停止后面的匹配，最大前缀匹配只是一个临时的结果，nginx 还需要继续检查正则location （但至于最终是和普通location 的最大前缀匹配，还是正则location 的匹配，截止当前的内容还没讲，但后面会讲）。第二层是“regular expressions are checked in the order defined in the configuration file. The first regular expression to match the query will stop the search. ”，意思是说“正则location ”与“正则location”内部的匹配规则是：按照正则location 在配置文件中的物理顺序（编辑顺序）匹配的（这句话就说明location 并不是一定跟顺序无关，只是普通location 与顺序无关，正则location 还是与顺序有关的），并且只要匹配到一条正则location ，就不再考虑后面的（这与“普通location ”与“正则location ”之间的规则不一样，“普通location ”与“正则location ”之间的规则是：选择出“普通location ”的最大前缀匹配结果后，还需要继续搜索正则location ）。 If no regular expression matches are found, the result from the literal string search is used. 这句话回答了“普通location ”的最大前缀匹配结果与继续搜索的“正则location ”匹配结果的决策关系。如果继续搜索的“正则location ”也有匹配上的，那么“正则location ”覆盖 “普通location ”的最大前缀匹配（因为有这个覆盖关系，所以造成有些同学以为正则location 先于普通location 执行的错误理解）；但是如果“正则location ”没有能匹配上，那么就用“普通location ”的最大前缀匹配结果。For case insensitive operating systems, like Mac OS X or Windows with Cygwin, literal string matching is done in a case insensitive way (0.7.7). However, comparison is limited to single-byte locale’s only.Regular expression may contain captures (0.7.40), which can then be used in other directives. It is possible to disable regular expression checks after literal string matching by using “^~” prefix.If the most specific match literal location has this prefix: regular expressions aren’t checked. 通常的规则是，匹配完了“普通location ”指令，还需要继续匹配“正则location ”，但是你也可以告诉Nginx ：匹配到了“普通location ”后，不再需要继续匹配“正则location ”了，要做到这一点只要在“普通location ”前面加上“^~ ”符号（^ 表示“非”，~ 表示“正则”，字符意思是：不要继续匹配正则）。 By using the “=” prefix we define the exact match between request URI and location. When matched search stops immediately. E.g., if the request “/” occurs frequently, using “location = /” will speed up processing of this request a bit as search will stop after first comparison. 除了上文的“^~ ”可以阻止继续搜索正则location 外，你还可以加“= ”。那么如果“^~ ”和“= ”都能阻止继续搜索正则location 的话，那它们之间有什么区别呢？区别很简单，共同点是它们都能阻止继续搜索正则location ，不同点是“^~ ”依然遵守“最大前缀”匹配规则，然而“= ”不是“最大前缀”，而是必须是严格匹配（exact match ）。这里顺便讲下“location / {} ”和“location = / {} ”的区别，“location / {} ”遵守普通location 的最大前缀匹配，由于任何URI 都必然以“/ ”根开头，所以对于一个URI ，如果有更specific 的匹配，那自然是选这个更specific 的，如果没有，“/ ”一定能为这个URI 垫背（至少能匹配到“/ ”），也就是说“location / {} ”有点默认配置的味道，其他更specific的配置能覆盖overwrite 这个默认配置（这也是为什么我们总能看到location / {} 这个配置的一个很重要的原因）。而“location = / {} ”遵守的是“严格精确匹配exact match ”，也就是只能匹配 http://host:port/ 请求，同时会禁止继续搜索正则location 。因此如果我们只想对“GET / ”请求配置作用指令，那么我们可以选“location = / {} ”这样能减少正则location 的搜索，因此效率比“location / {}” 高（注：前提是我们的目的仅仅只想对“GET / ”起作用）。 On exact match with literal location without “=” or “^~” prefixes search is also immediately terminated. 前面我们说了，普通location 匹配完后，还会继续匹配正则location ；但是nginx 允许你阻止这种行为，方法很简单，只需要在普通location 前加“^~ ”或“= ”。但其实还有一种“隐含”的方式来阻止正则location 的搜索，这种隐含的方式就是：当“最大前缀”匹配恰好就是一个“严格精确（exact match ）”匹配，照样会停止后面的搜索。原文字面意思是：只要遇到“精确匹配exact match ”，即使普通location 没有带“= ”或“^~ ”前缀，也一样会终止后面的匹配。 先举例解释下，后面例题会用实践告诉大家。假设当前配置是：location /exact/match/test.html { 配置指令块1}，location /prefix/ { 配置指令块2} 和 location ~ .html$ { 配置指令块3} ，如果我们请求 GET /prefix/index.html ，则会被匹配到指令块3 ，因为普通location /prefix/ 依据最大匹配原则能匹配当前请求，但是会被后面的正则location 覆盖；当请求GET /exact/match/test.html ，会匹配到指令块1 ，因为这个是普通location 的exact match ，会禁止继续搜索正则location 。 To summarize, the order in which directives are checked is as follows: Directives with the “=” prefix that match the query exactly. If found, searching stops. All remaining directives with conventional strings. If this match used the “^~” prefix, searching stops. Regular expressions, in the order they are defined in the configuration file. If #3 yielded a match, that result is used. Otherwise, the match from #2 is used. 这个顺序没必要再过多解释了。但我想用自己的话概括下上面的意思“正则 location 匹配让步普通location 的严格精确匹配结果；但覆盖普通 location 的最大前缀匹配结果”。 It is important to know that nginx does the comparison against decoded URIs. For example, if you wish to match “/images/ /test”, then you must use “/images/ /test” to determine the location. 在浏览器上显示的URL 一般都会进行URLEncode ，例如“空格”会被编码为 ，但是Nginx 的URL 的匹配都是针对URLDecode 之后的。也就是说，如果你要匹配“/images/ /test ”，你写location 的时候匹配目标应该是：“/images/ /test ”。 Example:123456789101112131415161718192021222324location = / &#123; # matches the query / only. [ configuration A ]&#125;location / &#123; # matches any query, since all queries begin with /, but regular # expressions and any longer conventional blocks will be # matched first. [ configuration B ]&#125;location ^~ /images/ &#123; # matches any query beginning with /images/ and halts searching, # so regular expressions will not be checked. [ configuration C ]&#125;location ~* \.(gif|jpg|jpeg)$ &#123; # matches any request ending in gif, jpg, or jpeg. However, all # requests to the /images/ directory will be handled by # Configuration C. [ configuration D ]&#125; 上述这4 个location 的配置，没什么好解释的，唯一需要说明的是location / {[configuration B]} ，原文的注释严格来说是错误的，但我相信原文作者是了解规则的，只是文字描述上简化了下，但这个简化容易给读者造成“误解：先检查正则location ，再检查普通location ”。原文：“matches any query, since all queries begin with /, butregular expressions and any longer conventional blocks will be matched first. ”大意是说：“location / {} 能够匹配所有HTTP 请求，因为任何HTTP 请求都必然是以‘/ ’开始的（这半句没有错误）。但是，正则location 和其他任何比‘/ ’更长的普通location （location / {} 是普通location 里面最短的，因此其他任何普通location 都会比它更长，当然location = / {} 和 location ^~ / {} 是一样长的）会优先匹配（matched first ）。” 原文作者说“ but regular expressions will be matched first. ”应该只是想说正则 location 会覆盖这里的 location / {} ，但依然是普通location / {} 先于正则 location 匹配，接着再正则 location 匹配；但其他更长的普通 location （ any longer conventional blocks ）的确会先于 location / {} 匹配。 Example requests: / -&gt; configuration A /documents/document.html -&gt; configuration B /images/1.gif -&gt; configuration C /documents/1.jpg -&gt; configuration D Note that you could define these 4 configurations in any order and the results would remain the same. 需要提醒下：这里说“in any order ”和“… remain the same ”是因为上面只有一个正则location 。文章前面已经说了正则location 的匹配是跟编辑顺序有关系的。 While nested locations are allowed by the configuration file parser, their use is discouraged and may produce unexpected results. 实际上 nginx 的配置文件解析程序是允许 location 嵌套定义的（ location / { location /uri/ {} } ）。但是我们平时却很少看见这样的配置，那是因为 nginx 官方并不建议大家这么做，因为这样会导致很多意想不到的后果。 The prefix “@” specifies a named location. Such locations are not used during normal processing of requests, they are intended only to process internally redirected requests (see error_page ,try_files ). 文章开始说了location 的语法中，可以有“= ”，“^~ ”，“~ ”和“~* ”前缀，或者干脆没有任何前缀，还有“@ ”前缀，但是后面的分析我们始终没有谈到“@ ”前缀。文章最后点内容，介绍了“＠”的用途：“@ ”是用来定义“Named Location ”的（你可以理解为独立于“普通location （location using literal strings ）”和“正则location （location using regular expressions ）”之外的第三种类型），这种“Named Location ”不是用来处理普通的HTTP 请求的，它是专门用来处理“内部重定向（internally redirected ）”请求的。注意：这里说的“内部重定向（internally redirected ）”或许说成“forward ”会好点，以为内internally redirected 是不需要跟浏览器交互的，纯粹是服务端的一个转发行为。 location 实例练习Nginx 的语法形式是：location [=|~|~*|^~|@] /uri/ { … }，意思是可以以“ = ”或“ ~* ”或“ ~ ”或“ ^~ ”或“ @ ”符号为前缀，当然也可以没有前缀（因为 [A] 是表示可选的 A ； A|B 表示 A 和 B 选一个），紧接着是 /uri/ ，再接着是{…} 指令块，整个意思是对于满足这样条件的 /uri/ 适用指令块 {…} 的指令。 上述各种 location 可分两大类，分别是：“普通 location ”，官方英文说法是 location using literal strings 和“正则 location ”，英文说法是 location using regular expressions 。其中“普通 location ”是以“ = ”或“ ^~ ”为前缀或者没有任何前缀的 /uri/ ；“正则 location ”是以“ ~ ”或“ ~* ”为前缀的 /uri/ 。那么，当我们在一个 server 上下文编写了多个 location 的时候， Nginx 对于一个 HTTP 请求，是如何匹配到一个 location 做处理呢？用一句话简单概括 Nginx 的 location 匹配规则是：“正则 location ”让步 “普通 location”的严格精确匹配结果；但覆盖 “普通 location ”的最大前缀匹配结果。理解这句话，我想通过下面的实例来说明。 先普通 location ，再正则 location周边不少童鞋告诉我， nginx 是“先匹配正则 location 再匹配普通 location ”，其实这是一个误区， nginx 其实是“先匹配普通 location ，再匹配正则 location ”，但是普通 location 的匹配结果又分两种：一种是“严格精确匹配”，官方英文说法是“ exact match ”；另一种是“最大前缀匹配”，官方英文说法是“ Literal strings match the beginning portion of the query – the most specific match will be used. ”。我们做个实验： 例题 1 ：假设 nginx 的配置如下123456789101112server &#123; listen 9090; server_name localhost; location / &#123; root html; index index.html index.htm; deny all; &#125; location ~ \.html$ &#123; allow all; &#125;&#125; 附录 nginx 的目录结构是： nginx-&gt;html-&gt;index.html上述配置的意思是： location / {… deny all;} 普通 location 以“ / ”开始的 URI 请求（注意任何 HTTP 请求都必然以“/ ”开始，所以“ / ”的意思是所有的请求都能被匹配上），都拒绝访问； location ~.html$ {allow all;} 正则 location以 .html 结尾的 URI 请求，都允许访问。 测试结果：123456789101112131415161718192021222324252627[root@web108 ~]# curl http://localhost:9090/&lt;html&gt;&lt;head&gt;&lt;title&gt;403 Forbidden&lt;/title&gt;&lt;/head&gt;&lt;body bgcolor=”white”&gt;&lt;center&gt;&lt;h1&gt;403 Forbidden&lt;/h1&gt;&lt;/center&gt;&lt;hr&gt;&lt;center&gt;nginx/1.1.0&lt;/center&gt;&lt;/body&gt;&lt;/html&gt;[root@web108 ~]# curl http://localhost:9090/index.html&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;&lt;/head&gt;&lt;body bgcolor=”white” text=”black”&gt;&lt;center&gt;&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;&lt;/center&gt;&lt;/body&gt;&lt;/html&gt;[root@web108 ~]# curl http://localhost:9090/index_notfound.html&lt;html&gt;&lt;head&gt;&lt;title&gt;404 Not Found&lt;/title&gt;&lt;/head&gt;&lt;body bgcolor=”white”&gt;&lt;center&gt;&lt;h1&gt;404 Not Found&lt;/h1&gt;&lt;/center&gt;&lt;hr&gt;&lt;center&gt;nginx/1.1.0&lt;/center&gt;&lt;/body&gt;&lt;/html&gt; 测试结果如下： URI请求 HTTP响应 curl http://localhost:9090/ 403 Forbidden curl http://localhost:9090/index.html Welcome to nginx! curl http://localhost:9090/index_notfound.html 404 Not Found curl http://localhost:9090/ 的结果是“ 403 Forbidden ”，说明被匹配到“ location / {..deny all;} ”了，原因很简单HTTP 请求 GET / 被“严格精确”匹配到了普通 location / {} ，则会停止搜索正则 location ； curl http://localhost:9090/index.html 结果是“ Welcome to nginx! ”，说明没有被“ location / {…deny all;} ”匹配，否则会 403 Forbidden ，但 /index.html 的确也是以“ / ”开头的，只不过此时的普通 location / 的匹配结果是“最大前缀”匹配，所以 Nginx 会继续搜索正则 location ， location ~ .html$ 表达了以 .html 结尾的都 allow all; 于是接着就访问到了实际存在的 index.html 页面。 curl http://localhost:9090/index_notfound.html 同样的道理先匹配 location / {} ，但属于“普通 location 的最大前缀匹配”，于是后面被“正则 location ” location ~ .html$ {} 覆盖了，最终 allow all ； 但的确目录下不存在index_notfound.html 页面，于是 404 Not Found 。 如果此时我们访问 http://localhost:9090/index.txt 会是什么结果呢？显然是 deny all ；因为先匹配上了 location / {..deny all;} 尽管属于“普通 location ”的最大前缀匹配结果，继续搜索正则 location ，但是 /index.txt 不是以 .html结尾的，正则 location 失败，最终采纳普通 location 的最大前缀匹配结果，于是 deny all 了。12345678[root@web108 ~]# curl http://localhost:9090/index.txt&lt;html&gt;&lt;head&gt;&lt;title&gt;403 Forbidden&lt;/title&gt;&lt;/head&gt;&lt;body bgcolor=”white”&gt;&lt;center&gt;&lt;h1&gt;403 Forbidden&lt;/h1&gt;&lt;/center&gt;&lt;hr&gt;&lt;center&gt;nginx/1.1.0&lt;/center&gt;&lt;/body&gt;&lt;/html&gt; 普通 location 的“隐式”严格匹配 例题 2 ：我们在例题 1 的基础上增加精确配置 123456789101112131415server &#123; listen 9090; server_name localhost; location /exact/match.html &#123; allow all; &#125; location / &#123; root html; index index.html index.htm; deny all; &#125; location ~ \.html$ &#123; allow all; &#125;&#125; 测试请求：12345678[root@web108 ~]# curl http://localhost:9090/exact/match.html&lt;html&gt;&lt;head&gt;&lt;title&gt;404 Not Found&lt;/title&gt;&lt;/head&gt;&lt;body bgcolor=”white”&gt;&lt;center&gt;&lt;h1&gt;404 Not Found&lt;/h1&gt;&lt;/center&gt;&lt;hr&gt;&lt;center&gt;nginx/1.1.0&lt;/center&gt;&lt;/body&gt;&lt;/html&gt; 结果进一步验证了“普通 location ”的“严格精确”匹配会终止对正则 location 的搜索。这里我们小结下“普通 location”与“正则 location ”的匹配规则：先匹配普通 location ，再匹配正则 location ，但是如果普通 location 的匹配结果恰好是“严格精确（ exact match ）”的，则 nginx 不再尝试后面的正则 location ；如果普通 location 的匹配结果是“最大前缀”，则正则 location 的匹配覆盖普通 location 的匹配。也就是前面说的“正则 location 让步普通location 的严格精确匹配结果，但覆盖普通 location 的最大前缀匹配结果”。 普通 location 的“显式”严格匹配和“ ^~ ” 前缀 上面我们演示的普通 location 都是不加任何前缀的，其实普通 location 也可以加前缀：“ ^~ ”和“ = ”。其中“ ^~”的意思是“非正则，不需要继续正则匹配”，也就是通常我们的普通 location ，还会继续搜索正则 location （恰好严格精确匹配除外），但是 nginx 很人性化允许配置人员告诉 nginx 某条普通 location ，无论最大前缀匹配，还是严格精确匹配都终止继续搜索正则 location ；而“ = ”则表达的是普通 location 不允许“最大前缀”匹配结果，必须严格等于，严格精确匹配。 例题 3 ：“ ^~ ”前缀的使用123456789101112131415server &#123; listen 9090; server_name localhost; location /exact/match.html &#123; allow all; &#125; location ^~ / &#123; root html; index index.html index.htm; deny all; &#125; location ~ \.html$ &#123; allow all; &#125;&#125; 把例题 2 中的 location / {} 修改成 location ^~ / {} ，再看看测试结果： URL请求 修改前 修改后 curl http://localhost:9090/ 403 Forbidden 403 Forbidden curl http://localhost:9090/index.html Welcome to nginx! 403 Forbidden curl http://localhost:9090/index_notfound.html 404 Not Found 403 Forbidden curl http://localhost:9090/exact/match.html 404 Not Found 404 Not Found 除了 GET /exact/match.html 是 404 Not Found ，其余都是 403 Forbidden ，原因很简单所有请求都是以“ / ”开头，所以所有请求都能匹配上“ / ”普通 location ，但普通 location 的匹配原则是“最大前缀”，所以只有/exact/match.html 匹配到 location /exact/match.html {allow all;} ，其余都 location ^~ / {deny all;} 并终止正则搜索。 例题 4 ：“ = ”前缀的使用123456789101112131415server &#123; listen 9090; server_name localhost; location /exact/match.html &#123; allow all; &#125; location = / &#123; root html; index index.html index.htm; deny all; &#125; location ~ \.html$ &#123; allow all; &#125;&#125; 例题 4 相对例题 2 把 location / {} 修改成了 location = / {} ，再次测试结果： URL请求 修改前 修改后 curl http://localhost:9090/ 403 Forbidden 403 Forbidden curl http://localhost:9090/index.html Welcome to nginx! Welcome to nginx! curl http://localhost:9090/index_notfound.html 404 Not Found 404 Not Found curl http://localhost:9090/exact/match.html 404 Not Found 404 Not Found curl http://localhost:9090/test.jsp 403 Forbidden 404 Not Found 最能说明问题的测试是 GET /test.jsp ，实际上 /test.jsp 没有匹配正则 location （ location ~.html$ ），也没有匹配 location = / {} ，如果按照 location / {} 的话，会“最大前缀”匹配到普通 location / {} ，结果是 deny all 。 正则 location 与编辑顺序location 的指令与编辑顺序无关，这句话不全对。对于普通 location 指令，匹配规则是：最大前缀匹配（与顺序无关），如果恰好是严格精确匹配结果或者加有前缀“ ^~ ”或“ = ”（符号“ = ”只能严格匹配，不能前缀匹配），则停止搜索正则 location ；但对于正则 location 的匹配规则是：按编辑顺序逐个匹配（与顺序有关），只要匹配上，就立即停止后面的搜索。 123456789101112131415161718192021222324配置 3.1server &#123; listen 9090; server_name localhost; location ~ \.html$ &#123; allow all; &#125; location ~ ^/prefix/.*\.html$ &#123; deny all; &#125; &#125;配置 3.2server &#123; listen 9090; server_name localhost; location ~ ^/prefix/.*\.html$ &#123; deny all; &#125; location ~ \.html$ &#123; allow all; &#125; &#125; 测试结果： URL请求 配置3.1 配置3.2 curl http://localhost:9090/regextest.html 404 Not Found 404 Not Found curl http://localhost:9090/prefix/regextest.html 404 Not Found 403 Forbidden 解释：Location ~ ^/prefix/.*\.html$ {deny all;} 表示正则 location 对于以 /prefix/ 开头， .html 结尾的所有 URI 请求，都拒绝访问； location ~\.html${allow all;} 表示正则 location 对于以 .html 结尾的 URI 请求，都允许访问。 实际上，prefix 的是 ~.html$ 的子集。 在“配置 3.1 ”下，两个请求都匹配上 location ~\.html$ {allow all;} ，并且停止后面的搜索，于是都允许访问， 404 Not Found ；在“配置 3.2 ”下， /regextest.html 无法匹配 prefix ，于是继续搜索 ~.html$ ，允许访问，于是 404 Not Found ；然而 /prefix/regextest.html 匹配到 prefix ，于是 deny all ， 403 Forbidden 。123456789101112131415161718192021222324配置 3.3server &#123; listen 9090; server_name localhost; location /prefix/ &#123; deny all; &#125; location /prefix/mid/ &#123; allow all; &#125; &#125;配置 3.4server &#123; listen 9090; server_name localhost; location /prefix/mid/ &#123; allow all; &#125; location /prefix/ &#123; deny all; &#125; &#125; 测试结果： URL请求 配置3.3 配置3.4 curl http://localhost:9090/prefix/t.html 403 Forbidden 403 Forbidden curl http://localhost:9090/prefix/mid/t.html 404 Not Found 404 Not Found 测试结果表明：普通 location 的匹配规则是“最大前缀”匹配，而且与编辑顺序无关。 “@” 前缀 Named Location 使用REFER: http://wiki.nginx.org/HttpCoreModule#error_page假设配置如下：123456789101112131415server &#123; listen 9090; server_name localhost; location / &#123; root html; index index.html index.htm; allow all; &#125; #error_page 404 http://www.baidu.com # 直接这样是不允许的 error_page 404 = @fallback; location @fallback &#123; proxy_pass http://www.baidu.com; &#125;&#125; 上述配置文件的意思是：如果请求的 URI 存在，则本 nginx 返回对应的页面；如果不存在，则把请求代理到baidu.com 上去做个弥补（注： nginx 当发现 URI 对应的页面不存在， HTTP_StatusCode 会是 404 ，此时error_page 404 指令能捕获它）。 测试一： 123456789101112131415161718[root@web108 ~]# curl http://localhost:9090/nofound.html -iHTTP/1.1 302 FoundServer: nginx/1.1.0Date: Sat, 06 Aug 2011 08:17:21 GMTContent-Type: text/html; charset=iso-8859-1Location: http://localhost:9090/search/error.htmlConnection: keep-aliveCache-Control: max-age=86400Expires: Sun, 07 Aug 2011 08:17:21 GMTContent-Length: 222&lt;!DOCTYPE HTML PUBLIC “-//IETF//DTD HTML 2.0//EN”&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;302 Found&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Found&lt;/h1&gt;&lt;p&gt;The document has moved &lt;a href=”http://www.baidu.com/search/error.html”&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt; 当我们 GET /nofound.html 发送给本 nginx ， nginx 找不到对应的页面，于是 error_page 404 = @fallback ，请求被代理到 http://www.baidu.com ，于是 nginx 给 http://www.baidu.com 发送了 GET /nofound.html ，但/nofound.html 页面在百度也不存在，百度 302 跳转到错误页。直接访问 http://www.baidu.com/nofound.html 结果： 123456789101112131415161718[root@web108 ~]# curl http://www.baidu.com/nofound.html -iHTTP/1.1 302 FoundDate: Sat, 06 Aug 2011 08:20:05 GMTServer: ApacheLocation: http://www.baidu.com/search/error.htmlCache-Control: max-age=86400Expires: Sun, 07 Aug 2011 08:20:05 GMTContent-Length: 222Connection: Keep-AliveContent-Type: text/html; charset=iso-8859-1 &lt;!DOCTYPE HTML PUBLIC “-//IETF//DTD HTML 2.0//EN”&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;302 Found&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Found&lt;/h1&gt;&lt;p&gt;The document has moved &lt;a href=”http://www.baidu.com/search/error.html”&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt; 测试二：访问一个 nginx 不存在，但 baidu 存在的页面1234567891011121314151617181920212223[root@web108 ~]# curl http://www.baidu.com/duty/ -iHTTP/1.1 200 OKDate: Sat, 06 Aug 2011 08:21:56 GMTServer: ApacheP3P: CP=” OTI DSP COR IVA OUR IND COM ”P3P: CP=” OTI DSP COR IVA OUR IND COM ”Set-Cookie: BAIDUID=5C5D2B2FD083737A0C88CA7075A6601A:FG=1; expires=Sun, 05-Aug-12 08:21:56 GMT; max-age=31536000; path=/; domain=.baidu.com; version=1Set-Cookie: BAIDUID=5C5D2B2FD083737A2337F78F909CCB90:FG=1; expires=Sun, 05-Aug-12 08:21:56 GMT; max-age=31536000; path=/; domain=.baidu.com; version=1Last-Modified: Wed, 05 Jan 2011 06:44:53 GMTETag: “d66-49913b8efe340″Accept-Ranges: bytesContent-Length: 3430Cache-Control: max-age=86400Expires: Sun, 07 Aug 2011 08:21:56 GMTVary: Accept-Encoding,User-AgentConnection: Keep-AliveContent-Type: text/html&lt;!DOCTYPE HTML PUBLIC “-//W3C//DTD HTML 4.01 Transitional//EN”“http://www.w3.org/TR/html4/loose.dtd”&gt;。。。。&lt;/body&gt;&lt;/html&gt; 显示，的确百度这个页面是存在的。 123456789101112131415161718192021222324[root@web108 ~]# curl http://localhost:9090/duty/ -iHTTP/1.1 200 OKServer: nginx/1.1.0Date: Sat, 06 Aug 2011 08:23:23 GMTContent-Type: text/htmlConnection: keep-aliveP3P: CP=” OTI DSP COR IVA OUR IND COM ”P3P: CP=” OTI DSP COR IVA OUR IND COM ”Set-Cookie: BAIDUID=8FEF0A3A2C31D277DCB4CC5F80B7F457:FG=1; expires=Sun, 05-Aug-12 08:23:23 GMT; max-age=31536000; path=/; domain=.baidu.com; version=1Set-Cookie: BAIDUID=8FEF0A3A2C31D277B1F87691AFFD7440:FG=1; expires=Sun, 05-Aug-12 08:23:23 GMT; max-age=31536000; path=/; domain=.baidu.com; version=1Last-Modified: Wed, 05 Jan 2011 06:44:53 GMTETag: “d66-49913b8efe340″Accept-Ranges: bytesContent-Length: 3430Cache-Control: max-age=86400Expires: Sun, 07 Aug 2011 08:23:23 GMTVary: Accept-Encoding,User-Agent&lt;!DOCTYPE HTML PUBLIC “-//W3C//DTD HTML 4.01 Transitional//EN”“http://www.w3.org/TR/html4/loose.dtd”&gt;&lt;html&gt;。。。&lt;/body&gt;&lt;/html&gt; 当 curl http://localhost:9090/duty/ -i 时， nginx 没找到对应的页面，于是 error_page = @fallback ，把请求代理到 baidu.com 。注意这里的 error_page = @fallback 不是靠重定向实现的，而是所说的“ internally redirected （forward ）”。 proxy_pass URL 末尾加与不加/（斜线）的区别 Proxy_pass末尾带”/”和不带是有区别的：不带斜杠转发的是除hostname以外的部分，包括目录。可以使用正则表达式匹配location，且任意正则匹配成功后，转发的都是完整目录路径。带斜杠转发的是除hostname及目录外的所有部分。不能使用正则表达式匹配location块，只能使用完整路径名准确匹配。 1234配置1location /tobaidu &#123; proxy_pass http://127.0.0.1:8087;&#125; 测试结果： 请求URL 请求结果 curl http://127.0.0.1/tobaidu http://127.0.0.1:8087/tobaidu curl http://127.0.0.1/tobaidu/ http://127.0.0.1:8087/tobaidu/ curl http://127.0.0.1/tobaidu/xxxx http://127.0.0.1:8087/tobaidu/xxxx 1234配置2location /tobaidu &#123; proxy_pass http://127.0.0.1:8087/define;&#125; 测试结果： 请求URL 请求结果 curl http://127.0.0.1/tobaidu http://127.0.0.1:8087/define curl http://127.0.0.1/tobaidu/ http://127.0.0.1:8087/define/ curl http://127.0.0.1/tobaidu/xxxx http://127.0.0.1:8087/define/xxxx 1234配置3location /tobaidu/ &#123; proxy_pass http://127.0.0.1:8087;&#125; 测试结果： 请求URL 请求结果 curl http://127.0.0.1/tobaidu 重定向到http://127.0.0.1/tobaidu/ curl http://127.0.0.1/tobaidu/ http://127.0.0.1:8087/tobaidu/ curl http://127.0.0.1/tobaidu/xxxx http://127.0.0.1:8087/tobaidu/xxxx 1234配置4location /tobaidu/ &#123; proxy_pass http://127.0.0.1:8087/define;&#125; 测试结果： 请求URL 请求结果 curl http://127.0.0.1/tobaidu 重定向到http://127.0.0.1/tobaidu/ curl http://127.0.0.1/tobaidu/ http://127.0.0.1:8087/define curl http://127.0.0.1/tobaidu/xxxx http://127.0.0.1:8087/define/xxxx 1234配置5location /tobaidu &#123; proxy_pass http://127.0.0.1:8087/;&#125; 测试结果： 请求URL 请求结果 curl http://127.0.0.1/tobaidu http://127.0.0.1:8087/ curl http://127.0.0.1/tobaidu/ http://127.0.0.1:8087// curl http://127.0.0.1/tobaidu/xxxx http://127.0.0.1:8087//xxxx 1234配置6location /tobaidu &#123; proxy_pass http://127.0.0.1:8087/define/;&#125; 测试结果： 请求URL 请求结果 curl http://127.0.0.1/tobaidu http://127.0.0.1:8087/define/ curl http://127.0.0.1/tobaidu/ http://127.0.0.1:8087/define// curl http://127.0.0.1/tobaidu/xxxx http://127.0.0.1:8087/define//xxxx 1234配置7location /tobaidu/ &#123; proxy_pass http://127.0.0.1:8087/;&#125; 测试结果： 请求URL 请求结果 curl http://127.0.0.1/tobaidu 重定向到http://127.0.0.1/tobaidu/ curl http://127.0.0.1/tobaidu/ http://127.0.0.1:8087/ curl http://127.0.0.1/tobaidu/xxxx http://127.0.0.1:8087/xxxx 1234配置8location /tobaidu/ &#123; proxy_pass http://127.0.0.1:8087/define/;&#125; 测试结果： 请求URL 请求结果 curl http://127.0.0.1/tobaidu 重定向到http://127.0.0.1/tobaidu/ curl http://127.0.0.1/tobaidu/ http://127.0.0.1:8087/define/ curl http://127.0.0.1/tobaidu/xxxx http://127.0.0.1:8087/define/xxxx 结论URL符合 protocol://ip:port 同时结尾不加/,则nginx会代理匹配路径部分,否则不代理匹配路径,同时自动添加不匹配路径”部分”,比如/tobaidu/xxxx的/xxxx部分]]></content>
      <categories>
        <category>web</category>
      </categories>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shell脚本echo颜色]]></title>
    <url>%2F2018%2F01%2F21%2Fecho-color.html</url>
    <content type="text"><![CDATA[echo显示颜色格式echo 显示内容颜色，需要使用 -e 参数-e :打开反斜杠转义 (默认不打开) ,可以转义 “\n, \t” 等-n :在最后不自动换行 字体颜色列表12345678910#字体颜色：30m-37m 黑、红、绿、黄、蓝、紫、青、白str="=====Hello Word====="echo -e "\033[30m $&#123;str&#125;\033[0m" ## 黑色字体echo -e "\033[31m $&#123;str&#125;\033[0m" ## 红色echo -e "\033[32m $&#123;str&#125;\033[0m" ## 绿色echo -e "\033[33m $&#123;str&#125;\033[0m" ## 黄色echo -e "\033[34m $&#123;str&#125;\033[0m" ## 蓝色echo -e "\033[35m $&#123;str&#125;\033[0m" ## 紫色echo -e "\033[36m $&#123;str&#125;\033[0m" ## 青色echo -e "\033[37m $&#123;str&#125;\033[0m" ## 白色 背景颜色列表12345678#背景颜色：40-47 黑、红、绿、黄、蓝、紫、青、白str="=====Hello Word====="echo -e "\033[41;37m $&#123;str&#125; \033[0m" ## 红色背景色，白色字体echo -e "\033[41;33m $&#123;str&#125; \033[0m" ## 红底黄字echo -e "\033[1;41;33m $&#123;str&#125; \033[0m" ## 红底黄字 高亮加粗显示echo -e "\033[5;41;33m $&#123;str&#125; \033[0m" ## 红底黄字 字体闪烁显示echo -e "\033[47;30m $&#123;str&#125; \033[0m" ## 白底黑字echo -e "\033[40;37m $&#123;str&#125; \033[0m" ## 黑底白字 其他参数说明\033[1;m: 设置高亮加粗\033[3;m: 斜体\033[4;m: 下划线\033[5;m: 闪烁 颜色在脚本中使用我们这里以最常用的三种颜色举例，分别是红色——错误；黄色——警告；绿色——成功，来代表演示，日常我们在写脚本用的最多的也是这三种颜色！1234567891011121314151617181920212223242526272829303132333435363738394041424344454647#!/bin/shSTR='=====Hello Word====='RES='\033[0m'# 正常红黄绿Normal_Color() &#123; local RED="\e[31m" local YELLOW="\e[33m" local GREEN="\e[32m" echo -e "$&#123;RED&#125;$STR$&#123;RES&#125;" echo -e "$&#123;YELLOW&#125;$&#123;STR&#125;$&#123;RES&#125;" echo -e "$&#123;GREEN&#125;$&#123;STR&#125;$&#123;RES&#125;"&#125;Normal_Colorecho # 高亮红黄绿Light_Color() &#123; local RED="\e[1;31m" local YELLOW="\e[41;1;33m" local GREEN="\e[1;32m" echo -e "$&#123;RED&#125;$STR$&#123;RES&#125;" echo -e "$&#123;YELLOW&#125;$&#123;STR&#125;$&#123;RES&#125;" echo -e "$&#123;GREEN&#125;$&#123;STR&#125;$&#123;RES&#125;"&#125;Light_Colorecho # 高亮闪烁红黄绿Light_Blink_Color() &#123; local RED="\e[5;1;31m" local YELLOW="\e[5;1;33m" local GREEN="\e[5;1;32m" echo -e "$&#123;RED&#125;$STR$&#123;RES&#125;" echo -e "$&#123;YELLOW&#125;$&#123;STR&#125;$&#123;RES&#125;" echo -e "$&#123;GREEN&#125;$&#123;STR&#125;$&#123;RES&#125;"&#125;Light_Blink_Colorecho # 有底色闪烁红黄绿Under_Blink_Color() &#123; local RED="\e[5;47;31m" local YELLOW="\e[5;41;33m" local GREEN="\e[5;45;32m" echo -e "$&#123;RED&#125;$STR$&#123;RES&#125;" echo -e "$&#123;YELLOW&#125;$&#123;STR&#125;$&#123;RES&#125;" echo -e "$&#123;GREEN&#125;$&#123;STR&#125;$&#123;RES&#125;"&#125;Under_Blink_Color 效果如下：]]></content>
      <categories>
        <category>shell</category>
      </categories>
      <tags>
        <tag>shell</tag>
        <tag>color</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OpenSSL自签证书]]></title>
    <url>%2F2018%2F01%2F10%2Fopenssl-cert.html</url>
    <content type="text"><![CDATA[概念证书: 英文叫digital certificate 或public key certificate。 用于在客户端和服务器端做通信时的一种验证证明。CA: CA是Certificate Authority的缩写,也叫“证书授权中心”。如果要被他人安全访问，该证书授权中心为第三方公司（要钱）CA证书: 顾名思义，就是CA(证书授权中心)颁发的证书。 安装想要使用证书，首先需要安装openssl。利用openssl生成根证书，以后的服务器端证书或者客户端证书都用他来签发，可以建立多个根证书。123456# 安装$ yum install -y openssl-devel# 查看版本$ openssl versionOpenSSL 1.0.2k-fips 26 Jan 2017 X509证书链x509证书一般会用到三类文件，它们分别是key，csr，crt。 key是私用密钥，openssl格式，通常是rsa算法。csr是证书请求文件，用于申请证书。在制作csr文件的时候，必须使用自己的私钥来签署申请，还可以设定一个密钥。crt是CA认证后的证书文件，签署人用自己的key给你签署的凭证。 证书格式.key格式：私有的密钥.csr格式：证书签名请求（证书请求文件），含有公钥信息，certificate signing request的缩写.crt格式：证书文件，certificate的缩写.crl格式：证书吊销列表，Certificate Revocation List的缩写.pem格式：用于导出，导入证书时候的证书的格式，有证书开头，结尾的格式 openssl常用命令12345678910111213141516171819202122232425262728genrsa 产生RSA密钥命令-new 表示生成新的证书请求。-key 指定证书私钥-out 输出路径-in 输入文件req 产生证书签发申请命令x509 签发X.509格式证书命令。 -signkey 表示自签名密钥-req 表示证书输入请求。-days 表示有效天数-CAcreateserial 表示创建CA证书序列号-CAkey 表示CA证书密钥-CA 表示CA证书-CAserial 表示CA证书序列号文件ca 签发证书命令-cert 表示证书文件-keyfile 表示根证书密钥文件-subj 指定用户信息 -clcerts 表示仅导出客户证书。-inkey 表示输入文件-export 表示导出证书。-extensions 表示按OpenSSL配置文件v3_ca项添加扩展。-extensions 表示按OpenSSL配置文件v3_req项添加扩展。-sha1 表示证书摘要算法,这里为SHA1算法。pkcs12 PKCS#12编码格式证书命令。rand 随机数命令 -aes256 使用AES算法（256为密钥）对产生的私钥加密。可选算法包括DES,DESede,IDEA和AES。 生成CA证书步骤生成CA私钥（.key）–&gt; 生成CA证书请求（.csr）–&gt; 自签名得到CA根证书（.crt）（CA给自已颁发的证书）。该CA证书可用于签名其他服务证书。 生成CA密钥1234567891011121314151617181920212223242526272829$ openssl genrsa -out cakey.pem 2048$ cat ca.key-----BEGIN RSA PRIVATE KEY-----MIIEpAIBAAKCAQEAtTAp8GmUsWJ7q3UPcaQUCYx7ue32Ki0nzI6X8BA2zNT0lMkDHhxdbWPWXaSEiaoiRPtMoCOtemLMzX8MRjCtA31yf3RRsXKIIg6OFfhesgBDdGcf7Zkhk9zKq1evs8y4G7wNduOg76lKLQg3SuWSS8OlRBS4t17jPY7Z4Kr6UZtn40Fc19I1FEEjZaOCTUk/WN9G2E/pO+NWN23+2m6O/STiESyFSq5wSx1Vqio2BAoidiChdJgxT4lL8tv3YBHCO4oWkex6Nb3yhZq86yfJIwXXj+feJZb0hFJQ2MpECMhk95HHkiwLlQc+y3Nj0fOrqbBMFdnmyCv2wzC6xx1/HQIDAQABAoIBAC586BXOER+eJBru0wKWVanJiKlA2+sgYNjEMUmf71+IuCRAmvMr1fDOL98g6fykUVyfmZ5w6P7AwMls8opDzPBbTHhVMOy1dSY/08bhTfKfzK7eErwUkR/uA3YI7oTUXtyG2HGLn+w95FE/jWhDFNEppoqcQnSR/P37W/2gAM/VADSAwF44mBeJMwWEH51FbTQVDWtnaXnzEtnGqXBe7Jrfy01EvNp9auQ6arGgdBf0YcmGEd/5d0numkvCrmm0YgkNzhG5abcl75W6bwkY8ubGUhbe/JXxyXfr2ryKcin1LkiERgmwSUwjcAuskuIJMJmAk4weIos5RXLpIgx2r8ECgYEA3mv7dSAOtEs3ztD5D5VqirEs/QLb6hKtKSDJhwn66FAl3A6NWvTUp/nhZ/RznP2VGPRCAJeBg9SFj7wkMP77bz5dPyJBWfX8CK92PMWreSfzYrs32xahZqs6+7rML94MSPuSwNE7rY4tyUwyFj1wA7UqwheRe6rfkEej7LveATECgYEA0Iqb3VmxKwIraNlPlhgOxwX2cNzLPCUT0UWSYMdy1E6BwOuluauqx1VHdGZ7k51Di9SaDRgMQpVPF9Bk83P/x9h4qF6EsVCV+sVw5pWB+dhAEjA3UgOFGYG+RjyfFRboQbccsMQpXjfcdJT4lRSRA3WICPxLFn2/m+7d5CQNga0CgYEA2crovma2n0qsCgLMbrsDSW12PQWIq6rADm7Bh055dvPMLq+9MJxeg2EGm9FdSBNy5K2A162DL8BxTC6RTbzQHbz2d7SmQ12//g05/QYeAxPgmgPzDMAbKTpwFkByYkjOxMQ6jj4Tbr2zDdJjlS1xut+yT73eQjculMvhsxS+rXECgYAYp4ptzODJOORw7OAf2pBEr0vHZBMS9T82iocXsfy9ZNqqODHLlaQHFOnxtPv/I6SMr4HW8nTgmk5Tfmuw7JHcypbZMPN3ExPoJdeHKz3Gj+5jOBgSNiBSN6iLHTehgqfKvR9DNq29WdVSYxpQZbIPOqHujgVCj3NLuB27jxeZsQKBgQCIkOrz8Ly2heIhhkBLcP5D90RW4lG4nbWpHSmKU06t8lKozB1YppUH2JsKJm8X4kh+VfzcpheY3Hju3Nw/aL/xQDmoh74kE8/VvTfB4HuAVbV9cQArL+6CUANsu9K+NtnME1cbluQtLpe0XhElcix5mXWnGYX4ZojBzVxt/o+mww==-----END RSA PRIVATE KEY----- 生成CA证书请求1234567891011121314151617181920212223242526272829303132333435363738394041$ openssl req -new -key cakey.pem -out ca.csrYou are about to be asked to enter information that will be incorporatedinto your certificate request.What you are about to enter is what is called a Distinguished Name or a DN.There are quite a few fields but you can leave some blankFor some fields there will be a default value,If you enter '.', the field will be left blank.-----Country Name (2 letter code) [XX]:CN # ----- 证书持有者所在国家State or Province Name (full name) []:GuangDong # ----- 证书持有者所在州或省份Locality Name (eg, city) [Default City]:ShenZhen # ----- 证书持有者所在城市Organization Name (eg, company) [Default Company Ltd]:派有限公司 # ----- 证书持有者公司Organizational Unit Name (eg, section) []:IT # ----- 证书持有者所属部门Common Name (eg, your name or your server's hostname) []:ssl.ipyker.com # ---- 域名Email Address []:pyker@qq.com # ----- 邮箱 （可不填）Please enter the following 'extra' attributesto be sent with your certificate requestA challenge password []: # ----- 证书密码An optional company name []: # ----- 确认证书密码$ cat ca.csr-----BEGIN CERTIFICATE REQUEST-----MIIC2DCCAcACAQAwgZIxCzAJBgNVBAYTAkNOMRIwEAYDVQQIDAlHdWFuZ0RvbmcxETAPBgNVBAcMCFNoZW5aaGVuMQ8wDQYDVQQKDAZpcHlrZXIxFTATBgNVBAsMDHB5a2VyJ3MgYmxvZzEXMBUGA1UEAwwOc3NsLmlweWtlci5jb20xGzAZBgkqhkiG9w0BCQEWDHB5a2VyQHFxLmNvbTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBALUwKfBplLFie6t1D3GkFAmMe7nt9iotJ8yOl/AQNszU9JTJAx4cXW1j1l2khImqIkT7TKAjrXpizM1/DEYwrQN9cn90UbFyiCIOjhX4XrIAQ3RnH+2ZIZPcyqtXr7PMuBu8DXbjoO+pSi0IN0rlkkvDpUQUuLde4z2O2eCq+lGbZ+NBXNfSNRRBI2Wjgk1JP1jfRthP6TvjVjdt/tpujv0k4hEshUqucEsdVaoqNgQKInYgoXSYMU+JS/Lb92ARwjuKFpHsejW98oWavOsnySMF14/n3iWW9IRSUNjKRAjIZPeRx5IsC5UHPstzY9Hzq6mwTBXZ5sgr9sMwuscdfx0CAwEAAaAAMA0GCSqGSIb3DQEBCwUAA4IBAQA4UgXfpamRmrL74i7EpzaLw1RQ0T4dF/VCbNuVU8ajvR8ajm6pe1XkoYjKarnc8mQo0WwqukWZhyGoG0ggn0HpgfEQA4AQSjQTjp7Kh8B0N5i+KnfygzQzRyDr+MXdAvvwd9aq1Hkyb2dqQhamyK62gQ4A2AG9xraxVsJB4KLEGY/wCLzIjaqToPv1LlyWpejn6jeJXbfC+pI52yRdWYCvEBXzp8pNp0EdCVeUXJ05c6KZHEx+GuhiRMxWqLRqiqrgdi2uOX5v4HxMuvHrd3l7eb2qt/82gnmIlNODfFv3ogIAZz/1ut4d+XUle6gWlTtDjCLlBKGSxBd2ELFmSTCc-----END CERTIFICATE REQUEST----- 自签名得到CA根证书123456789101112131415161718192021222324$ openssl x509 -req -in ca.csr -out ca.pem -signkey cakey.pem -days 3650$ cat ca.pem -----BEGIN CERTIFICATE-----MIIDojCCAooCCQD1vqZ2J4Kr9TANBgkqhkiG9w0BAQsFADCBkjELMAkGA1UEBhMCQ04xEjAQBgNVBAgMCUd1YW5nRG9uZzERMA8GA1UEBwwIU2hlblpoZW4xDzANBgNVBAoMBmlweWtlcjEVMBMGA1UECwwMcHlrZXIncyBibG9nMRcwFQYDVQQDDA5zc2wuaXB5a2VyLmNvbTEbMBkGCSqGSIb3DQEJARYMcHlrZXJAcXEuY29tMB4XDTE5MDYyNjAyMDcxOVoXDTI5MDYyMzAyMDcxOVowgZIxCzAJBgNVBAYTAkNOMRIwEAYDVQQIDAlHdWFuZ0RvbmcxETAPBgNVBAcMCFNoZW5aaGVuMQ8wDQYDVQQKDAZpcHlrZXIxFTATBgNVBAsMDHB5a2VyJ3MgYmxvZzEXMBUGA1UEAwwOc3NsLmlweWtlci5jb20xGzAZBgkqhkiG9w0BCQEWDHB5a2VyQHFxLmNvbTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBALUwKfBplLFie6t1D3GkFAmMe7nt9iotJ8yOl/AQNszU9JTJAx4cXW1j1l2khImqIkT7TKAjrXpizM1/DEYwrQN9cn90UbFyiCIOjhX4XrIAQ3RnH+2ZIZPcyqtXr7PMuBu8DXbjoO+pSi0IN0rlkkvDpUQUuLde4z2O2eCq+lGbZ+NBXNfSNRRBI2Wjgk1JP1jfRthP6TvjVjdt/tpujv0k4hEshUqucEsdVaoqNgQKInYgoXSYMU+JS/Lb92ARwjuKFpHsejW98oWavOsnySMF14/n3iWW9IRSUNjKRAjIZPeRx5IsC5UHPstzY9Hzq6mwTBXZ5sgr9sMwuscdfx0CAwEAATANBgkqhkiG9w0BAQsFAAOCAQEAOtAJpnww2LLO1pMVb/tBChgAYloCj0GfykGUryLiWqDCeFXYMO5KLonW0T87aDxvYDEKgo1ITAK26EXlcvi03Pq1DfUfLWrLK+B1+IXXFYGuAcWyrxqM1hp8TN/g9CB5GXnY8AiEflLw7IpajYRJ0f9sSaIcH8o0sUVTZzBJ9kExv+m0w6GW8+oX+2Olh8KKavw7ctNze2/A9dTRucIfWH+a+A0dtPX5rnSnkiiBG8xHfMxKsgTl83l5CunrFcarJMeI/f/49rdiaUr68thNTVx/eUWpAKdgjO2t5JLPGx1iqs9bJCiaWbpgM97z4KTuB6BizyQBicILVDULhMCtag==-----END CERTIFICATE----- 以上步骤即是自签根证书了。 用户证书生成步骤一般说的生成用户证书分两种，客户端证书和服务端证书，除了名称不一样步骤命令完全一样一样的。生成私钥（.key）--&gt;生成证书请求（.csr）--&gt;用CA根证书签名得到证书（.crt） 生成密钥1234567891011121314151617181920212223242526272829$ openssl genrsa -out serverkey.pem 2048$ cat server.key -----BEGIN RSA PRIVATE KEY-----MIIEogIBAAKCAQEAvC2I47h2lMYsZalJ16Uy9HYqcoecTep2gT/RRMO3wnHF8e1pI8iU+GQXkIsMhTmjjcyhjfxxtooxnkp+U6mVdhkDkufMm+ER3z5mCXlhbA8T2Ia6GGcUScX+4qA834H/Ams8r0UoOc6zi9Tnku+AnK6eq7apYc3KoIc2Uj6qyS9jAkGNJFplEdjUgSHBsaB78wi8ukN2nGeZR9c4WVoXm8Cz63RR8a9FB/JjVrCe8Ey1Xk55uTiU/NJa2VWLi/92MKwvnyulr004yIS1DsN06X3fneffV8HVux+LPgL6QHH55UJjKp1LBqvmNFrrS4AReI73JVQR5j6vVvsyeSXbuwIDAQABAoIBADpkJsYCx0kC9WPWVAOGT3lr8V/4lJfY2Uzh8J3V3X+IrlOTx7xC0XcCGA3SF+B/MjEd/kOAwghSeXMUyn5LcQVkXaeIJgV4oYMUabUm5QQS6aWWqMhJtBHwTlckQb9ZJzgo7nu0ifbmHPCW8AS4LMBxruq5k3W11dpaGpEKwRQMB97ZvbgaIJxgIp/9Oevjq6OYX9UXa0ztxY8kitvLpMPaNlSc39T5XYumq52MvOccZGzDO3Njv9MG6rWt/Fw1NWsB/6RshUr4yeVeQD66Wp+HaD9qaycb9jIR3SeJQoKaEgq0mmA/y5np3SKP/HzcmYK/ubcapF/7cilEXR9MwYECgYEA5i7d4qSmX1BmphrTJ1yWcTESx4aXsjAuDY+HHMe1nLyozek3aXucIkTx4JE/HGYqjOeminYhuaZ5jJLRf07s1VT+8lk/XobDplmQmH8kAHtHFuIWY5l/EW2L+QV31AFv0Dt5OEhUuTE6F1+6xG8dDPoIGIWIg4luNtylslhQO30CgYEA0UiX4ED2yHF5cd9cLKWi0niYQZILO87N9U0IRxIQbpdm7UZGuLPLMs/W7Xf+kjOb3qFChNS5KGT3JbuhbcwO+34gULrK7AYejwhUhO7CYi28OaRropjoxId7nMcQw2ciISTEeJHKVXz0PKW7Wt2uq3n6unwW8VNZm6qXfA3G6ZcCgYA78ZqRCkXVbo+81CGHD6KSCbCVS2S337ouh+EsyoluLuda8FAg5TLs7b17uPeRgr20AiOpzUfNHCBtTlLGb5xXlhHqtPk+uaO773krbXjHs1L5D5m7CF9B/6BDEnx5NoKS3NodoSCHNd2l9qUhwLn1BiwTjrrVXnXYTa/M+Riz1QKBgEIdjN1rqIrqTlOLHLN+IFIdhvwwBxx92NMF4veQ3WAStJGBAhaXtjn3Lw8WOXY2l6ddioYsLdJ1Ex74h6cIMDODRPI8EJ8/z6egGhNk2kPp7uzG5LoZVG/B3WtJ+CHDEyUlWGw+oo0fTIlcUjQClIvXnT4MtbLHgieLXQ/zykNBAoGADlz53txUajLxhBPdEs6f7rIOux0e02AaYff9oJlvikjmIAdvUDQ2HI6i118pZpTwyi8wDDw6QlhpHssEIUEeZ4Umu1k/tvKqGg5VWfisUU9gch5lbxtZ7tdpRpsBDBtqfjUbWhnApZCxfA45mfB/xGwPPFLdPUKX9578wsDQQlU=-----END RSA PRIVATE KEY----- 生成证书请求文件1234567891011121314151617181920$ openssl req -new -key serverkey.pem -out server.csr -subj "/C=CN/ST=GuangDong/L=ShenZhen/O=公司名/OU=it/CN=tls.ipyker.com/emailAddress=pyker@qq.com"$ cat server.csr -----BEGIN CERTIFICATE REQUEST-----MIIC2jCCAcICAQAwgZQxCzAJBgNVBAYTAkNOMRIwEAYDVQQIDAlHdWFuZ0RvbmcxETAPBgNVBAcMCFNoZW5aaGVuMRswGQYDVQQKDBLDpcKFwqzDpcKPwrjDpcKQwo0xCzAJBgNVBAsMAml0MRcwFQYDVQQDDA50bHMuaXB5a2VyLmNvbTEbMBkGCSqGSIb3DQEJARYMcHlrZXJAcXEuY29tMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAvC2I47h2lMYsZalJ16Uy9HYqcoecTep2gT/RRMO3wnHF8e1pI8iU+GQXkIsMhTmjjcyhjfxxtooxnkp+U6mVdhkDkufMm+ER3z5mCXlhbA8T2Ia6GGcUScX+4qA834H/Ams8r0UoOc6zi9Tnku+AnK6eq7apYc3KoIc2Uj6qyS9jAkGNJFplEdjUgSHBsaB78wi8ukN2nGeZR9c4WVoXm8Cz63RR8a9FB/JjVrCe8Ey1Xk55uTiU/NJa2VWLi/92MKwvnyulr004yIS1DsN06X3fneffV8HVux+LPgL6QHH55UJjKp1LBqvmNFrrS4AReI73JVQR5j6vVvsyeSXbuwIDAQABoAAwDQYJKoZIhvcNAQELBQADggEBACILQ8Exa81xOEPliER0b86h5xKqxBXInqETn0vKYXjU+D5hjePtNB0NTP/gleYMNrjKV14XMj3iI9Dq3lMAiWa06Z4Mx2CowqbQY9dvkm/MoDOIoA2q+fWr4H+NACsALYWh3/sALDdcry9FcOrC6nxD9sBcK8n65z3hBjWudvBFtFYTQ+zRuWHeD4aBxmbpfaDCW/rl644AJ6varcAQ29yqxiWFvyD+C3lKycCmFY2D52pJLcw4IoVvGx5VzG7KrTUDW+qPclyM6vj0SeDaYKZKmUDRLzPTroxNganc+TDsaz6CmgXQ5sJGaIiDmGdpbGzPevDa5t3GovDNWiyZDog=-----END CERTIFICATE REQUEST----- 使用CA证书及CA密钥 对请求签发证书进行签发，生成 x509证书123456789101112131415161718192021222324$ openssl x509 -req -in server.csr -CA ca.pem -CAkey cakey.pem -CAcreateserial -out server.pem -days 3650$ cat server.pem-----BEGIN CERTIFICATE-----MIIDmjCCAoICCQCO6Ai3pDkDkjANBgkqhkiG9w0BAQsFADCBiDELMAkGA1UEBhMCQ04xEjAQBgNVBAgMCUdVQU5HRE9ORzERMA8GA1UEBwwIU0hFTlpIRU4xDzANBgNVBAoMBklQWUtFUjELMAkGA1UECwwCSVQxFzAVBgNVBAMMDnNzbC5pcHlrZXIuY29tMRswGQYJKoZIhvcNAQkBFgxweWtlckBxcS5jb20wHhcNMTkwNjI2MDIyNDM0WhcNMjkwNjIzMDIyNDM0WjCBlDELMAkGA1UEBhMCQ04xEjAQBgNVBAgMCUd1YW5nRG9uZzERMA8GA1UEBwwIU2hlblpoZW4xGzAZBgNVBAoMEsOlwoXCrMOlwo/CuMOlwpDCjTELMAkGA1UECwwCaXQxFzAVBgNVBAMMDnRscy5pcHlrZXIuY29tMRswGQYJKoZIhvcNAQkBFgxweWtlckBxcS5jb20wggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDGbvCmdHFwF5fEI49P0CsFcIjvCFX8esLNQyX7ldEAVXBaW/JHfeNZv2koC6TsXLrywaKhqzK4IOf15tLd3pQGEA+9N9JtpZIjNNztzby2shWZqCWxDa+k1qZFtQ7TgX4LrytSDtjJHZgHR0zPZkjBTI8SBETfhTxIFWE75dGQrejhaSXeqL32Rs1xOJmBlcJq0eNvljeCnZ+VPU9Ujnge8WwEZ8bMgd35XuvmPZyZiP0HAjg6+8BI1iccPgu9utPoF8sZ4+npYkkWUht7hmw03pWGv3xu5XE2nwdnuUTlpKwcOYv90YD3kVvP5sHlMvimErLSV7jRWF394GyRBhItAgMBAAEwDQYJKoZIhvcNAQELBQADggEBAJkl54O5RpxaIfDtdT+LOxB2iwLe36XybJV0EvBffZH9vyK0Xg/OBLrMHAnZ8B5Y1u+tP9nn5QtM+pnBXBt+qtO45nUG1bjGZQPHpvC68tG6eB+iw98u72cuMD6wbPE+kfaQQ3VS+xbyDvdRwscmOQjDLMfR0mLVM1WTWOW7IaII75IIJuHBTx7eSvhERtyVFP1gj7FaJMmMhK0vCrG4awofpzdlV2bTGp7oD0XaEAxYb5pF+x3e5GzOZXFR/1gGezC8ubyFuyUkifpg4015e+Rnv7KSIRWaegT4fGSZ0eMMNaSn+8WUIhOvnZtptZCeL9Rpf0vimC2Ad3IyRt8GmsQ=-----END CERTIFICATE----- 查看当前生成的CA、客户端证书私钥、证书请求以及CA证书。12345678$ ls -l-rw-r--r-- 1 root root 1050 Jun 26 10:21 ca.csr-rw-r--r-- 1 root root 1679 Jun 26 10:21 cakey.pem-rw-r--r-- 1 root root 1294 Jun 26 10:21 ca.pem-rw-r--r-- 1 root root 17 Jun 26 10:24 ca.srl-rw-r--r-- 1 root root 1066 Jun 26 10:22 server.csr-rw-r--r-- 1 root root 1679 Jun 26 10:22 serverkey.pem-rw-r--r-- 1 root root 1310 Jun 26 10:24 server.pem]]></content>
      <categories>
        <category>TLS</category>
      </categories>
      <tags>
        <tag>openssl</tag>
        <tag>ssl</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTTP请求头响应头字段详解]]></title>
    <url>%2F2017%2F12%2F26%2Fhttp-header.html</url>
    <content type="text"><![CDATA[HTTP消息头是指，在超文本传输协议（ Hypertext Transfer Protocol ，HTTP）的请求和响应消息中，协议头部分的那些组件。HTTP消息头用来准确描述正在获取的资源、服务器或者客户端的行为，定义了HTTP事务中的具体操作参数。 关于HTTP消息头HTTP消息头是在客户端请求（Request） 或服务器响应（Response） 时传递的，为请求或响应的第一行，HTTP消息体（请求或响应的内容）是其后传输。HTTP消息头，以明文的字符串格式传送，是以冒号分隔的键/值对，如：Accept-Charset: utf-8，每一个消息头最后以回车符(CR)和换行符(LF)结尾。HTTP消息头结束后，会用一个空白的字段来标识，这样就会出现两个连续的CR-LF。HTTP消息头支持自定义， 自定义的专用消息头一般会添加’X-‘前缀。 常用标准请求头字段Accept 设置接受的内容类型 Accept: text/plain Accept-Charset 设置接受的字符编码 Accept-Charset: utf-8 Accept-Encoding 设置接受的编码格式 Accept-Encoding: gzip, deflate Accept-Datetime 设置接受的版本时间 Accept-Datetime: Thu, 31 May 2007 20:35:00 GMT Accept-Language 设置接受的语言 Accept-Language: en-US Authorization 设置HTTP身份验证的凭证 Authorization: Basic QWxhZGRpbjpvcGVuIHNlc2FtZQ== Cache-Control 设置请求响应链上所有的缓存机制必须遵守的指令 Cache-Control: no-cache Connection 设置当前连接和hop-by-hop协议请求字段列表的控制选项 Connection: keep-alive Connection: Upgrade Content-Length 设置请求体的字节长度 Content-Length: 348 Content-MD5 设置基于MD5算法对请求体内容进行Base64二进制编码 Content-MD5: Q2hlY2sgSW50ZWdyaXR5IQ== Content-Type 设置请求体的MIME类型（适用POST和PUT请求） Content-Type: application/x-www-form-urlencoded Cookie 设置服务器使用Set-Cookie发送的http cookie Cookie: $Version=1; Skin=new; Date 设置消息发送的日期和时间 Date: Tue, 15 Nov 1994 08:12:31 GMT Expect 标识客户端需要的特殊浏览器行为 Expect: 100-continue Forwarded 披露客户端通过http代理连接web服务的源信息 Forwarded: for=192.0.2.60;proto=http;by=203.0.113.43 Forwarded: for=192.0.2.43, for=198.51.100.17 From 设置发送请求的用户的email地址 From: user@example.com Host 设置服务器域名和TCP端口号，如果使用的是服务请求标准端口号，端口号可以省略 Host: en.wikipedia.org:8080 Host: en.wikipedia.org If-Match 设置客户端的ETag,当时客户端ETag和服务器生成的ETag一致才执行，适用于更新自从上次更新之后没有改变的资源 If-Match: &quot;737060cd8c284d8af7ad3082f209582d If-Modified-Since 设置更新时间，从更新时间到服务端接受请求这段时间内如果资源没有改变，允许服务端返回304 Not Modified If-Modified-Since: Sat, 29 Oct 1994 19:43:31 GMT If-None-Match 设置客户端ETag，如果和服务端接受请求生成的ETage相同，允许服务端返回304 Not Modified If-None-Match: &quot;737060cd8c284d8af7ad3082f209582d&quot; If-Range 设置客户端ETag，如果和服务端接受请求生成的ETage相同，返回缺失的实体部分；否则返回整个新的实体 If-Range: &quot;737060cd8c284d8af7ad3082f209582d&quot; If-Unmodified-Since 设置更新时间，只有从更新时间到服务端接受请求这段时间内实体没有改变，服务端才会发送响应 If-Unmodified-Since: Sat, 29 Oct 1994 19:43:31 GMT Max-Forwards 限制代理或网关转发消息的次数 Max-Forwards: 10 Origin 标识跨域资源请求（请求服务端设置Access-Control-Allow-Origin响应字段） Origin: http://www.example-social-network.com Pragma 设置特殊实现字段，可能会对请求响应链有多种影响 Pragma: no-cache Proxy-Authorization 为连接代理授权认证信息 Proxy-Authorization: Basic QWxhZGRpbjpvcGVuIHNlc2FtZQ== Range 请求部分实体，设置请求实体的字节数范围，具体可以参见HTTP/1.1中的Byte serving Range: bytes=500-999 Referer 设置前一个页面的地址，并且前一个页面中的连接指向当前请求，意思就是如果当前请求是在A页面中发送的，那么referer就是A页面的url地址（轶 事：这个单词正确的拼法应该是”referrer”,但是在很多规范中都拼成了”referer”，所以这个单词也就成为标准用法） Referer: http://en.wikipedia.org/wiki/Main_Page TE 设置用户代理期望接受的传输编码格式，和响应头中的Transfer-Encoding字段一样 TE: trailers, deflate Upgrade 请求服务端升级协议 Upgrade: HTTP/2.0, HTTPS/1.3, IRC/6.9, RTA/x11, websocket User-Agent 用户代理的字符串值 User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:12.0) Gecko/20100101 Firefox/21.0 Via 通知服务器代理请求 Via: 1.0 fred, 1.1 example.com (Apache/1.1) Warning 实体可能会发生的问题的通用警告 Warning: 199 Miscellaneous warning 常用非标准请求头字段X-Requested-With 标识Ajax请求，大部分js框架发送请求时都会设置它为XMLHttpRequest X-Requested-With: XMLHttpRequest DNT 请求web应用禁用用户追踪 DNT: 1 (Do Not Track Enabled) DNT: 0 (Do Not Track Disabled) X-Forwarded-For 一个事实标准，用来标识客户端通过HTTP代理或者负载均衡器连接的web服务器的原始IP地址 X-Forwarded-For: client1, proxy1, proxy2 X-Forwarded-For: 129.78.138.66, 129.78.64.103 X-Forwarded-Host 一个事实标准，用来标识客户端在HTTP请求头中请求的原始host,因为主机名或者反向代理的端口可能与处理请求的原始服务器不同 X-Forwarded-Host: en.wikipedia.org:8080 X-Forwarded-Host: en.wikipedia.org X-Forwarded-Proto 一个事实标准，用来标识HTTP原始协议，因为反向代理或者负载均衡器和web服务器可能使用http,但是请求到反向代理使用的是https X-Forwarded-Proto: https Front-End-Https 微软应用程序和负载均衡器使用的非标准header字段 Front-End-Https: on X-Http-Method-Override 请求web应用时，使用header字段中给定的方法（通常是put或者delete）覆盖请求中指定的方法（通常是post）,如果用户代理或者防火墙不支持直接使用put或者delete方法发送请求时，可以使用这个字段 X-HTTP-Method-Override: DELETE X-ATT-DeviceId 允许更简单的解析用户代理在AT&amp;T设备上的MakeModel/Firmware X-Att-Deviceid: GT-P7320/P7320XXLPG X-Wap-Profile 设置描述当前连接设备的详细信息的xml文件在网络中的位置 x-wap-profile: http://wap.samsungmobile.com/uaprof/SGH-I777.xml Proxy-Connection 早起HTTP版本中的一个误称，现在使用标准的connection字段 Proxy-Connection: keep-alive X-UIDH 服务端深度包检测插入的一个唯一ID标识Verizon Wireless的客户 X-UIDH: ... X-Csrf-Token,X-CSRFToken,X-XSRF-TOKEN 防止跨站请求伪造 X-Csrf-Token: i8XNjC4b8KVok4uw5RftR38Wgp2BFwql X-Request-ID,X-Correlation-ID 标识客户端和服务端的HTTP请求 X-Request-ID: f058ebd6-02f7-4d3f-942e-904344e8cde5 常用标准响应头字段Access-Control-Allow-Origin 指定哪些站点可以参与跨站资源共享 Access-Control-Allow-Origin: * Accept-Patch 指定服务器支持的补丁文档格式，适用于http的patch方法 Accept-Patch: text/example;charset=utf-8 Accept-Ranges 服务器通过byte serving支持的部分内容范围类型 Accept-Ranges: bytes Age 对象在代理缓存中暂存的秒数 Age: 12 Allow 设置特定资源的有效行为，适用方法不被允许的http 405错误 Allow: GET, HEAD Alt-Svc 服务器使用”Alt-Svc”（Alternative Servicesde的缩写）头标识资源可以通过不同的网络位置或者不同的网络协议获取 Alt-Svc: h2=&quot;http2.example.com:443&quot;; ma=7200 Cache-Control 告诉服务端到客户端所有的缓存机制是否可以缓存这个对象，单位是秒 Cache-Control: max-age=3600 Connection 设置当前连接和hop-by-hop协议请求字段列表的控制选项 Connection: close Content-Disposition 告诉客户端弹出一个文件下载框，并且可以指定下载文件名 Content-Disposition: attachment; filename=&quot;fname.ext&quot; Content-Encoding 设置数据使用的编码类型 Content-Encoding: gzip Content-Language 为封闭内容设置自然语言或者目标用户语言 Content-Language: en Content-Length 响应体的字节长度 Content-Length: 348 Content-Location 设置返回数据的另一个位置 Content-Location: /index.htm Content-MD5 设置基于MD5算法对响应体内容进行Base64二进制编码 Content-MD5: Q2hlY2sgSW50ZWdyaXR5IQ== Content-Range 标识响应体内容属于完整消息体中的那一部分 Content-Range: bytes 21010-47021/47022 Content-Type 设置响应体的MIME类型 Content-Type: text/html; charset=utf-8 Date 设置消息发送的日期和时间 Date: Tue, 15 Nov 1994 08:12:31 GMT ETag 特定版本资源的标识符，通常是消息摘要 ETag: &quot;737060cd8c284d8af7ad3082f209582d&quot; Expires 设置响应体的过期时间 Expires: Thu, 01 Dec 1994 16:00:00 GMT Last-Modified 设置请求对象最后一次的修改日期 Last-Modified: Tue, 15 Nov 1994 12:45:26 GMT Link 设置与其他资源的类型关系 Link: &lt;/feed&gt;; rel=&quot;alternate&quot; Location 在重定向中或者创建新资源时使用 Location: http://www.w3.org/pub/WWW/People.html P3P 以P3P:CP=”your_compact_policy”的格式设置支持P3P(Platform for Privacy Preferences Project)策略，大部分浏览器没有完全支持P3P策略，许多站点设置假的策略内容欺骗支持P3P策略的浏览器以获取第三方cookie的授权 P3P: CP=&quot;This is not a P3P policy! See http://www.google.com/support/accounts/bin/answer.py?hl=en&amp;answer=151657 for more info.&quot; Pragma 设置特殊实现字段，可能会对请求响应链有多种影响 Pragma: no-cache Proxy-Authenticate 设置访问代理的请求权限 Proxy-Authenticate: Basic Public-Key-Pins 设置站点的授权TLS证书 Public-Key-Pins: max-age=2592000; pin-sha256=&quot;E9CZ9INDbd+2eRQozYqqbQ2yXLVKB9+xcprMF+44U1g=&quot;; Refresh “重定向或者新资源创建时使用，在页面的头部有个扩展可以实现相似的功能，并且大部分浏览器都支持meta http-equiv=&quot;refresh&quot; content=&quot;5; url=http://example.com/ Refresh: 5; url=http://www.w3.org/pub/WWW/People.html Retry-After 如果实体暂时不可用，可以设置这个值让客户端重试，可以使用时间段（单位是秒）或者HTTP时间 Example 1: Retry-After: 120 Example 2: Retry-After: Fri, 07 Nov 2014 23:59:59 GMT Server 服务器WEB名称 Server: Apache/2.4.1 (Unix) Set-Cookie 设置HTTP Cookie Set-Cookie: UserID=JohnDoe; Max-Age=3600; Version=1 Status 设置HTTP响应状态 Status: 200 OK Strict-Transport-Security 一种HSTS策略通知HTTP客户端缓存HTTPS策略多长时间以及是否应用到子域 Strict-Transport-Security: max-age=16070400; includeSubDomains Trailer 标识给定的header字段将展示在后续的chunked编码的消息中 Trailer: Max-Forwards Transfer-Encoding 设置传输实体的编码格式，目前支持的格式： chunked, compress, deflate, gzip, identity Transfer-Encoding: chunked TSV Tracking Status Value，在响应中设置给DNT(do-not-track),可能的取值 &quot;!&quot; — under construction &quot;?&quot; — dynamic &quot;G&quot; — gateway to multiple parties &quot;N&quot; — not tracking &quot;T&quot; — tracking &quot;C&quot; — tracking with consent &quot;P&quot; — tracking only if consented &quot;D&quot; — disregarding DNT &quot;U&quot; — updated TSV: ? Upgrade 请求客户端升级协议 Upgrade: HTTP/2.0, HTTPS/1.3, IRC/6.9, RTA/x11, websocket Vary 通知下级代理如何匹配未来的请求头已让其决定缓存的响应是否可用而不是重新从源主机请求新的 Example 1: Vary: * Example 2: Vary: Accept-Language Via 通知客户端代理，通过其要发送什么响应 Via: cache46.l2cn1341[84,206-0,H], cache38.l2c0c801[91,0], kunlun6.cn536[172,304-0,H], kunlun7.cn196[174,0] x-cache 是否命中缓存，HIT和MISS x-cache: HIT TCP_IMS_HIT dirn:-2:-2 Warning 实体可能会发生的问题的通用警告 Warning: 199 Miscellaneous warning WWW-Authenticate 标识访问请求实体的身份验证方案 WWW-Authenticate: Basic X-Frame-Options 点击劫持保护： deny frame中不渲染 sameorigin 如果源不匹配不渲染 allow-from 允许指定位置访问 allowall 不标准，允许任意位置访问 X-Frame-Options: deny 常用非标准响应头字段X-XSS-Protection 过滤跨站脚本 X-XSS-Protection: 1; mode=block Content-Security-Policy, X-Content-Security-Policy,X-WebKit-CSP 定义内容安全策略 X-WebKit-CSP: default-src &apos;self&apos; X-Content-Type-Options 唯一的取值是””,阻止IE在响应中嗅探定义的内容格式以外的其他MIME格式 X-Content-Type-Options: nosniff X-Powered-By 指定支持web应用的技术 X-Powered-By: PHP/5.4.0 X-UA-Compatible 推荐首选的渲染引擎来展示内容，通常向后兼容，也用于激活IE中内嵌chrome框架插件&lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;chrome=1&quot; /&gt; X-UA-Compatible: IE=EmulateIE7 X-UA-Compatible: IE=edge X-UA-Compatible: Chrome=1 X-Content-Duration 提供音视频的持续时间，单位是秒，只有Gecko内核浏览器支持 X-Content-Duration: 42.666 Upgrade-Insecure-Requests 标识服务器是否可以处理HTTPS协议 Upgrade-Insecure-Requests: 1 X-Request-ID,X-Correlation-ID 标识一个客户端和服务端的请求 X-Request-ID: f058ebd6-02f7-4d3f-942e-904344e8cde5 以下是一次客户端请求某网页的过程 如图通过二次请求对网页状态说明 客户端通过浏览器打开某网页，判断本地缓存是否过期。 没过期直接从缓存读取并且返回结果。 如果过期，服务器算出一个哈希值并通过 ETag 返回给浏览器，浏览器把哈希值和页面同时缓存在本地，当下次再次向服务器请求时，会通过类似 If-None-Match: “etag值” 的请求头把ETag发送给服务器，服务器再次计算页面的哈希值并和浏览器返回的值做比较，如果发现发生了变化就把页面返回给浏览器(200)，如果发现没有变化就给浏览器返回一个304未修改。这样通过控制浏览器端的缓存，可以节省服务器的带宽，因为服务器不需要每次都把全量数据返回给客户端。当未携带Etag时，客户端访问页面，服务器会将页面最后修改时间通过 Last-Modified 标识由服务器发往客户端，客户端记录修改时间，再次请求本地存在的cache页面时，客户端会通过 If-Modified-Since 头将先前服务器端发过来的Last-Modified时间戳发送回去，服务器端通过这个时间戳判断客户端的页面是否是最新的，如果不是最新的，则返回新的内容，如果是最新的，则 返回 304 告诉客户端其本地 cache 的页面是最新的。]]></content>
      <categories>
        <category>web</category>
      </categories>
      <tags>
        <tag>https</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS7 内核参数详解]]></title>
    <url>%2F2017%2F11%2F21%2Fcentos-kernel-analysis.html</url>
    <content type="text"><![CDATA[sysctl.conf 配置参数详解所谓Linux服务器内核参数优化，主要是指在Linux系统中针对业务服务应用而进行的系统内核参数调整，优化并无一定的标准。下面以生产环境下Linux常见的内核优化为例进行讲解，仅供参考。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495$ cat /etc/sysctl.conf#打开的文件句柄的数量fs.file-max = 265535#关闭ipv6net.ipv6.conf.all.disable_ipv6 = 1net.ipv6.conf.default.disable_ipv6 = 1# 避免放大攻击net.ipv4.icmp_echo_ignore_broadcasts = 1# 开启恶意icmp错误消息保护net.ipv4.icmp_ignore_bogus_error_responses = 1#关闭路由转发net.ipv4.ip_forward = 0net.ipv4.conf.all.send_redirects = 0net.ipv4.conf.default.send_redirects = 0#开启反向路径过滤net.ipv4.conf.all.rp_filter = 1net.ipv4.conf.default.rp_filter = 1#处理无源路由的包net.ipv4.conf.all.accept_source_route = 0net.ipv4.conf.default.accept_source_route = 0#关闭sysrq功能kernel.sysrq = 0#core文件名中添加pid作为扩展名kernel.core_uses_pid = 1# 开启SYN洪水攻击保护net.ipv4.tcp_syncookies = 1#修改消息队列长度kernel.msgmnb = 65536kernel.msgmax = 65536#设置最大内存共享段大小byteskernel.shmmax = 68719476736kernel.shmall = 4294967296#timewait的数量，默认180000net.ipv4.tcp_max_tw_buckets = 6000net.ipv4.tcp_sack = 1net.ipv4.tcp_window_scaling = 1net.ipv4.tcp_rmem = 4096 87380 4194304net.ipv4.tcp_wmem = 4096 16384 4194304net.core.wmem_default = 8388608net.core.rmem_default = 8388608net.core.rmem_max = 16777216net.core.wmem_max = 16777216#每个网络接口接收数据包的速率比内核处理这些包的速率快时，允许送到队列的数据包的最大数目net.core.netdev_max_backlog = 262144#限制仅仅是为了防止简单的DoS 攻击net.ipv4.tcp_max_orphans = 3276800#未收到客户端确认信息的连接请求的最大值net.ipv4.tcp_max_syn_backlog = 262144net.ipv4.tcp_timestamps = 0#内核放弃建立连接之前发送SYNACK 包的数量net.ipv4.tcp_synack_retries = 1#内核放弃建立连接之前发送SYN 包的数量net.ipv4.tcp_syn_retries = 1#启用timewait 快速回收net.ipv4.tcp_tw_recycle = 1#开启重用。允许将TIME-WAIT sockets 重新用于新的TCP 连接net.ipv4.tcp_tw_reuse = 1net.ipv4.tcp_mem = 94500000 915000000 927000000net.ipv4.tcp_fin_timeout = 1#当keepalive 起用的时候，TCP 发送keepalive 消息的频度。缺省是2 小时net.ipv4.tcp_keepalive_time = 30#允许系统打开的端口范围net.ipv4.ip_local_port_range = 1024 65000#修改防火墙表大小，默认65536net.netfilter.nf_conntrack_max=655350net.netfilter.nf_conntrack_tcp_timeout_established=1200# 确保无人能修改路由表net.ipv4.conf.all.accept_redirects = 0net.ipv4.conf.default.accept_redirects = 0net.ipv4.conf.all.secure_redirects = 0net.ipv4.conf.default.secure_redirects = 0 net.ipv4.tcp_tw_recycle = 1 启用TIME-WAIT状态sockets的快速回收，这个选项不推荐启用。在NAT(Network Address Translation)网络下，会导致大量的TCP连接建立错误。 我们在一些高并发的 WebServer上，为了端口能够快速回收，常常会打开了 tcp_tw_reccycle 。在关闭 tcp_tw_reccycle 的时候，kernel 是不会检查对端机器的包的时间戳的；而打开了 tcp_tw_reccycle 了，就会检查时间戳，很不幸网络发来的包的时间戳是乱跳的，所以我方的就把带了“倒退”的时间戳的包当作是“recycle的tw连接的重传数据，不是新的请求”，于是丢掉不回包，造成大量丢包。 当运行sysctl -p命令时报error: ‘net.ipv4.ip_conntrack_max’ is an unknown key 错时,通过以下命令修正：$ modprobe ip_conntrack$ echo “modprobe ip_conntrack” &gt;&gt; /etc/rc.local 一套生产环境使用过的内核参数 sysctl.conf文件 1234567891011121314151617181920212223242526272829303132333435363738394041fs.file-max=65535net.ipv4.ip_forward = 0net.ipv4.conf.default.rp_filter = 1net.ipv4.conf.default.accept_source_route = 0net.ipv4.tcp_max_tw_buckets = 6000net.ipv4.ip_local_port_range = 1024 65000net.ipv4.tcp_timestamps = 1net.ipv4.tcp_tw_recycle = 0net.ipv4.tcp_tw_reuse = 1net.ipv4.tcp_syncookies = 1kernel.msgmnb = 65536kernel.msgmax = 65536kernel.shmmax = 68719476736kernel.shmall = 4294967296net.ipv4.tcp_max_syn_backlog = 262144net.core.netdev_max_backlog = 262144net.core.somaxconn = 262144net.ipv4.tcp_max_orphans = 262144net.ipv4.tcp_synack_retries = 1net.ipv4.tcp_syn_retries = 1net.ipv4.tcp_fin_timeout = 1net.ipv4.tcp_keepalive_time = 30net.ipv4.tcp_sack = 1net.ipv4.tcp_window_scaling = 1net.ipv4.tcp_rmem = 4096 87380 4194304net.ipv4.tcp_wmem = 4096 16384 4194304net.core.wmem_default = 8388608net.core.rmem_default = 8388608net.core.rmem_max = 16777216net.core.wmem_max = 16777216net.ipv4.tcp_mem = 94500000 915000000 927000000net.nf_conntrack_max = 6553500#redisvm.dirty_ratio=10vm.dirty_background_ratio=5 禁用SELINUX 12$ vi /etc/sysconfig/selinux 设置为disabled 同步时间 12$ crontal -l*/20 * * * * /usr/sbin/ntpdate pool.ntp.org &gt; /dev/null 2&gt;&amp;1 文件描述符数量修改/etc/security/limits.conf文件，在文件末尾添加 1234root soft nofile 65535root hard nofile 65535* soft nofile 65535* hard nofile 65535 还需要修改/etc/security/limits.d下面的conf文件(会覆盖前面的配置信息)，我的是20-nproc.conf12* soft nproc 65535* hard nproc 65535 禁用自带的Firewalld防火墙 12systemctl stop firewalld.service 停止firewallsystemctl display firewalld.service 禁止firewall开机自启动]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[systemctl unit服务格式]]></title>
    <url>%2F2017%2F11%2F16%2Fsystemctl-unit.html</url>
    <content type="text"><![CDATA[systemctl是RHEL 7 的服务管理工具中主要的工具，它融合之前service和chkconfig的功能于一体。可以使用它永久性或只在当前会话中启用/禁用服务。 服务权限systemd有系统和用户区分；系统/user/lib/systemd/system/, 用户/etc/lib/systemd/user/一般系统管理员手工创建的单元文件建议存放在/etc/systemd/system/目录下面。或者/usr/lib/systemd/system/下面 ，然后可以通过systemctl enable xxx.service方式将该服务添加到/etc/systemd/system/multi-user.target.wants/目录下面设置为开机自启动。 格式介绍systemctl的服务文件主要包含[Unit]、[Service]、[Install]三类。下面我们对这三类进行说明。 [Unit]该部分主要对服务进行说明。 Description : 服务的简单描述。 Documentation ： 服务文档地址说明。 Before=xxx.service：代表本服务在xxx.service启动之前启动。 After=xxx.service：代表本服务在xxx.service启动之后启动。 Requires： 本服务启动后，它需要的服务也会启动；而本服务需要的服务被停止了，本服务也停止了。 Wants： 推荐使用。本服务启动了，它需要的服务也会被启动；而本服务需要的服务被停止了，对本单元没有影响。 [Service]该部分的配置服务的启动、重启、停止命令全部要求使用绝对路径，使用相对路径则会报错。 WorkingDirectory：指定服务运行的工作目录。 EnvironmentFile：指定服务需要的配置文件，下文可以用${}方式引用文件中的值。 Type=simple（默认值）：systemd认为该服务将立即启动。服务进程不会fork。如果该服务要启动其他服务，不要使用此类型启动，除非该服务是socket激活型。 Type=forking：systemd认为当该服务进程fork，且父进程退出后服务启动成功。对于常规的守护进程（daemon），除非你确定此启动方式无法满足需求，使用此类型启动即可。使用此启动类型应同时指定 PIDFile=，以便systemd能够跟踪服务的主进程。 Type=oneshot：这一选项适用于只执行一项任务、随后立即退出的服务。可能需要同时设置 RemainAfterExit=yes 使得 systemd 在服务进程退出之后仍然认为服务处于激活状态。 Type=notify：与 Type=simple 相同，但约定服务会在就绪后向 systemd 发送一个信号。这一通知的实现由 libsystemd-daemon.so 提供。 Type=dbus：若以此方式启动，当指定的 BusName 出现在DBus系统总线上时，systemd认为服务就绪。 Type=idle: systemd会等待所有任务(Jobs)处理完成后，才开始执行idle类型的单元。除此之外，其他行为和Type=simple 类似。 PIDFile：指定pid文件路径 ExecStart：指定启动单元的命令或者脚本 ExecStartPre和ExecStartPost：指定在执行ExecStart之前或者之后执行用户自定义的脚本，Type=oneshot允许用户执行多个按顺序定义的脚本。 ExecReload：指定单元重载时执行的命令或者脚本。 ExecStop：指定单元停止时执行的命令或者脚本。 PrivateTmp：True表示给服务分配独立的临时空间 LimitNOFILE：设置文件描述符相关参数个数，或者设置为无穷 infinity LimitNPROC：设置文件描述符相关参数个数，或者设置为无穷 infinity LimitCORE：设置文件描述符相关参数个数，或者设置为无穷 infinity Restart：这个选项如果被允许，服务重启的时候进程会退出，会通过systemctl命令执行清除并重启的操作。通常设置为on-failure。 RemainAfterExit：如果设置这个选择为真，服务会被认为是在激活状态，即使所以的进程已经退出，默认的值为假，这个选项只有在Type=oneshot时需要被配置。 [Install]定义如何安装这个配置文件，即怎样做到开机启动。 Alias：为单元提供一个空间分离的附加名字。 RequiredBy：单元被允许运行需要的一系列依赖单元，RequiredBy列表从Require获得依赖信息。 WantBy：单元被允许运行需要的弱依赖性单元，Wantby从Want列表获得依赖信息。 Also：指出和单元一起安装或者被协助的单元。 DefaultInstance：实例单元的限制，这个选项指定如果单元被允许运行默认的实例。 mysql.service样例123456789101112131415161718192021222324[Unit]Description=MySQL ServerDocumentation=man:mysqld(8)Documentation=http://dev.mysql.com/doc/refman/en/using-systemd.htmlAfter=network.targetAfter=syslog.target[Service]User=mysqlGroup=mysqlType=forkingPIDFile=/var/run/mysqld/mysqld.pidTimeoutSec=0PermissionsStartOnly=trueExecStartPre=/usr/bin/mysqld_pre_systemdExecStart=/usr/sbin/mysqld --daemonize --pid-file=/var/run/mysqld/mysqld.pid $MYSQLD_OPTSEnvironmentFile=-/etc/sysconfig/mysqlLimitNOFILE = 5000Restart=on-failureRestartPreventExitStatus=1PrivateTmp=false[Install]WantedBy=multi-user.target etcd.service样例12345678910111213141516[Unit]Description=Etcd ServerAfter=network.targetAfter=network-online.targetWants=network-online.target[Service]Type=notifyWorkingDirectory=/var/lib/etcd/EnvironmentFile=-/etc/etcd/etcd.conf #etcd配置文件路径ExecStart=/bin/bash -c "GOMAXPROCS=$(nproc) /usr/bin/etcd --name=\"$&#123;ETCD_NAME&#125;\" --data-dir=\"$&#123;ETCD_DATA_DIR&#125;\" --listen-client-urls=\"$&#123;ETCD_LISTEN_CLIENT_URLS&#125;\""Restart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.target # 说明：其中WorkingDirectory为etcd数据库目录，需要在etcd**安装前创建**]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[https证书加密解密原理解析]]></title>
    <url>%2F2017%2F10%2F26%2Fhttps-tls.html</url>
    <content type="text"><![CDATA[我们都知道HTTPS能够加密信息，以免敏感信息被第三方获取。所以很多银行网站或电子邮箱等等安全级别较高的服务都会采用HTTPS协议。而HTTPS可以看作是安全的HTTP，你可能听说过关于HTTPS的一些问题，比如什么握手，什么证书，加密之类的等等。HTTPS为何能保障web的安全，其运行原理是怎样的，当我们深入了解下去，其设计的思路对我们其他安全方面的设计也有一定的启发作用。 与http的区别HTTPS其实是有两部分组成：HTTP + SSL / TLS ，也就是在HTTP上又加了一层处理加密信息的模块。服务端和客户端的信息传输都会通过TLS进行加密，所以传输的数据都是加密后的数据。如下图。1234567891011HTTP HTTPS |-------| |-------| | HTTP | | HTTPS | |-------| |-------| | TCP | |ssl TLS| |-------| |-------| | IP | | TCP | |-------| |-------| | IP | |-------| HTTPS加密解密证书流程图HTTPS流程包含握手和后续的数据传输，握手的目的是为了客户端与服务端协商加密算法等参数。SSL/TLS基本过程是这样的： 客户端向服务器端所要并验证证书 双方协定加密算法以及“对话密钥” 双方采用协商后的“对话密钥”进行加密通信 整个流程步骤如下： 客户端发起HTTPS请求 这个没什么好说的，就是用户在浏览器里输入一个https网址，然后连接到server的443端口。 服务端的配置 采用HTTPS协议的服务器必须要有一套数字证书，可以自己制作，也可以向组织申请。区别就是自己颁发的证书需要客户端验证通过，才可以继续访问，而使用受信任的公司申请的证书则不会弹出提示页面(startssl就是个不错的选择，有1年的免费服务)。这套证书其实就是一对公钥和私钥。如果对公钥和私钥不太理解，可以想象成一把钥匙和一个锁头，只是全世界只有你一个人有这把钥匙，你可以把锁头给别人，别人可以用这个锁把重要的东西锁起来，然后发给你，因为只有你一个人有这把钥匙，所以只有你才能看到被这把锁锁起来的东西。 传送证书 这个证书其实就是公钥，只是包含了很多信息，如证书的颁发机构，过期时间等等。 客户端解析证书 这部分工作是有客户端的TLS来完成的，首先会验证公钥是否有效，比如颁发机构，过期时间等等，如果发现异常，则会弹出一个警告框，提示证书存在问题。如果证书没有问题，那么就生成一个随即值。然后用证书对该随机值进行加密。就好像上面说的，把随机值用锁头锁起来，这样除非有钥匙，不然看不到被锁住的内容。 传送加密信息 这部分传送的是用证书加密后的随机值，目的就是让服务端得到这个随机值，以后客户端和服务端的通信就可以通过这个随机值来进行加密解密了。 服务段解密信息 服务端用私钥解密后，得到了客户端传过来的随机值(私钥)，然后把内容通过该值进行对称加密。所谓对称加密就是，将信息和私钥通过某种算法混合在一起，这样除非知道私钥，不然无法获取内容，而正好客户端和服务端都知道这个私钥，所以只要加密算法够彪悍，私钥够复杂，数据就够安全。 传输加密后的信息 这部分信息是服务段用私钥加密后的信息，可以在客户端被还原 客户端解密信息 客户端用之前生成的私钥解密服务段传过来的信息，于是获取了解密后的内容。整个过程第三方即使监听到了数据，也束手无策。 HTTPS加密解密证书流程图解释HTTPS在传输数据之前需要客户端（浏览器）与服务端（网站）之间进行一次握手，在握手过程中将确立双方加密传输数据的密码信息。TLS/SSL协议不仅仅是一套加密传输的协议，更是一件经过艺术家精心设计的艺术品，TLS/SSL中使用了非对称加密，对称加密以及HASH算法。握手过程的具体描述如下：11.浏览器将自己支持的一套加密规则发送给网站。 12.网站从中选出一组加密算法与HASH算法，并将自己的身份信息以证书的形式发回给浏览器。证书里面包含了网站地址，加密公钥，以及证书的颁发机构等信息。 12343.浏览器获得网站证书之后浏览器要做以下工作： a) 验证证书的合法性（颁发证书的机构是否合法，证书中包含的网站地址是否与正在访问的地址一致等），如果证书受信任，则浏览器栏里面会显示一个小锁头，否则会给出证书不受信的提示。 b) 如果证书受信任，或者是用户接受了不受信的证书，浏览器会生成一串随机数的密码，并用证书中提供的公钥加密。 c) 使用约定好的HASH算法计算握手消息，并使用生成的随机数对消息进行加密，最后将之前生成的所有信息发送给网站。 1234.网站接收浏览器发来的数据之后要做以下的操作： a) 使用自己的私钥将信息解密取出密码，使用密码解密浏览器发来的握手消息，并验证HASH是否与浏览器发来的一致。 b) 使用密码加密一段握手消息，发送给浏览器。 15.浏览器解密并计算握手消息的HASH，如果与服务端发来的HASH一致，此时握手过程结束，之后所有的通信数据将由之前浏览器生成的随机密码并利用对称加密算法进行加密。 这里浏览器与网站互相发送加密的握手消息并验证，目的是为了保证双方都获得了一致的密码，并且可以正常的加密解密数据，为后续真正数据的传输做一次测试。另外，HTTPS一般使用的加密与HASH算法如下：123非对称加密算法：RSA，DSA/DSS 对称加密算法：AES，RC4，3DES HASH算法：MD5，SHA1，SHA256]]></content>
      <categories>
        <category>web</category>
      </categories>
      <tags>
        <tag>https</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Markdown格式书写]]></title>
    <url>%2F2017%2F07%2F23%2Fmarkdown-format.html</url>
    <content type="text"><![CDATA[区块元素标题Markdown 支持两种标题的语法，类似 Setext 和类 atx 形式。类 Setext 形式是用底线的形式，利用 = （最高阶标题）和 - （第二阶标题），例如：1234This is an H1=============This is an H2------------- 也可以是类 Atx 形式则是在行首插入 1 到 6 个 # ，对应到标题 1 到 6 阶，例如：123# 这是 H1## 这是 H2###### 这是 H6 区块引用 Blockquotes区块引用是使用类似 email 中用 &gt; 的引用方式。例如:12345&gt; This is a blockquote with two paragraphs. Lorem ipsum dolor sit amet,&gt; consectetuer adipiscing elit. Aliquam hendrerit mi posuere lectus.&gt; Vestibulum enim wisi, viverra nec, fringilla in, laoreet vitae, risus.&gt;&gt; Donec sit amet nisl. Aliquam semper ipsum sit amet velit. Suspendisse 区块引用可以嵌套（例如：引用内的引用），只要根据层次加上不同数量的 &gt; ：12345&gt; This is the first level of quoting.&gt;&gt; &gt; This is nested blockquote.&gt;&gt; Back to the first level. 引用的区块内也可以使用其他的 Markdown 语法，包括标题、列表、代码区块等：12345678&gt; ### 这是一个标题。&gt; &gt; 1. 这是第一行列表项。&gt; 2. 这是第二行列表项。&gt; &gt; 给出一些例子代码：&gt; &gt; return shell_exec("echo $input | $markdown_script"); 列表Markdown 支持有序列表和无序列表。 无序列表使用星号、加号或是减号作为列表标记： 12345678910111213* Red* Green* Blue等同于：+ Red+ Green+ Blue也等同于：- Red- Green- Blue 有序列表则使用数字接着一个英文句点： 1231. Bird2. McHale3. Parish 要让列表看起来更漂亮，你可以把内容用固定的缩进整理好： * Lorem ipsum dolor sit amet, consectetuer adipiscing elit. Aliquam hendrerit mi posuere lectus. Vestibulum enim wisi, viverra nec, fringilla in, laoreet vitae, risus. * Donec sit amet nisl. Aliquam semper ipsum sit amet velit. Suspendisse id sem consectetuer libero luctus adipiscing. 列表项目可以包含多个段落， 每个项目下的段落都必须缩进 4 个空格或是 1 个制表符： 1. This is a list item with two paragraphs. Lorem ipsum dolor sit amet, consectetuer adipiscing elit. Aliquam hendrerit mi posuere lectus. Vestibulum enim wisi, viverra nec, fringilla in, laoreet vitae, risus. Donec sit amet nisl. Aliquam semper ipsum sit amet velit. 2. Suspendisse id sem consectetuer libero luctus adipiscing. 如果要在列表项目内放进引用，那 &gt; 就需要缩进： * A list item with a blockquote: &gt; This is a blockquote &gt; inside a list item. 如果要放代码区块的话，该区块就需要缩进两次，也就是 8 个空格或是 2 个制表符： * 一列表项包含一个列表区块： &lt;代码写在这&gt; 代码区块和程序相关的写作或是标签语言原始码通常会有已经排版好的代码区块，通常这些区块我们并不希望它以一般段落文件的方式去排版，而是照原来的样子显示，要在 Markdown 中建立代码区块很简单，只要代码块每行简单地缩进 4 个空格或是 1 个制表符就可以，例如，下面的输入：123这是一个普通段落： 这是一个代码区块。 markdown也支持html的源码格式，只需要复制贴上，再加上缩进就可以了，剩下的 Markdown 都会帮你处理，例如： &lt;div class=&quot;footer&quot;&gt; &amp;copy; 2004 Foo Corporation &lt;/div&gt; 代码区块中，一般的 Markdown 语法不会被转换，像是星号便只是星号，这表示你可以很容易地以 Markdown 语法撰写 Markdown 语法相关的文件。 分割线你可以在一行中用三个以上的星号、减号、底线来建立一个分隔线，行内不能有其他东西。你也可以在星号或是减号中间插入空格。下面每种写法都可以建立分隔线：1234567* * *********- - - 表格在markdown中也支持表格格式，格式如下：12345菜单1 | 菜单2 | 菜单3 | 菜单n----: | :---：| :---- | :----内容1 | 内容1 | 内容1 | 内容1内容2 | 内容2 | 内容2 | 内容2内容3 | 内容3 | 内容3 | 内容3 Markdown 插入的表格，单元格中默认左对齐；表头单元格中的内容会一直居中对齐，可以使用：来设置对齐方式，:---: 居中对齐，---: 右对齐， :--- 左对齐，- 的个数不限制。 区段元素链接Markdown 支持两种形式的链接语法： 行内式 和参考式 两种形式。不管是哪一种，链接文字都是用 [方括号] 来标记。要建立一个行内式 的链接，只要在方块括号后面紧接着圆括号并插入网址链接即可，如果你还想要加上链接的 title 文字，只要在网址后面，用双引号把 title 文字包起来即可，例如：12This is [an example](http://example.com/ "Title") inline link.[This link](http://example.net/) has no title attribute. 如果你是要链接到同样主机的资源，你可以使用相对路径： See my [About](/about/) page for details. 参考式 的链接是在链接文字的括号后面再接上另一个方括号，而在第二个方括号里面要填入用以辨识链接的标记,接着，在文件的任意处，你可以把这个标记的链接内容定义出来：：12This is [an example][id] reference-style link.[id]: http://example.com/ "Optional Title Here" 链接内容定义的形式为： 方括号（前面可以选择性地加上至多三个空格来缩进），里面输入链接文字 接着一个冒号 接着一个以上的空格或制表符 接着链接的网址 选择性地接着 title 内容，可以用单引号、双引号或是括弧包着 下面这三种链接的定义都是相同：123[foo]: http://example.com/ "Optional Title Here"[foo]: http://example.com/ 'Optional Title Here'[foo]: http://example.com/ (Optional Title Here) 请注意： 有一个已知的问题是 Markdown.pl 1.0.1 会忽略单引号包起来的链接 title。 强调Markdown 使用星号（*）和底线（_）作为标记强调字词 的符号，被 或 _ 包围的字词会被转成用 标签包围，用`两个 或 _ 包起来的话，则会被转成 &lt;strong&gt;，被三个星号（*）包起来则为倾斜和加粗文字，被两个波浪线（~~）` 包为起来是要在文字上加删除线。例如：1234567891011*single asterisks*_single underscores_**double asterisks**__double underscores__***double underscores***~~double underscores~~ 你可以随便用你喜欢的样式，唯一的限制是，你用什么符号开启标签，就要用什么符号结束。 但是如果你的 * 和 _ 两边都有空白的话，它们就只会被当成普通的符号。 如果要在文字前后直接插入普通的星号或底线，你可以用反斜线： \*this text is surrounded by literal asterisks\* 代码 单行短句 如果要标记一小段行内代码，你可以用反引号把它包起来` `，例如：1Use the `printf()` function. 如果要在代码区段内插入反引号，你可以用多个反引号来开启和结束代码区段： ``There is a literal backtick (`) here.`` 多行代码块 123代码块1代码块2代码块3 其中```也可以用~~~代替，且```后也支持语言关键字书写，使代码块格式化。例如```bash``` 语言对应关键字请参照本文最末尾。 图片很明显地，要在纯文字应用中设计一个「自然」的语法来插入图片是有一定难度的。Markdown 使用一种和链接很相似的语法来标记图片，同样也允许两种样式： 行内式 和参考式 。行内式的图片语法看起来像是：12![Alt text](/path/to/img.jpg)![Alt text](/path/to/img.jpg "Optional title") 详细叙述如下： 一个惊叹号 ! 接着一个方括号，里面放上图片下面的说明文字，相当于对图片内容的解释。 接着一个普通括号，里面放上图片的网址，最后还可以用引号包住并加上 选择性的 ‘title’ 文字。 参考式 的图片语法则长得像这样：12![Alt text][id][id]: url/to/image "Optional title attribute" 「id」是图片参考的名称，图片参考的定义方式则和连结参考一样： 字体和颜色Markdown是一种可以使用普通文本编辑器编写的标记语言，通过类似HTML的标记语法，它可以使普通文本内容具有一定的格式。但是它本身是不支持修改字体、字号与颜色 等功能的！为了使其修改字体和颜色，我们可以用html语法代替，如：123456&lt;font face="黑体"&gt;我是黑体字&lt;/font&gt;&lt;font face="微软雅黑"&gt;我是微软雅黑&lt;/font&gt;&lt;font face="STCAIYUN"&gt;我是华文彩云&lt;/font&gt;&lt;font color=#FF0000 size=3 face="黑体"&gt;这是一行红色3号大小的黑体文本&lt;/font&gt;&lt;font color=#00ffff size=72&gt;这是16位颜色值表示&lt;/font&gt;&lt;font color=gray size=72&gt;也可以用颜色的英文单词表示&lt;/font&gt; Size：规定文本的尺寸大小。可能的值：从 1 到 7 的数字。浏览器默认值是 3。颜色16进制值可以参考：https://www.114la.com/other/rgb.htm note和label颜色下面是note显示的颜色： default primary success info warning danger danger no-icon1234567&lt;div class="note default"&gt;&lt;p&gt;default&lt;/p&gt;&lt;/div&gt;&lt;div class="note primary"&gt;&lt;p&gt;primary&lt;/p&gt;&lt;/div&gt;&lt;div class="note success"&gt;&lt;p&gt;success&lt;/p&gt;&lt;/div&gt;&lt;div class="note info"&gt;&lt;p&gt;info&lt;/p&gt;&lt;/div&gt;&lt;div class="note warning"&gt;&lt;p&gt;warning&lt;/p&gt;&lt;/div&gt;&lt;div class="note danger"&gt;&lt;p&gt;danger&lt;/p&gt;&lt;/div&gt;&lt;div class="note danger no-icon"&gt;&lt;p&gt;danger no-icon&lt;/p&gt;&lt;/div&gt; 下面是label显示的颜色primarydefaultsuccessinfowarningdanger123456&#123;% label primary@primary %&#125;&#123;% label default@default %&#125;&#123;% label success@success %&#125;&#123;% label info@info %&#125;&#123;% label warning@warning %&#125;&#123;% label danger@danger %&#125; 其他自动链接Markdown 支持以比较简短的自动链接形式来处理网址和电子邮件信箱，只要是用方括号包起来， Markdown 就会自动把它转成链接。一般网址的链接文字就和链接地址一样，例如：1&lt;http://example.com/&gt; Markdown 会转为： &lt;a href=&quot;http://example.com/&quot;&gt;http://example.com/&lt;/a&gt; 反斜杠Markdown 可以利用反斜杠来插入一些在语法中有其它意义的符号，例如：如果你想要用星号加在文字旁边的方式来做出强调效果（但不用 标签），你可以在星号的前面加上反斜杠：1\*literal asterisks\* Markdown 支持以下这些符号前面加上反斜杠来帮助插入普通的符号：123456789101112\ 反斜线` 反引号* 星号_ 底线&#123;&#125; 花括号[] 方括号() 括弧# 井字号+ 加号- 减号. 英文句点! 惊叹号 语言关键字 语言名 关键字 Bash bash CoffeeScript coffeescript C++ cpp C# cs CSS css Diff diff HTTP http Ini ini Java java JavaScript javascript JSON json XML xml Makefile makefile Markdown markdown Objective-C objectivec Perl perl Python python Ruby ruby SQL sql]]></content>
      <categories>
        <category>markdown</category>
      </categories>
      <tags>
        <tag>markdown</tag>
      </tags>
  </entry>
</search>
